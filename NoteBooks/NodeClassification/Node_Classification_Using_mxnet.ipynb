{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experimental Validation"
      ],
      "metadata": {
        "id": "MqE9qBTDwMMC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KARATE CLUB"
      ],
      "metadata": {
        "id": "o_G9cMaq9N4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "from networkx import read_edgelist, set_node_attributes\n",
        "from pandas import read_csv, Series\n",
        "from numpy import array\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "kZ3vetvHMlnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DataSet = namedtuple(\n",
        "    'DataSet',\n",
        "    field_names=['X_train', 'y_train', 'X_test', 'y_test', 'network']\n",
        ")\n",
        "\n",
        "def load_karate_club():\n",
        "    network = read_edgelist(\n",
        "        'karate.edgelist',\n",
        "        nodetype=int)\n",
        "\n",
        "    attributes = read_csv(\n",
        "        'karate.attributes.csv',\n",
        "        index_col=['node'])\n",
        "\n",
        "    for attribute in attributes.columns.values:\n",
        "        set_node_attributes(\n",
        "            network,\n",
        "            values=Series(\n",
        "                attributes[attribute],\n",
        "                index=attributes.index).to_dict(),\n",
        "            name=attribute\n",
        "        )\n",
        "  \n",
        "    X_train,y_train = map(array, zip(*[\n",
        "        ([node], data['role'] == 'Administrator')\n",
        "        for node, data in network.nodes(data=True)\n",
        "        if data['role'] in {'Administrator', 'Instructor'}\n",
        "    ]))\n",
        "  \n",
        "    X_test, y_test = map(array, zip(*[\n",
        "        ([node], data['community'] == 'Administrator')\n",
        "        for node, data in network.nodes(data=True)\n",
        "        if data['role'] == 'Member'\n",
        "    ]))\n",
        " \n",
        "    return DataSet(\n",
        "        X_train, y_train,\n",
        "        X_test, y_test,\n",
        "        network)"
      ],
      "metadata": {
        "id": "ysTfin-29PHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from networkx import to_numpy_array"
      ],
      "metadata": {
        "id": "vB_Dpwx-7pCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "zkc = load_karate_club()\n",
        "X_train = zkc.X_train.flatten()\n",
        "X_test= zkc.X_test.flatten()\n",
        "y_train = zkc.y_train\n",
        "y_test = zkc.y_test\n",
        "print(y_train)\n",
        "A=to_numpy_array(zkc.network)\n",
        "#A = to_numpy_matrix(zkc.network)\n",
        "#A = torch.from_numpy(np.array(A,dtype=object))\n",
        "print(A)\n",
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUImblupL-Fj",
        "outputId": "5ee9cbd2-d225-49f2-ba4e-231943d64eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ True False]\n",
            "[[0. 1. 1. ... 0. 0. 0.]\n",
            " [1. 0. 1. ... 0. 0. 0.]\n",
            " [1. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 1. 0. 0.]]\n",
            "[ 0 33]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mxnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZN5lWovr7zZ3",
        "outputId": "ce8453a8-6034-4ffd-92b9-15236ca5844c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mxnet\n",
            "  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (1.22.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (2.27.1)\n",
            "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.4)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.20.1\n",
            "    Uninstalling graphviz-0.20.1:\n",
            "      Successfully uninstalled graphviz-0.20.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mxnet.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "_FjJRI8HYoeE",
        "outputId": "539112fa-0238-4d26-c0e3-1507975875af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4314eb85fa16>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmxnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'mxnet' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from networkx import degree_centrality, betweenness_centrality, shortest_path_length,to_numpy_array\n",
        "import mxnet.ndarray as nd\n",
        "\n"
      ],
      "metadata": {
        "id": "8B_z9JwE9RzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Viewing the graph:"
      ],
      "metadata": {
        "id": "Iv9mu28Uj72l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "pos = nx.fruchterman_reingold_layout(zkc.network)\n",
        "nx.draw_networkx(zkc.network, pos=pos, node_size = 800, font_size=8, node_color=\"pink\")\n",
        "nx.draw_networkx_edges(zkc.network, pos=pos)\n",
        "edge_labels = nx.get_edge_attributes(zkc.network, 'weight')\n",
        "nx.draw_networkx_edge_labels(zkc.network, pos=pos, edge_labels=edge_labels)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "tzM_LsAYDjtF",
        "outputId": "ae6049cf-8ffa-4d20-e329-84336cefe01a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GCN model is below:\n",
        "\n",
        "First, the spectral rule has been implemented. Next, a two-layer architecture is used where the first hidden layer has 4 units and the second hidden layer has 2 units. Finally,  a logistic regression layer is added on top of the GCN for node classification."
      ],
      "metadata": {
        "id": "jGSGIajHj9N8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mxnet.gluon import HybridBlock\n",
        "from mxnet.gluon.nn import Activation\n",
        "import mxnet.ndarray as nd\n",
        "\n",
        "class SpectralRule(HybridBlock):\n",
        "    def __init__(self, A, in_units, out_units, activation='relu', **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        I = nd.eye(*A.shape)\n",
        "        A_hat = A.copy() + I\n",
        "\n",
        "        D = nd.sum(A_hat, axis=0)\n",
        "        D_inv = D**-0.5\n",
        "        D_inv = nd.diag(D_inv)\n",
        "\n",
        "        A_hat = D_inv * A_hat * D_inv\n",
        "        \n",
        "        self.in_units, self.out_units = in_units, out_units\n",
        "        \n",
        "        with self.name_scope():\n",
        "            self.A_hat = self.params.get_constant('A_hat', A_hat)\n",
        "            self.W = self.params.get(\n",
        "                'W', shape=(self.in_units, self.out_units)\n",
        "            )\n",
        "            if activation == 'identity':\n",
        "                self.activation = lambda X: X\n",
        "            else:\n",
        "                self.activation = Activation(activation)\n",
        "\n",
        "    def hybrid_forward(self, F, X, A_hat, W):\n",
        "        aggregate = F.dot(A_hat, X)\n",
        "        propagate = self.activation(\n",
        "            F.dot(aggregate, W))\n",
        "        return propagate"
      ],
      "metadata": {
        "id": "ce3Co_O79eK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressor(HybridBlock):\n",
        "    def __init__(self, in_units, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        with self.name_scope():\n",
        "            self.w = self.params.get(\n",
        "                'w', shape=(1, in_units)\n",
        "            )\n",
        "\n",
        "            self.b = self.params.get(\n",
        "                'b', shape=(1, 1)\n",
        "            )\n",
        "\n",
        "    def hybrid_forward(self, F, X, w, b):\n",
        "        # Change shape of b to comply with MXnet addition API\n",
        "        b = F.broadcast_axis(b, axis=(0,1), size=(34, 1))\n",
        "        y = F.dot(X, w, transpose_b=True) + b\n",
        "\n",
        "        return F.sigmoid(y)"
      ],
      "metadata": {
        "id": "qe5ftbI0AE5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mxnet.gluon.nn import HybridSequential, Activation\n",
        "from mxnet.ndarray import array\n",
        "from mxnet.initializer import One, Uniform, Xavier\n",
        "from mxnet.gluon.loss import SigmoidBinaryCrossEntropyLoss\n",
        "\n",
        "def build_features(A, X):\n",
        "    hidden_layer_specs = [(4, 'tanh'), (2, 'tanh')] # Format: (units in layer, activation function)\n",
        "    in_units = in_units=X.shape[1]\n",
        "  \n",
        "    features = HybridSequential()\n",
        "    with features.name_scope():\n",
        "        for i, (layer_size, activation_func) in enumerate(hidden_layer_specs):\n",
        "            layer = SpectralRule(\n",
        "                A, in_units=in_units, out_units=layer_size, \n",
        "                activation=activation_func)\n",
        "            features.add(layer)\n",
        "\n",
        "            in_units = layer_size\n",
        "    return features, in_units\n",
        "\n",
        "def build_model(A, X):\n",
        "    model = HybridSequential()\n",
        "    hidden_layer_specs = [(4, 'tanh'), (2, 'tanh')]\n",
        "    in_units = in_units=X.shape[1]\n",
        "\n",
        "    with model.name_scope():\n",
        "        features, out_units = build_features(A, X)\n",
        "        model.add(features)\n",
        "\n",
        "        classifier = LogisticRegressor(out_units)\n",
        "        model.add(classifier)\n",
        "\n",
        "    model.hybridize()\n",
        "    model.initialize(Uniform(1))\n",
        "\n",
        "    return model, features"
      ],
      "metadata": {
        "id": "QSaKinosAG9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(A)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPlVno4WF42C",
        "outputId": "b16c1540-d9ae-45d6-c80e-73666d028659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "808omIeY92r0",
        "outputId": "25ed3c56-0f55-4e79-c5bd-940d105e598a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 1., ..., 0., 0., 0.],\n",
              "       [1., 0., 1., ..., 0., 0., 0.],\n",
              "       [1., 1., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mxnet\n",
        "X_1 = I = nd.eye(*A.shape)\n",
        "model_1, features_1 = build_model(nd.array(A), X_1)\n",
        "#model_1(X_1)"
      ],
      "metadata": {
        "id": "sx53Q3mxAJZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1(X_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPpQskFJGpIu",
        "outputId": "f9305d46-5b70-4b67-e7b4-d4e8bf7a978d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[[0.37296507]\n",
              " [0.37318814]\n",
              " [0.3731439 ]\n",
              " [0.37353897]\n",
              " [0.3687405 ]\n",
              " [0.37294304]\n",
              " [0.37234062]\n",
              " [0.37076518]\n",
              " [0.37428105]\n",
              " [0.37697908]\n",
              " [0.38529056]\n",
              " [0.3775322 ]\n",
              " [0.37292567]\n",
              " [0.37116757]\n",
              " [0.37769338]\n",
              " [0.3667578 ]\n",
              " [0.37239736]\n",
              " [0.3763874 ]\n",
              " [0.38741082]\n",
              " [0.3715641 ]\n",
              " [0.37834412]\n",
              " [0.37354776]\n",
              " [0.3752206 ]\n",
              " [0.37326825]\n",
              " [0.37362063]\n",
              " [0.3643201 ]\n",
              " [0.37039787]\n",
              " [0.3698834 ]\n",
              " [0.37370875]\n",
              " [0.37336743]\n",
              " [0.37572038]\n",
              " [0.37264198]\n",
              " [0.36601457]\n",
              " [0.3753558 ]]\n",
              "<NDArray 34x1 @cpu(0)>"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%time\n",
        "from mxnet import autograd\n",
        "from mxnet.gluon import Trainer\n",
        "from mxnet.ndarray import sum as ndsum\n",
        "import numpy as np\n",
        "\n",
        "def train(model, features, X, X_train, y_train, epochs):\n",
        "    cross_entropy = SigmoidBinaryCrossEntropyLoss(from_sigmoid=True)\n",
        "    trainer = Trainer(model.collect_params(), 'sgd', {'learning_rate': 0.001, 'momentum': 1})\n",
        "\n",
        "    feature_representations = [features(X).asnumpy()]\n",
        "\n",
        "    for e in range(1, epochs + 1):\n",
        "        cum_loss = 0\n",
        "        cum_preds = []\n",
        "\n",
        "        for i, x in enumerate(X_train):\n",
        "            y = array(y_train)[i]\n",
        "            with autograd.record():\n",
        "                preds = model(X)[x]\n",
        "                loss = cross_entropy(preds, y)\n",
        "            loss.backward()\n",
        "            trainer.step(1)\n",
        "\n",
        "            cum_loss += loss.asscalar()\n",
        "            cum_preds += [preds.asscalar()]\n",
        "\n",
        "        feature_representations.append(features(X).asnumpy())\n",
        "            \n",
        "        #if (e % (epochs//10)) == 0:\n",
        "        print(f\"Epoch {e}/{epochs} -- Loss: {cum_loss: .4f}\")\n",
        "        print(cum_preds)\n",
        "    return feature_representations\n",
        "\n",
        "def predict(model, X, nodes):\n",
        "    preds = model(X)[nodes].asnumpy().flatten()\n",
        "    return np.where(preds >= 0.5, 1, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbLbJ_EEAK-B",
        "outputId": "5cfbd292-526b-4fa5-b47f-42496c52ee9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 12.4 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQT76GXsYdLg",
        "outputId": "bec6ca19-d70f-4dff-db11-1e0ccad967b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[[1. 0. 0. ... 0. 0. 0.]\n",
              " [0. 1. 0. ... 0. 0. 0.]\n",
              " [0. 0. 1. ... 0. 0. 0.]\n",
              " ...\n",
              " [0. 0. 0. ... 1. 0. 0.]\n",
              " [0. 0. 0. ... 0. 1. 0.]\n",
              " [0. 0. 0. ... 0. 0. 1.]]\n",
              "<NDArray 34x34 @cpu(0)>"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 1 below: Feature is identity matrix\n",
        "\n",
        "This is considered as our baseline model."
      ],
      "metadata": {
        "id": "hIvbVox2j3I6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model_3, features_3 = build_model(nd.array(A), X_1)\n",
        "model_3(X_1)\n",
        "feature_representations_3= train(model_3, features_3, X_1, X_train, y_train, epochs=100)\n",
        "y_pred_3 = predict(model_3, X_1, X_test)\n",
        "print(classification_report(y_test, y_pred_3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8vhGDzBFjtP",
        "outputId": "ba4497bd-b64c-4665-bed0-fd4e182f40bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 -- Loss:  1.4558\n",
            "[0.5403239, 0.56839335]\n",
            "Epoch 2/100 -- Loss:  1.4558\n",
            "[0.5404112, 0.5684408]\n",
            "Epoch 3/100 -- Loss:  1.4557\n",
            "[0.5404445, 0.5684231]\n",
            "Epoch 4/100 -- Loss:  1.4555\n",
            "[0.5404238, 0.5683401]\n",
            "Epoch 5/100 -- Loss:  1.4553\n",
            "[0.54034925, 0.568192]\n",
            "Epoch 6/100 -- Loss:  1.4551\n",
            "[0.5402209, 0.56797886]\n",
            "Epoch 7/100 -- Loss:  1.4548\n",
            "[0.5400388, 0.567701]\n",
            "Epoch 8/100 -- Loss:  1.4544\n",
            "[0.5398033, 0.5673586]\n",
            "Epoch 9/100 -- Loss:  1.4540\n",
            "[0.5395146, 0.56695205]\n",
            "Epoch 10/100 -- Loss:  1.4535\n",
            "[0.53917295, 0.56648177]\n",
            "Epoch 11/100 -- Loss:  1.4530\n",
            "[0.5387787, 0.56594825]\n",
            "Epoch 12/100 -- Loss:  1.4525\n",
            "[0.53833246, 0.5653519]\n",
            "Epoch 13/100 -- Loss:  1.4519\n",
            "[0.5378345, 0.56469345]\n",
            "Epoch 14/100 -- Loss:  1.4513\n",
            "[0.53728557, 0.5639735]\n",
            "Epoch 15/100 -- Loss:  1.4506\n",
            "[0.536686, 0.5631928]\n",
            "Epoch 16/100 -- Loss:  1.4499\n",
            "[0.5360367, 0.562352]\n",
            "Epoch 17/100 -- Loss:  1.4491\n",
            "[0.5353381, 0.56145215]\n",
            "Epoch 18/100 -- Loss:  1.4484\n",
            "[0.5345912, 0.560494]\n",
            "Epoch 19/100 -- Loss:  1.4475\n",
            "[0.53379667, 0.5594786]\n",
            "Epoch 20/100 -- Loss:  1.4467\n",
            "[0.5329554, 0.5584069]\n",
            "Epoch 21/100 -- Loss:  1.4458\n",
            "[0.5320683, 0.55727994]\n",
            "Epoch 22/100 -- Loss:  1.4449\n",
            "[0.53113633, 0.556099]\n",
            "Epoch 23/100 -- Loss:  1.4440\n",
            "[0.53016055, 0.5548653]\n",
            "Epoch 24/100 -- Loss:  1.4430\n",
            "[0.5291419, 0.5535799]\n",
            "Epoch 25/100 -- Loss:  1.4420\n",
            "[0.5280816, 0.5522443]\n",
            "Epoch 26/100 -- Loss:  1.4410\n",
            "[0.5269808, 0.55085987]\n",
            "Epoch 27/100 -- Loss:  1.4400\n",
            "[0.52584064, 0.5494279]\n",
            "Epoch 28/100 -- Loss:  1.4390\n",
            "[0.52466244, 0.54795]\n",
            "Epoch 29/100 -- Loss:  1.4379\n",
            "[0.52344745, 0.54642767]\n",
            "Epoch 30/100 -- Loss:  1.4369\n",
            "[0.52219707, 0.54486245]\n",
            "Epoch 31/100 -- Loss:  1.4358\n",
            "[0.5209126, 0.54325604]\n",
            "Epoch 32/100 -- Loss:  1.4347\n",
            "[0.5195955, 0.54161006]\n",
            "Epoch 33/100 -- Loss:  1.4337\n",
            "[0.5182473, 0.5399263]\n",
            "Epoch 34/100 -- Loss:  1.4326\n",
            "[0.5168694, 0.5382065]\n",
            "Epoch 35/100 -- Loss:  1.4315\n",
            "[0.5154635, 0.5364525]\n",
            "Epoch 36/100 -- Loss:  1.4305\n",
            "[0.51403093, 0.53466606]\n",
            "Epoch 37/100 -- Loss:  1.4294\n",
            "[0.5125735, 0.53284925]\n",
            "Epoch 38/100 -- Loss:  1.4284\n",
            "[0.5110927, 0.5310039]\n",
            "Epoch 39/100 -- Loss:  1.4273\n",
            "[0.5095904, 0.529132]\n",
            "Epoch 40/100 -- Loss:  1.4263\n",
            "[0.50806814, 0.5272355]\n",
            "Epoch 41/100 -- Loss:  1.4253\n",
            "[0.5065276, 0.52531654]\n",
            "Epoch 42/100 -- Loss:  1.4243\n",
            "[0.50497067, 0.52337706]\n",
            "Epoch 43/100 -- Loss:  1.4233\n",
            "[0.5033991, 0.5214192]\n",
            "Epoch 44/100 -- Loss:  1.4223\n",
            "[0.50181454, 0.51944506]\n",
            "Epoch 45/100 -- Loss:  1.4214\n",
            "[0.5002189, 0.51745677]\n",
            "Epoch 46/100 -- Loss:  1.4205\n",
            "[0.49861398, 0.5154565]\n",
            "Epoch 47/100 -- Loss:  1.4196\n",
            "[0.49700162, 0.5134463]\n",
            "Epoch 48/100 -- Loss:  1.4187\n",
            "[0.4953836, 0.51142836]\n",
            "Epoch 49/100 -- Loss:  1.4178\n",
            "[0.4937619, 0.50940484]\n",
            "Epoch 50/100 -- Loss:  1.4170\n",
            "[0.4921382, 0.507378]\n",
            "Epoch 51/100 -- Loss:  1.4162\n",
            "[0.4905145, 0.50534993]\n",
            "Epoch 52/100 -- Loss:  1.4154\n",
            "[0.4888926, 0.5033228]\n",
            "Epoch 53/100 -- Loss:  1.4147\n",
            "[0.48727438, 0.5012987]\n",
            "Epoch 54/100 -- Loss:  1.4140\n",
            "[0.48566163, 0.49928007]\n",
            "Epoch 55/100 -- Loss:  1.4133\n",
            "[0.4840562, 0.49726865]\n",
            "Epoch 56/100 -- Loss:  1.4126\n",
            "[0.48246, 0.4952669]\n",
            "Epoch 57/100 -- Loss:  1.4119\n",
            "[0.48087475, 0.4932768]\n",
            "Epoch 58/100 -- Loss:  1.4113\n",
            "[0.47930235, 0.49130043]\n",
            "Epoch 59/100 -- Loss:  1.4107\n",
            "[0.47774458, 0.4893399]\n",
            "Epoch 60/100 -- Loss:  1.4102\n",
            "[0.4762032, 0.48739722]\n",
            "Epoch 61/100 -- Loss:  1.4096\n",
            "[0.47467992, 0.48547444]\n",
            "Epoch 62/100 -- Loss:  1.4091\n",
            "[0.47317654, 0.48357353]\n",
            "Epoch 63/100 -- Loss:  1.4086\n",
            "[0.47169474, 0.48169646]\n",
            "Epoch 64/100 -- Loss:  1.4081\n",
            "[0.47023618, 0.4798451]\n",
            "Epoch 65/100 -- Loss:  1.4077\n",
            "[0.4688025, 0.4780215]\n",
            "Epoch 66/100 -- Loss:  1.4073\n",
            "[0.46739545, 0.47622725]\n",
            "Epoch 67/100 -- Loss:  1.4069\n",
            "[0.46601647, 0.4744644]\n",
            "Epoch 68/100 -- Loss:  1.4065\n",
            "[0.46466723, 0.47273454]\n",
            "Epoch 69/100 -- Loss:  1.4061\n",
            "[0.4633492, 0.47103947]\n",
            "Epoch 70/100 -- Loss:  1.4058\n",
            "[0.46206382, 0.46938077]\n",
            "Epoch 71/100 -- Loss:  1.4054\n",
            "[0.46081266, 0.46776018]\n",
            "Epoch 72/100 -- Loss:  1.4051\n",
            "[0.45959702, 0.46617922]\n",
            "Epoch 73/100 -- Loss:  1.4048\n",
            "[0.45841834, 0.4646394]\n",
            "Epoch 74/100 -- Loss:  1.4045\n",
            "[0.45727792, 0.4631422]\n",
            "Epoch 75/100 -- Loss:  1.4042\n",
            "[0.456177, 0.46168903]\n",
            "Epoch 76/100 -- Loss:  1.4039\n",
            "[0.45511693, 0.46028122]\n",
            "Epoch 77/100 -- Loss:  1.4036\n",
            "[0.4540988, 0.4589202]\n",
            "Epoch 78/100 -- Loss:  1.4034\n",
            "[0.45312384, 0.4576071]\n",
            "Epoch 79/100 -- Loss:  1.4031\n",
            "[0.45219305, 0.45634308]\n",
            "Epoch 80/100 -- Loss:  1.4028\n",
            "[0.45130762, 0.45512944]\n",
            "Epoch 81/100 -- Loss:  1.4025\n",
            "[0.45046848, 0.45396712]\n",
            "Epoch 82/100 -- Loss:  1.4023\n",
            "[0.44967663, 0.45285726]\n",
            "Epoch 83/100 -- Loss:  1.4020\n",
            "[0.44893292, 0.45180064]\n",
            "Epoch 84/100 -- Loss:  1.4017\n",
            "[0.44823828, 0.4507984]\n",
            "Epoch 85/100 -- Loss:  1.4014\n",
            "[0.44759348, 0.4498512]\n",
            "Epoch 86/100 -- Loss:  1.4011\n",
            "[0.44699925, 0.44895992]\n",
            "Epoch 87/100 -- Loss:  1.4008\n",
            "[0.44645637, 0.4481253]\n",
            "Epoch 88/100 -- Loss:  1.4005\n",
            "[0.44596553, 0.447348]\n",
            "Epoch 89/100 -- Loss:  1.4002\n",
            "[0.4455272, 0.44662857]\n",
            "Epoch 90/100 -- Loss:  1.3999\n",
            "[0.44514203, 0.4459676]\n",
            "Epoch 91/100 -- Loss:  1.3996\n",
            "[0.44481048, 0.44536564]\n",
            "Epoch 92/100 -- Loss:  1.3992\n",
            "[0.4445331, 0.44482303]\n",
            "Epoch 93/100 -- Loss:  1.3988\n",
            "[0.44431016, 0.4443402]\n",
            "Epoch 94/100 -- Loss:  1.3984\n",
            "[0.4441421, 0.44391742]\n",
            "Epoch 95/100 -- Loss:  1.3981\n",
            "[0.44402912, 0.443555]\n",
            "Epoch 96/100 -- Loss:  1.3976\n",
            "[0.44397154, 0.4432532]\n",
            "Epoch 97/100 -- Loss:  1.3972\n",
            "[0.44396958, 0.44301197]\n",
            "Epoch 98/100 -- Loss:  1.3968\n",
            "[0.44402325, 0.44283158]\n",
            "Epoch 99/100 -- Loss:  1.3963\n",
            "[0.44413272, 0.44271192]\n",
            "Epoch 100/100 -- Loss:  1.3958\n",
            "[0.44429794, 0.44265303]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.50      1.00      0.67        16\n",
            "        True       0.00      0.00      0.00        16\n",
            "\n",
            "    accuracy                           0.50        32\n",
            "   macro avg       0.25      0.50      0.33        32\n",
            "weighted avg       0.25      0.50      0.33        32\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model_3, features_3 = build_model(nd.array(A), X_1)\n",
        "model_3(X_1)\n",
        "feature_representations_3= train(model_3, features_3, X_1, X_train, y_train, epochs=5000)\n",
        "y_pred_3 = predict(model_3, X_1, X_test)\n",
        "print(classification_report(y_test, y_pred_3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfz-R1ABXjUu",
        "outputId": "08c8533d-b52f-4dcf-f960-76d612a0e192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500/5000 -- Loss:  0.0007\n",
            "[0.9997912, 0.000512959]\n",
            "Epoch 1000/5000 -- Loss:  0.0000\n",
            "[1.0, 8.9488424e-08]\n",
            "Epoch 1500/5000 -- Loss:  0.0000\n",
            "[1.0, 1.6404622e-11]\n",
            "Epoch 2000/5000 -- Loss:  0.0000\n",
            "[1.0, 3.013723e-15]\n",
            "Epoch 2500/5000 -- Loss:  0.0000\n",
            "[1.0, 5.533144e-19]\n",
            "Epoch 3000/5000 -- Loss:  0.0000\n",
            "[1.0, 1.0126335e-22]\n",
            "Epoch 3500/5000 -- Loss:  0.0000\n",
            "[1.0, 1.8532443e-26]\n",
            "Epoch 4000/5000 -- Loss:  0.0000\n",
            "[1.0, 3.3916653e-30]\n",
            "Epoch 4500/5000 -- Loss:  0.0000\n",
            "[1.0, 6.2071662e-34]\n",
            "Epoch 5000/5000 -- Loss:  0.0000\n",
            "[1.0, 1.1359879e-37]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.40      0.25      0.31        16\n",
            "        True       0.45      0.62      0.53        16\n",
            "\n",
            "    accuracy                           0.44        32\n",
            "   macro avg       0.43      0.44      0.42        32\n",
            "weighted avg       0.43      0.44      0.42        32\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ksG9jv_r8mgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding number of cycles"
      ],
      "metadata": {
        "id": "D0V2-NcKjn8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cycle3_1 = []\n",
        "\n",
        "for i in range(0,34):\n",
        "  for j in range(0,34):\n",
        "    if(A[i][j]>0): #if edge exists\n",
        "      for k in range(0,34):\n",
        "        if(i!=k and A[j][k]>0 and A[k][i]>0):  #if other two edges exist\n",
        "          if([i,j,k] not in cycle3_1 and [j,i,k] not in cycle3_1 and [k,i,j] not in cycle3_1 and [i,k,j] not in cycle3_1 and [j,k,i] not in cycle3_1 and [k,j,i] not in cycle3_1):\n",
        "            cycle3_1.append([i,j,k])\n",
        "            #print(characters[i],characters[j],characters[k]);"
      ],
      "metadata": {
        "id": "-apu7ziRAlDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cycle3_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcekNq6IQQ6I",
        "outputId": "a21a5d2e-780b-4f57-b8df-1883eb127889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cycle3_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mb_4fsN8Cf8r",
        "outputId": "321771db-e2ff-4834-9071-de49a5639c0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 1, 2],\n",
              " [0, 1, 3],\n",
              " [0, 1, 7],\n",
              " [0, 1, 12],\n",
              " [0, 1, 13],\n",
              " [0, 1, 14],\n",
              " [0, 1, 15],\n",
              " [0, 2, 3],\n",
              " [0, 2, 7],\n",
              " [0, 2, 8],\n",
              " [0, 2, 12],\n",
              " [0, 3, 7],\n",
              " [0, 3, 11],\n",
              " [0, 3, 12],\n",
              " [0, 4, 6],\n",
              " [0, 4, 9],\n",
              " [0, 5, 6],\n",
              " [0, 5, 9],\n",
              " [1, 2, 3],\n",
              " [1, 2, 7],\n",
              " [1, 2, 12],\n",
              " [1, 3, 7],\n",
              " [1, 3, 12],\n",
              " [2, 3, 7],\n",
              " [2, 3, 12],\n",
              " [2, 8, 21],\n",
              " [5, 6, 22],\n",
              " [8, 17, 21],\n",
              " [8, 17, 23],\n",
              " [8, 21, 23],\n",
              " [16, 20, 23],\n",
              " [16, 21, 23],\n",
              " [16, 30, 32],\n",
              " [17, 21, 23],\n",
              " [19, 23, 29],\n",
              " [21, 23, 24],\n",
              " [21, 23, 25],\n",
              " [21, 23, 26],\n",
              " [21, 23, 27],\n",
              " [21, 23, 28],\n",
              " [21, 23, 29],\n",
              " [21, 23, 31],\n",
              " [21, 29, 31],\n",
              " [23, 29, 31],\n",
              " [23, 31, 33]]"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cycle6_1 = []\n",
        "\n",
        "for i in range(0,34):\n",
        "  for j in range(0,34):\n",
        "    if(A[i][j]>0): #edge btw i and j\n",
        "      for k in range(0,34):\n",
        "        if(i!=k and A[j][k]>0): #edge btw j and k\n",
        "          for l in range(0,34):\n",
        "            if(i!=l and j!=l and A[k][l]>0): #edge between l and k\n",
        "              for m in range(0,34):\n",
        "                if(i!=m and j!=m and k!=m and A[l][m]>0): #edge between l and m\n",
        "                  for n in range(0,34):\n",
        "                    if(i!=n and j!=n and k!=n and l!=n and A[m][n]>0 and A[n][i]>0): #edge between m and n and edge btw n and i to complete the cycle\n",
        "                      whatever = []\n",
        "                      whatever.append(i)\n",
        "                      whatever.append(j)\n",
        "                      whatever.append(k)\n",
        "                      whatever.append(l)\n",
        "                      whatever.append(m)\n",
        "                      whatever.append(n)\n",
        "                      whatever.sort()\n",
        "                      if(whatever not in cycle6_1):\n",
        "                        cycle6_1.append(whatever)\n",
        "\n",
        "                      # print(characters[i],characters[j],characters[k],characters[l],characters[m],characters[n])\n",
        "                  "
      ],
      "metadata": {
        "id": "4773GMK-QR56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cycle6_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5MgGP94QT7v",
        "outputId": "11fc0832-4214-4eab-e71e-cfb583ebed7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "672"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The features being added are:\n",
        "\n",
        "For each node, the number of cycles it appears in."
      ],
      "metadata": {
        "id": "gfSZY5hejlSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_3 = nd.zeros((A.shape[0], 2))\n",
        "for i in cycle3_1:\n",
        "  for node in i:\n",
        "     X_3[node][0]+=1\n",
        "for i in cycle6_1:\n",
        "  for node in i:\n",
        "     X_3[node][1]+=1"
      ],
      "metadata": {
        "id": "5j1nOyLQRDbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97OW4_sQR19_",
        "outputId": "f915ae14-1a11-4efe-faa1-ebcd9084a7ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[[ 18. 333.]\n",
              " [ 12. 269.]\n",
              " [ 11. 346.]\n",
              " [ 10. 133.]\n",
              " [  2.   1.]\n",
              " [  3.   1.]\n",
              " [  3.   1.]\n",
              " [  6.  96.]\n",
              " [  5. 201.]\n",
              " [  2.   1.]\n",
              " [  0.   0.]\n",
              " [  1.  23.]\n",
              " [  6. 189.]\n",
              " [  1.  26.]\n",
              " [  1. 125.]\n",
              " [  1.  26.]\n",
              " [  3. 146.]\n",
              " [  3. 156.]\n",
              " [  0.  40.]\n",
              " [  1.  40.]\n",
              " [  1.  73.]\n",
              " [ 13. 227.]\n",
              " [  1.   1.]\n",
              " [ 15. 366.]\n",
              " [  1.  21.]\n",
              " [  1.  21.]\n",
              " [  1.  21.]\n",
              " [  1.  21.]\n",
              " [  1.   0.]\n",
              " [  4.   0.]\n",
              " [  1.   0.]\n",
              " [  4.   0.]\n",
              " [  1.   0.]\n",
              " [  1.   0.]]\n",
              "<NDArray 34x2 @cpu(0)>"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model using the above feature while concatenating it with the identity matrix"
      ],
      "metadata": {
        "id": "Naan2ydOlAWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#X_3 = nd.concat(X_1,X_2,X_3)\n",
        "X_4=nd.concat(X_1,X_3)\n",
        "model_3, features_3 = build_model(nd.array(A), X_4)\n",
        "model_3(X_4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_17tDi4NSbjC",
        "outputId": "42b25b88-eda0-4593-d2c1-cfccc065880f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[[0.5179667 ]\n",
              " [0.5223833 ]\n",
              " [0.5214108 ]\n",
              " [0.5269411 ]\n",
              " [0.5141662 ]\n",
              " [0.5109241 ]\n",
              " [0.5129572 ]\n",
              " [0.53293055]\n",
              " [0.5294506 ]\n",
              " [0.5120415 ]\n",
              " [0.50494385]\n",
              " [0.5464045 ]\n",
              " [0.5294506 ]\n",
              " [0.54638517]\n",
              " [0.5380643 ]\n",
              " [0.5463878 ]\n",
              " [0.5269411 ]\n",
              " [0.5329305 ]\n",
              " [0.5463426 ]\n",
              " [0.5329305 ]\n",
              " [0.5380643 ]\n",
              " [0.51991165]\n",
              " [0.50941384]\n",
              " [0.517615  ]\n",
              " [0.54641277]\n",
              " [0.5464009 ]\n",
              " [0.5464096 ]\n",
              " [0.54641575]\n",
              " [0.546399  ]\n",
              " [0.52945065]\n",
              " [0.5381005 ]\n",
              " [0.53293407]\n",
              " [0.53810287]\n",
              " [0.54674023]]\n",
              "<NDArray 34x1 @cpu(0)>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_representations_3= train(model_3, features_3, X_4, X_train, y_train, epochs=100)\n",
        "y_pred_3 = predict(model_3, X_4, X_test)\n",
        "print(classification_report(y_test, y_pred_3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpOnS7UgFvdH",
        "outputId": "54b1ce5e-6c3c-44ea-847b-ce5a606effdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 -- Loss:  1.1419\n",
            "[0.39946833, 0.2009331]\n",
            "Epoch 2/100 -- Loss:  1.1416\n",
            "[0.3997139, 0.2011554]\n",
            "Epoch 3/100 -- Loss:  1.1409\n",
            "[0.40014485, 0.20147967]\n",
            "Epoch 4/100 -- Loss:  1.1399\n",
            "[0.40076095, 0.20190589]\n",
            "Epoch 5/100 -- Loss:  1.1386\n",
            "[0.40156206, 0.20243378]\n",
            "Epoch 6/100 -- Loss:  1.1369\n",
            "[0.40254772, 0.20306318]\n",
            "Epoch 7/100 -- Loss:  1.1349\n",
            "[0.40371758, 0.20379375]\n",
            "Epoch 8/100 -- Loss:  1.1326\n",
            "[0.4050711, 0.20462507]\n",
            "Epoch 9/100 -- Loss:  1.1300\n",
            "[0.40660775, 0.20555675]\n",
            "Epoch 10/100 -- Loss:  1.1271\n",
            "[0.40832677, 0.20658816]\n",
            "Epoch 11/100 -- Loss:  1.1239\n",
            "[0.41022736, 0.20771869]\n",
            "Epoch 12/100 -- Loss:  1.1204\n",
            "[0.4123086, 0.20894748]\n",
            "Epoch 13/100 -- Loss:  1.1166\n",
            "[0.4145694, 0.21027367]\n",
            "Epoch 14/100 -- Loss:  1.1125\n",
            "[0.4170087, 0.21169624]\n",
            "Epoch 15/100 -- Loss:  1.1082\n",
            "[0.4196251, 0.21321395]\n",
            "Epoch 16/100 -- Loss:  1.1036\n",
            "[0.42241704, 0.21482542]\n",
            "Epoch 17/100 -- Loss:  1.0988\n",
            "[0.42538294, 0.21652904]\n",
            "Epoch 18/100 -- Loss:  1.0937\n",
            "[0.428521, 0.2183231]\n",
            "Epoch 19/100 -- Loss:  1.0885\n",
            "[0.43182912, 0.22020549]\n",
            "Epoch 20/100 -- Loss:  1.0830\n",
            "[0.43530512, 0.22217394]\n",
            "Epoch 21/100 -- Loss:  1.0773\n",
            "[0.43894646, 0.22422594]\n",
            "Epoch 22/100 -- Loss:  1.0714\n",
            "[0.4427505, 0.22635853]\n",
            "Epoch 23/100 -- Loss:  1.0653\n",
            "[0.44671446, 0.22856854]\n",
            "Epoch 24/100 -- Loss:  1.0591\n",
            "[0.45083502, 0.23085241]\n",
            "Epoch 25/100 -- Loss:  1.0528\n",
            "[0.45510894, 0.23320606]\n",
            "Epoch 26/100 -- Loss:  1.0462\n",
            "[0.45953253, 0.2356252]\n",
            "Epoch 27/100 -- Loss:  1.0396\n",
            "[0.46410173, 0.23810485]\n",
            "Epoch 28/100 -- Loss:  1.0328\n",
            "[0.4688125, 0.2406396]\n",
            "Epoch 29/100 -- Loss:  1.0260\n",
            "[0.47366023, 0.24322358]\n",
            "Epoch 30/100 -- Loss:  1.0190\n",
            "[0.47864032, 0.24585019]\n",
            "Epoch 31/100 -- Loss:  1.0119\n",
            "[0.48374754, 0.2485123]\n",
            "Epoch 32/100 -- Loss:  1.0047\n",
            "[0.48897666, 0.25120205]\n",
            "Epoch 33/100 -- Loss:  0.9975\n",
            "[0.49432194, 0.2539109]\n",
            "Epoch 34/100 -- Loss:  0.9902\n",
            "[0.49977735, 0.25662965]\n",
            "Epoch 35/100 -- Loss:  0.9828\n",
            "[0.50533664, 0.25934815]\n",
            "Epoch 36/100 -- Loss:  0.9753\n",
            "[0.51099294, 0.2620557]\n",
            "Epoch 37/100 -- Loss:  0.9677\n",
            "[0.5167391, 0.26474068]\n",
            "Epoch 38/100 -- Loss:  0.9601\n",
            "[0.52256703, 0.26739088]\n",
            "Epoch 39/100 -- Loss:  0.9525\n",
            "[0.5284683, 0.26999348]\n",
            "Epoch 40/100 -- Loss:  0.9447\n",
            "[0.5344329, 0.27253518]\n",
            "Epoch 41/100 -- Loss:  0.9369\n",
            "[0.5404493, 0.27500263]\n",
            "Epoch 42/100 -- Loss:  0.9291\n",
            "[0.5465034, 0.27738273]\n",
            "Epoch 43/100 -- Loss:  0.9212\n",
            "[0.5525773, 0.27966332]\n",
            "Epoch 44/100 -- Loss:  0.9133\n",
            "[0.55864704, 0.2818341]\n",
            "Epoch 45/100 -- Loss:  0.9054\n",
            "[0.56467974, 0.28388757]\n",
            "Epoch 46/100 -- Loss:  0.8976\n",
            "[0.5706302, 0.28582078]\n",
            "Epoch 47/100 -- Loss:  0.8901\n",
            "[0.57643706, 0.2876368]\n",
            "Epoch 48/100 -- Loss:  0.8828\n",
            "[0.5820233, 0.28934628]\n",
            "Epoch 49/100 -- Loss:  0.8761\n",
            "[0.5873069, 0.29096797]\n",
            "Epoch 50/100 -- Loss:  0.8699\n",
            "[0.5922286, 0.29252708]\n",
            "Epoch 51/100 -- Loss:  0.8644\n",
            "[0.59679466, 0.2940508]\n",
            "Epoch 52/100 -- Loss:  0.8593\n",
            "[0.6011064, 0.29556167]\n",
            "Epoch 53/100 -- Loss:  0.8545\n",
            "[0.60533875, 0.29707152]\n",
            "Epoch 54/100 -- Loss:  0.8495\n",
            "[0.6096731, 0.2985799]\n",
            "Epoch 55/100 -- Loss:  0.8442\n",
            "[0.6142356, 0.30007666]\n",
            "Epoch 56/100 -- Loss:  0.8384\n",
            "[0.61907804, 0.30154696]\n",
            "Epoch 57/100 -- Loss:  0.8322\n",
            "[0.6241942, 0.30297634]\n",
            "Epoch 58/100 -- Loss:  0.8257\n",
            "[0.62954503, 0.30435342]\n",
            "Epoch 59/100 -- Loss:  0.8188\n",
            "[0.6350776, 0.3056711]\n",
            "Epoch 60/100 -- Loss:  0.8118\n",
            "[0.6407376, 0.3069266]\n",
            "Epoch 61/100 -- Loss:  0.8046\n",
            "[0.646475, 0.30812007]\n",
            "Epoch 62/100 -- Loss:  0.7973\n",
            "[0.65224725, 0.30925348]\n",
            "Epoch 63/100 -- Loss:  0.7901\n",
            "[0.65801984, 0.3103292]\n",
            "Epoch 64/100 -- Loss:  0.7828\n",
            "[0.66376597, 0.31134912]\n",
            "Epoch 65/100 -- Loss:  0.7757\n",
            "[0.66946507, 0.31231365]\n",
            "Epoch 66/100 -- Loss:  0.7686\n",
            "[0.6751017, 0.31322122]\n",
            "Epoch 67/100 -- Loss:  0.7617\n",
            "[0.6806644, 0.31406844]\n",
            "Epoch 68/100 -- Loss:  0.7548\n",
            "[0.6861444, 0.31484994]\n",
            "Epoch 69/100 -- Loss:  0.7480\n",
            "[0.69153494, 0.31555837]\n",
            "Epoch 70/100 -- Loss:  0.7413\n",
            "[0.6968307, 0.31618506]\n",
            "Epoch 71/100 -- Loss:  0.7346\n",
            "[0.7020273, 0.31672034]\n",
            "Epoch 72/100 -- Loss:  0.7280\n",
            "[0.7071212, 0.31715363]\n",
            "Epoch 73/100 -- Loss:  0.7215\n",
            "[0.71210945, 0.31747422]\n",
            "Epoch 74/100 -- Loss:  0.7149\n",
            "[0.71698946, 0.3176711]\n",
            "Epoch 75/100 -- Loss:  0.7084\n",
            "[0.72175914, 0.31773356]\n",
            "Epoch 76/100 -- Loss:  0.7018\n",
            "[0.7264166, 0.3176514]\n",
            "Epoch 77/100 -- Loss:  0.6953\n",
            "[0.7309605, 0.3174149]\n",
            "Epoch 78/100 -- Loss:  0.6886\n",
            "[0.7353897, 0.31701508]\n",
            "Epoch 79/100 -- Loss:  0.6820\n",
            "[0.7397032, 0.31644368]\n",
            "Epoch 80/100 -- Loss:  0.6752\n",
            "[0.7439004, 0.31569347]\n",
            "Epoch 81/100 -- Loss:  0.6684\n",
            "[0.7479811, 0.31475794]\n",
            "Epoch 82/100 -- Loss:  0.6614\n",
            "[0.7519451, 0.3136315]\n",
            "Epoch 83/100 -- Loss:  0.6544\n",
            "[0.75579244, 0.3123097]\n",
            "Epoch 84/100 -- Loss:  0.6473\n",
            "[0.75952363, 0.31078872]\n",
            "Epoch 85/100 -- Loss:  0.6400\n",
            "[0.763139, 0.30906585]\n",
            "Epoch 86/100 -- Loss:  0.6327\n",
            "[0.7666396, 0.30713925]\n",
            "Epoch 87/100 -- Loss:  0.6252\n",
            "[0.7700261, 0.30500793]\n",
            "Epoch 88/100 -- Loss:  0.6176\n",
            "[0.7732997, 0.30267185]\n",
            "Epoch 89/100 -- Loss:  0.6099\n",
            "[0.77646154, 0.30013183]\n",
            "Epoch 90/100 -- Loss:  0.6020\n",
            "[0.77951324, 0.2973895]\n",
            "Epoch 91/100 -- Loss:  0.5941\n",
            "[0.7824562, 0.29444745]\n",
            "Epoch 92/100 -- Loss:  0.5860\n",
            "[0.7852921, 0.29130897]\n",
            "Epoch 93/100 -- Loss:  0.5779\n",
            "[0.78802264, 0.2879783]\n",
            "Epoch 94/100 -- Loss:  0.5696\n",
            "[0.79064983, 0.28446022]\n",
            "Epoch 95/100 -- Loss:  0.5613\n",
            "[0.7931756, 0.28076056]\n",
            "Epoch 96/100 -- Loss:  0.5528\n",
            "[0.7956018, 0.27688563]\n",
            "Epoch 97/100 -- Loss:  0.5443\n",
            "[0.7979308, 0.27284268]\n",
            "Epoch 98/100 -- Loss:  0.5358\n",
            "[0.8001646, 0.26863933]\n",
            "Epoch 99/100 -- Loss:  0.5272\n",
            "[0.8023056, 0.26428404]\n",
            "Epoch 100/100 -- Loss:  0.5185\n",
            "[0.80435604, 0.2597859]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.71      0.75      0.73        16\n",
            "        True       0.73      0.69      0.71        16\n",
            "\n",
            "    accuracy                           0.72        32\n",
            "   macro avg       0.72      0.72      0.72        32\n",
            "weighted avg       0.72      0.72      0.72        32\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_representations_3= train(model_3, features_3, X_4, X_train, y_train, epochs=5000)\n",
        "y_pred_3 = predict(model_3, X_4, X_test)\n",
        "print(classification_report(y_test, y_pred_3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFg2a1vVSjFD",
        "outputId": "2a9163fc-9800-4ff1-e03b-673a21caba93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500/5000 -- Loss:  0.0036\n",
            "[0.9977749, 0.0013769849]\n",
            "Epoch 1000/5000 -- Loss:  0.0000\n",
            "[0.99999857, 3.8284517e-07]\n",
            "Epoch 1500/5000 -- Loss:  0.0000\n",
            "[1.0, 2.5712688e-10]\n",
            "Epoch 2000/5000 -- Loss:  0.0000\n",
            "[1.0, 1.0162764e-12]\n",
            "Epoch 2500/5000 -- Loss:  0.0000\n",
            "[1.0, 1.4710156e-13]\n",
            "Epoch 3000/5000 -- Loss:  0.0000\n",
            "[1.0, 2.1435366e-11]\n",
            "Epoch 3500/5000 -- Loss:  0.0002\n",
            "[1.0, 0.00018141394]\n",
            "Epoch 4000/5000 -- Loss:  0.0000\n",
            "[1.0, 3.970256e-14]\n",
            "Epoch 4500/5000 -- Loss:  0.0000\n",
            "[1.0, 5.0177972e-23]\n",
            "Epoch 5000/5000 -- Loss:  0.0000\n",
            "[1.0, 1.5884293e-30]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.67      0.88      0.76        16\n",
            "        True       0.82      0.56      0.67        16\n",
            "\n",
            "    accuracy                           0.72        32\n",
            "   macro avg       0.74      0.72      0.71        32\n",
            "weighted avg       0.74      0.72      0.71        32\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below the feature being added is: \n",
        "\n",
        "For each node, 0/1 depending if it appeared in a cycle or not"
      ],
      "metadata": {
        "id": "_h8IvIPqT1yA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_5 = nd.zeros((A.shape[0], 2))\n",
        "for i in cycle3_1:\n",
        "  for node in i:\n",
        "     X_5[node][0]=1\n",
        "for i in cycle6_1:\n",
        "  for node in i:\n",
        "     X_5[node][1]=1\n",
        "X_5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HemK3ix9TpiC",
        "outputId": "04ebfa84-d2c7-4352-8771-ec58d62e696a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[[1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [0. 0.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [0. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]\n",
              " [1. 1.]]\n",
              "<NDArray 34x2 @cpu(0)>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model for the above feature and concatenating it with the identity matrix"
      ],
      "metadata": {
        "id": "DNwKTW-DlItu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#X_6=X_5\n",
        "X_6=nd.concat(X_1,X_5)\n",
        "model_3, features_3 = build_model(nd.array(A), X_6)\n",
        "print(model_3(X_6))\n",
        "feature_representations_3= train(model_3, features_3, X_6, X_train, y_train, epochs=100)\n",
        "y_pred_3 = predict(model_3, X_6, X_test)\n",
        "print(classification_report(y_test, y_pred_3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqeFV2LEF1ox",
        "outputId": "2b1ed77c-af62-43f0-ae18-2a9c28a0e8ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[[0.32382098]\n",
            " [0.32509613]\n",
            " [0.3239554 ]\n",
            " [0.32343668]\n",
            " [0.32153335]\n",
            " [0.32628986]\n",
            " [0.32521373]\n",
            " [0.3295564 ]\n",
            " [0.32714784]\n",
            " [0.3291595 ]\n",
            " [0.35417828]\n",
            " [0.32060406]\n",
            " [0.32484898]\n",
            " [0.3165659 ]\n",
            " [0.31973585]\n",
            " [0.3316088 ]\n",
            " [0.3253776 ]\n",
            " [0.32660082]\n",
            " [0.32276508]\n",
            " [0.32080096]\n",
            " [0.3244985 ]\n",
            " [0.32381943]\n",
            " [0.3307243 ]\n",
            " [0.3234142 ]\n",
            " [0.3285477 ]\n",
            " [0.3226679 ]\n",
            " [0.32803047]\n",
            " [0.3344856 ]\n",
            " [0.3313096 ]\n",
            " [0.32524735]\n",
            " [0.32105455]\n",
            " [0.32085162]\n",
            " [0.32664686]\n",
            " [0.3240618 ]]\n",
            "<NDArray 34x1 @cpu(0)>\n",
            "Epoch 1/100 -- Loss:  1.5194\n",
            "[0.32382098, 0.3242102]\n",
            "Epoch 2/100 -- Loss:  1.5192\n",
            "[0.32404602, 0.32450458]\n",
            "Epoch 3/100 -- Loss:  1.5187\n",
            "[0.32442504, 0.32494494]\n",
            "Epoch 4/100 -- Loss:  1.5179\n",
            "[0.32495797, 0.3255311]\n",
            "Epoch 5/100 -- Loss:  1.5169\n",
            "[0.32564473, 0.32626295]\n",
            "Epoch 6/100 -- Loss:  1.5156\n",
            "[0.32648522, 0.32714036]\n",
            "Epoch 7/100 -- Loss:  1.5141\n",
            "[0.32747933, 0.328163]\n",
            "Epoch 8/100 -- Loss:  1.5123\n",
            "[0.32862693, 0.32933065]\n",
            "Epoch 9/100 -- Loss:  1.5103\n",
            "[0.3299278, 0.330643]\n",
            "Epoch 10/100 -- Loss:  1.5081\n",
            "[0.33138174, 0.33209965]\n",
            "Epoch 11/100 -- Loss:  1.5057\n",
            "[0.33298847, 0.33370012]\n",
            "Epoch 12/100 -- Loss:  1.5030\n",
            "[0.3347476, 0.33544385]\n",
            "Epoch 13/100 -- Loss:  1.5002\n",
            "[0.3366588, 0.33733028]\n",
            "Epoch 14/100 -- Loss:  1.4971\n",
            "[0.3387215, 0.33935866]\n",
            "Epoch 15/100 -- Loss:  1.4939\n",
            "[0.3409353, 0.34152818]\n",
            "Epoch 16/100 -- Loss:  1.4905\n",
            "[0.34329942, 0.34383798]\n",
            "Epoch 17/100 -- Loss:  1.4869\n",
            "[0.34581316, 0.34628692]\n",
            "Epoch 18/100 -- Loss:  1.4832\n",
            "[0.3484756, 0.3488739]\n",
            "Epoch 19/100 -- Loss:  1.4794\n",
            "[0.35128582, 0.3515977]\n",
            "Epoch 20/100 -- Loss:  1.4754\n",
            "[0.3542426, 0.35445678]\n",
            "Epoch 21/100 -- Loss:  1.4714\n",
            "[0.3573448, 0.3574495]\n",
            "Epoch 22/100 -- Loss:  1.4672\n",
            "[0.3605909, 0.36057416]\n",
            "Epoch 23/100 -- Loss:  1.4629\n",
            "[0.3639793, 0.36382878]\n",
            "Epoch 24/100 -- Loss:  1.4586\n",
            "[0.36750832, 0.36721128]\n",
            "Epoch 25/100 -- Loss:  1.4543\n",
            "[0.37117583, 0.37071922]\n",
            "Epoch 26/100 -- Loss:  1.4498\n",
            "[0.37497976, 0.3743502]\n",
            "Epoch 27/100 -- Loss:  1.4454\n",
            "[0.37891778, 0.37810132]\n",
            "Epoch 28/100 -- Loss:  1.4410\n",
            "[0.38298726, 0.38196972]\n",
            "Epoch 29/100 -- Loss:  1.4365\n",
            "[0.38718528, 0.38595214]\n",
            "Epoch 30/100 -- Loss:  1.4321\n",
            "[0.39150885, 0.39004517]\n",
            "Epoch 31/100 -- Loss:  1.4277\n",
            "[0.39595464, 0.39424515]\n",
            "Epoch 32/100 -- Loss:  1.4234\n",
            "[0.40051916, 0.3985482]\n",
            "Epoch 33/100 -- Loss:  1.4191\n",
            "[0.40519854, 0.40295008]\n",
            "Epoch 34/100 -- Loss:  1.4149\n",
            "[0.4099887, 0.4074465]\n",
            "Epoch 35/100 -- Loss:  1.4108\n",
            "[0.4148854, 0.41203275]\n",
            "Epoch 36/100 -- Loss:  1.4068\n",
            "[0.4198841, 0.416704]\n",
            "Epoch 37/100 -- Loss:  1.4030\n",
            "[0.42497987, 0.42145506]\n",
            "Epoch 38/100 -- Loss:  1.3992\n",
            "[0.43016773, 0.42628068]\n",
            "Epoch 39/100 -- Loss:  1.3956\n",
            "[0.4354423, 0.43117526]\n",
            "Epoch 40/100 -- Loss:  1.3921\n",
            "[0.4407982, 0.436133]\n",
            "Epoch 41/100 -- Loss:  1.3888\n",
            "[0.44622943, 0.4411479]\n",
            "Epoch 42/100 -- Loss:  1.3856\n",
            "[0.45173025, 0.44621375]\n",
            "Epoch 43/100 -- Loss:  1.3827\n",
            "[0.45729432, 0.4513243]\n",
            "Epoch 44/100 -- Loss:  1.3799\n",
            "[0.46291536, 0.4564729]\n",
            "Epoch 45/100 -- Loss:  1.3773\n",
            "[0.46858683, 0.4616529]\n",
            "Epoch 46/100 -- Loss:  1.3749\n",
            "[0.47430208, 0.46685752]\n",
            "Epoch 47/100 -- Loss:  1.3727\n",
            "[0.48005423, 0.47207975]\n",
            "Epoch 48/100 -- Loss:  1.3707\n",
            "[0.48583648, 0.47731256]\n",
            "Epoch 49/100 -- Loss:  1.3688\n",
            "[0.4916417, 0.4825489]\n",
            "Epoch 50/100 -- Loss:  1.3672\n",
            "[0.49746293, 0.48778158]\n",
            "Epoch 51/100 -- Loss:  1.3658\n",
            "[0.503293, 0.49300337]\n",
            "Epoch 52/100 -- Loss:  1.3646\n",
            "[0.5091248, 0.4982071]\n",
            "Epoch 53/100 -- Loss:  1.3636\n",
            "[0.5149512, 0.5033854]\n",
            "Epoch 54/100 -- Loss:  1.3628\n",
            "[0.52076524, 0.5085313]\n",
            "Epoch 55/100 -- Loss:  1.3622\n",
            "[0.52655965, 0.51363736]\n",
            "Epoch 56/100 -- Loss:  1.3618\n",
            "[0.5323276, 0.5186967]\n",
            "Epoch 57/100 -- Loss:  1.3615\n",
            "[0.5380623, 0.52370214]\n",
            "Epoch 58/100 -- Loss:  1.3614\n",
            "[0.5437569, 0.5286469]\n",
            "Epoch 59/100 -- Loss:  1.3615\n",
            "[0.54940474, 0.533524]\n",
            "Epoch 60/100 -- Loss:  1.3617\n",
            "[0.55499953, 0.53832674]\n",
            "Epoch 61/100 -- Loss:  1.3620\n",
            "[0.560535, 0.5430487]\n",
            "Epoch 62/100 -- Loss:  1.3625\n",
            "[0.566005, 0.5476833]\n",
            "Epoch 63/100 -- Loss:  1.3631\n",
            "[0.5714037, 0.5522244]\n",
            "Epoch 64/100 -- Loss:  1.3638\n",
            "[0.5767255, 0.55666584]\n",
            "Epoch 65/100 -- Loss:  1.3646\n",
            "[0.58196497, 0.56100166]\n",
            "Epoch 66/100 -- Loss:  1.3655\n",
            "[0.58711696, 0.56522614]\n",
            "Epoch 67/100 -- Loss:  1.3664\n",
            "[0.5921766, 0.5693337]\n",
            "Epoch 68/100 -- Loss:  1.3673\n",
            "[0.59713924, 0.5733189]\n",
            "Epoch 69/100 -- Loss:  1.3683\n",
            "[0.60200053, 0.5771766]\n",
            "Epoch 70/100 -- Loss:  1.3693\n",
            "[0.60675627, 0.58090156]\n",
            "Epoch 71/100 -- Loss:  1.3702\n",
            "[0.61140275, 0.5844891]\n",
            "Epoch 72/100 -- Loss:  1.3712\n",
            "[0.6159363, 0.5879344]\n",
            "Epoch 73/100 -- Loss:  1.3721\n",
            "[0.6203535, 0.59123296]\n",
            "Epoch 74/100 -- Loss:  1.3729\n",
            "[0.6246516, 0.5943802]\n",
            "Epoch 75/100 -- Loss:  1.3736\n",
            "[0.6288275, 0.59737206]\n",
            "Epoch 76/100 -- Loss:  1.3743\n",
            "[0.6328788, 0.6002042]\n",
            "Epoch 77/100 -- Loss:  1.3748\n",
            "[0.63680315, 0.6028728]\n",
            "Epoch 78/100 -- Loss:  1.3752\n",
            "[0.64059854, 0.60537374]\n",
            "Epoch 79/100 -- Loss:  1.3754\n",
            "[0.644263, 0.60770327]\n",
            "Epoch 80/100 -- Loss:  1.3754\n",
            "[0.6477951, 0.6098577]\n",
            "Epoch 81/100 -- Loss:  1.3753\n",
            "[0.6511932, 0.61183316]\n",
            "Epoch 82/100 -- Loss:  1.3749\n",
            "[0.65445614, 0.6136262]\n",
            "Epoch 83/100 -- Loss:  1.3743\n",
            "[0.657583, 0.61523306]\n",
            "Epoch 84/100 -- Loss:  1.3735\n",
            "[0.6605728, 0.6166503]\n",
            "Epoch 85/100 -- Loss:  1.3723\n",
            "[0.66342497, 0.61787426]\n",
            "Epoch 86/100 -- Loss:  1.3710\n",
            "[0.66613895, 0.6189014]\n",
            "Epoch 87/100 -- Loss:  1.3693\n",
            "[0.6687143, 0.619728]\n",
            "Epoch 88/100 -- Loss:  1.3673\n",
            "[0.6711508, 0.6203506]\n",
            "Epoch 89/100 -- Loss:  1.3649\n",
            "[0.6734484, 0.6207655]\n",
            "Epoch 90/100 -- Loss:  1.3623\n",
            "[0.6756071, 0.6209689]\n",
            "Epoch 91/100 -- Loss:  1.3593\n",
            "[0.6776269, 0.6209571]\n",
            "Epoch 92/100 -- Loss:  1.3559\n",
            "[0.6795082, 0.6207261]\n",
            "Epoch 93/100 -- Loss:  1.3521\n",
            "[0.6812512, 0.6202721]\n",
            "Epoch 94/100 -- Loss:  1.3480\n",
            "[0.68285644, 0.619591]\n",
            "Epoch 95/100 -- Loss:  1.3434\n",
            "[0.6843243, 0.6186786]\n",
            "Epoch 96/100 -- Loss:  1.3385\n",
            "[0.68565553, 0.61753094]\n",
            "Epoch 97/100 -- Loss:  1.3331\n",
            "[0.68685067, 0.6161435]\n",
            "Epoch 98/100 -- Loss:  1.3273\n",
            "[0.6879105, 0.61451197]\n",
            "Epoch 99/100 -- Loss:  1.3211\n",
            "[0.68883574, 0.6126319]\n",
            "Epoch 100/100 -- Loss:  1.3145\n",
            "[0.68962747, 0.61049867]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.00      0.00      0.00        16\n",
            "        True       0.50      1.00      0.67        16\n",
            "\n",
            "    accuracy                           0.50        32\n",
            "   macro avg       0.25      0.50      0.33        32\n",
            "weighted avg       0.25      0.50      0.33        32\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#X_6=X_5\n",
        "X_6=nd.concat(X_1,X_5)\n",
        "model_3, features_3 = build_model(nd.array(A), X_6)\n",
        "print(model_3(X_6))\n",
        "feature_representations_3= train(model_3, features_3, X_6, X_train, y_train, epochs=5000)\n",
        "y_pred_3 = predict(model_3, X_6, X_test)\n",
        "print(classification_report(y_test, y_pred_3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EIaICKuJUG73",
        "outputId": "83672da3-4bde-4904-eb40-24986f2618e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[[0.27892497]\n",
            " [0.2786535 ]\n",
            " [0.27885625]\n",
            " [0.27729917]\n",
            " [0.27731118]\n",
            " [0.2795008 ]\n",
            " [0.27957657]\n",
            " [0.27707726]\n",
            " [0.27678445]\n",
            " [0.2769948 ]\n",
            " [0.3002096 ]\n",
            " [0.26985264]\n",
            " [0.27792946]\n",
            " [0.28436393]\n",
            " [0.27447844]\n",
            " [0.27853984]\n",
            " [0.27650034]\n",
            " [0.27622613]\n",
            " [0.2706243 ]\n",
            " [0.27873725]\n",
            " [0.27409914]\n",
            " [0.27826005]\n",
            " [0.28290734]\n",
            " [0.27886075]\n",
            " [0.2722733 ]\n",
            " [0.28479993]\n",
            " [0.27065092]\n",
            " [0.27927738]\n",
            " [0.27051613]\n",
            " [0.2776721 ]\n",
            " [0.27720803]\n",
            " [0.27858883]\n",
            " [0.27346814]\n",
            " [0.2744386 ]]\n",
            "<NDArray 34x1 @cpu(0)>\n",
            "Epoch 1/5000 -- Loss:  1.5978\n",
            "[0.27892497, 0.27458233]\n",
            "Epoch 2/5000 -- Loss:  1.5974\n",
            "[0.27915984, 0.2748954]\n",
            "Epoch 3/5000 -- Loss:  1.5966\n",
            "[0.2795744, 0.27537793]\n",
            "Epoch 4/5000 -- Loss:  1.5954\n",
            "[0.2801688, 0.27603006]\n",
            "Epoch 5/5000 -- Loss:  1.5937\n",
            "[0.2809433, 0.27685195]\n",
            "Epoch 6/5000 -- Loss:  1.5917\n",
            "[0.2818982, 0.27784383]\n",
            "Epoch 7/5000 -- Loss:  1.5893\n",
            "[0.2830338, 0.27900594]\n",
            "Epoch 8/5000 -- Loss:  1.5865\n",
            "[0.2843505, 0.28033856]\n",
            "Epoch 9/5000 -- Loss:  1.5834\n",
            "[0.28584883, 0.28184202]\n",
            "Epoch 10/5000 -- Loss:  1.5798\n",
            "[0.28752914, 0.28351653]\n",
            "Epoch 11/5000 -- Loss:  1.5760\n",
            "[0.289392, 0.2853624]\n",
            "Epoch 12/5000 -- Loss:  1.5717\n",
            "[0.29143786, 0.28737995]\n",
            "Epoch 13/5000 -- Loss:  1.5672\n",
            "[0.29366726, 0.2895694]\n",
            "Epoch 14/5000 -- Loss:  1.5623\n",
            "[0.29608062, 0.29193094]\n",
            "Epoch 15/5000 -- Loss:  1.5572\n",
            "[0.29867843, 0.29446468]\n",
            "Epoch 16/5000 -- Loss:  1.5518\n",
            "[0.301461, 0.29717067]\n",
            "Epoch 17/5000 -- Loss:  1.5461\n",
            "[0.3044287, 0.30004892]\n",
            "Epoch 18/5000 -- Loss:  1.5401\n",
            "[0.3075817, 0.3030992]\n",
            "Epoch 19/5000 -- Loss:  1.5340\n",
            "[0.31092015, 0.30632123]\n",
            "Epoch 20/5000 -- Loss:  1.5276\n",
            "[0.31444395, 0.30971462]\n",
            "Epoch 21/5000 -- Loss:  1.5210\n",
            "[0.3181529, 0.31327868]\n",
            "Epoch 22/5000 -- Loss:  1.5143\n",
            "[0.3220467, 0.3170126]\n",
            "Epoch 23/5000 -- Loss:  1.5075\n",
            "[0.32612467, 0.32091534]\n",
            "Epoch 24/5000 -- Loss:  1.5005\n",
            "[0.33038604, 0.3249856]\n",
            "Epoch 25/5000 -- Loss:  1.4934\n",
            "[0.33482972, 0.3292218]\n",
            "Epoch 26/5000 -- Loss:  1.4863\n",
            "[0.33945435, 0.33362216]\n",
            "Epoch 27/5000 -- Loss:  1.4791\n",
            "[0.34425825, 0.33818448]\n",
            "Epoch 28/5000 -- Loss:  1.4719\n",
            "[0.34923947, 0.34290627]\n",
            "Epoch 29/5000 -- Loss:  1.4647\n",
            "[0.35439566, 0.34778476]\n",
            "Epoch 30/5000 -- Loss:  1.4575\n",
            "[0.35972404, 0.3528167]\n",
            "Epoch 31/5000 -- Loss:  1.4504\n",
            "[0.36522153, 0.3579985]\n",
            "Epoch 32/5000 -- Loss:  1.4434\n",
            "[0.37088463, 0.36332622]\n",
            "Epoch 33/5000 -- Loss:  1.4364\n",
            "[0.37670937, 0.36879545]\n",
            "Epoch 34/5000 -- Loss:  1.4296\n",
            "[0.3826913, 0.37440136]\n",
            "Epoch 35/5000 -- Loss:  1.4229\n",
            "[0.38882563, 0.38013873]\n",
            "Epoch 36/5000 -- Loss:  1.4164\n",
            "[0.39510703, 0.38600186]\n",
            "Epoch 37/5000 -- Loss:  1.4100\n",
            "[0.40152964, 0.39198464]\n",
            "Epoch 38/5000 -- Loss:  1.4039\n",
            "[0.40808728, 0.3980805]\n",
            "Epoch 39/5000 -- Loss:  1.3980\n",
            "[0.41477326, 0.40428245]\n",
            "Epoch 40/5000 -- Loss:  1.3924\n",
            "[0.42158034, 0.41058317]\n",
            "Epoch 41/5000 -- Loss:  1.3870\n",
            "[0.42850092, 0.41697466]\n",
            "Epoch 42/5000 -- Loss:  1.3819\n",
            "[0.43552688, 0.42344892]\n",
            "Epoch 43/5000 -- Loss:  1.3771\n",
            "[0.44264987, 0.42999724]\n",
            "Epoch 44/5000 -- Loss:  1.3726\n",
            "[0.449861, 0.43661073]\n",
            "Epoch 45/5000 -- Loss:  1.3684\n",
            "[0.45715094, 0.44328016]\n",
            "Epoch 46/5000 -- Loss:  1.3646\n",
            "[0.46451035, 0.44999596]\n",
            "Epoch 47/5000 -- Loss:  1.3611\n",
            "[0.47192928, 0.4567484]\n",
            "Epoch 48/5000 -- Loss:  1.3580\n",
            "[0.47939768, 0.46352732]\n",
            "Epoch 49/5000 -- Loss:  1.3552\n",
            "[0.48690525, 0.4703226]\n",
            "Epoch 50/5000 -- Loss:  1.3527\n",
            "[0.49444166, 0.4771239]\n",
            "Epoch 51/5000 -- Loss:  1.3507\n",
            "[0.50199616, 0.48392066]\n",
            "Epoch 52/5000 -- Loss:  1.3489\n",
            "[0.50955826, 0.49070248]\n",
            "Epoch 53/5000 -- Loss:  1.3476\n",
            "[0.5171173, 0.49745882]\n",
            "Epoch 54/5000 -- Loss:  1.3465\n",
            "[0.5246626, 0.504179]\n",
            "Epoch 55/5000 -- Loss:  1.3459\n",
            "[0.53218377, 0.5108527]\n",
            "Epoch 56/5000 -- Loss:  1.3455\n",
            "[0.5396702, 0.5174694]\n",
            "Epoch 57/5000 -- Loss:  1.3455\n",
            "[0.54711187, 0.52401906]\n",
            "Epoch 58/5000 -- Loss:  1.3458\n",
            "[0.5544987, 0.5304914]\n",
            "Epoch 59/5000 -- Loss:  1.3463\n",
            "[0.5618209, 0.5368767]\n",
            "Epoch 60/5000 -- Loss:  1.3472\n",
            "[0.5690691, 0.5431653]\n",
            "Epoch 61/5000 -- Loss:  1.3483\n",
            "[0.57623416, 0.5493478]\n",
            "Epoch 62/5000 -- Loss:  1.3497\n",
            "[0.58330745, 0.55541515]\n",
            "Epoch 63/5000 -- Loss:  1.3512\n",
            "[0.59028053, 0.56135845]\n",
            "Epoch 64/5000 -- Loss:  1.3530\n",
            "[0.5971456, 0.5671693]\n",
            "Epoch 65/5000 -- Loss:  1.3550\n",
            "[0.6038952, 0.5728396]\n",
            "Epoch 66/5000 -- Loss:  1.3570\n",
            "[0.6105223, 0.57836145]\n",
            "Epoch 67/5000 -- Loss:  1.3593\n",
            "[0.6170204, 0.5837274]\n",
            "Epoch 68/5000 -- Loss:  1.3616\n",
            "[0.6233835, 0.5889303]\n",
            "Epoch 69/5000 -- Loss:  1.3640\n",
            "[0.62960607, 0.5939632]\n",
            "Epoch 70/5000 -- Loss:  1.3664\n",
            "[0.63568294, 0.5988198]\n",
            "Epoch 71/5000 -- Loss:  1.3688\n",
            "[0.6416096, 0.6034937]\n",
            "Epoch 72/5000 -- Loss:  1.3713\n",
            "[0.64738196, 0.60797894]\n",
            "Epoch 73/5000 -- Loss:  1.3736\n",
            "[0.65299636, 0.6122701]\n",
            "Epoch 74/5000 -- Loss:  1.3759\n",
            "[0.65844953, 0.6163616]\n",
            "Epoch 75/5000 -- Loss:  1.3781\n",
            "[0.6637388, 0.6202483]\n",
            "Epoch 76/5000 -- Loss:  1.3801\n",
            "[0.6688617, 0.6239252]\n",
            "Epoch 77/5000 -- Loss:  1.3820\n",
            "[0.6738164, 0.6273875]\n",
            "Epoch 78/5000 -- Loss:  1.3837\n",
            "[0.67860115, 0.6306306]\n",
            "Epoch 79/5000 -- Loss:  1.3851\n",
            "[0.68321484, 0.63364977]\n",
            "Epoch 80/5000 -- Loss:  1.3863\n",
            "[0.6876565, 0.6364407]\n",
            "Epoch 81/5000 -- Loss:  1.3872\n",
            "[0.6919257, 0.63899875]\n",
            "Epoch 82/5000 -- Loss:  1.3877\n",
            "[0.6960219, 0.64131963]\n",
            "Epoch 83/5000 -- Loss:  1.3879\n",
            "[0.6999453, 0.64339876]\n",
            "Epoch 84/5000 -- Loss:  1.3877\n",
            "[0.7036958, 0.64523166]\n",
            "Epoch 85/5000 -- Loss:  1.3871\n",
            "[0.70727396, 0.64681375]\n",
            "Epoch 86/5000 -- Loss:  1.3861\n",
            "[0.7106804, 0.64814013]\n",
            "Epoch 87/5000 -- Loss:  1.3845\n",
            "[0.7139159, 0.64920604]\n",
            "Epoch 88/5000 -- Loss:  1.3825\n",
            "[0.7169813, 0.6500062]\n",
            "Epoch 89/5000 -- Loss:  1.3800\n",
            "[0.7198778, 0.65053535]\n",
            "Epoch 90/5000 -- Loss:  1.3770\n",
            "[0.7226064, 0.65078783]\n",
            "Epoch 91/5000 -- Loss:  1.3733\n",
            "[0.7251685, 0.65075773]\n",
            "Epoch 92/5000 -- Loss:  1.3691\n",
            "[0.72756547, 0.65043885]\n",
            "Epoch 93/5000 -- Loss:  1.3643\n",
            "[0.7297988, 0.64982456]\n",
            "Epoch 94/5000 -- Loss:  1.3589\n",
            "[0.7318698, 0.6489079]\n",
            "Epoch 95/5000 -- Loss:  1.3528\n",
            "[0.7337803, 0.64768165]\n",
            "Epoch 96/5000 -- Loss:  1.3460\n",
            "[0.73553175, 0.646138]\n",
            "Epoch 97/5000 -- Loss:  1.3386\n",
            "[0.7371258, 0.6442689]\n",
            "Epoch 98/5000 -- Loss:  1.3305\n",
            "[0.7385641, 0.6420656]\n",
            "Epoch 99/5000 -- Loss:  1.3216\n",
            "[0.7398485, 0.63951933]\n",
            "Epoch 100/5000 -- Loss:  1.3121\n",
            "[0.7409805, 0.6366206]\n",
            "Epoch 101/5000 -- Loss:  1.3018\n",
            "[0.74196196, 0.6333597]\n",
            "Epoch 102/5000 -- Loss:  1.2908\n",
            "[0.7427947, 0.62972647]\n",
            "Epoch 103/5000 -- Loss:  1.2791\n",
            "[0.74348044, 0.6257106]\n",
            "Epoch 104/5000 -- Loss:  1.2667\n",
            "[0.74402094, 0.62130135]\n",
            "Epoch 105/5000 -- Loss:  1.2535\n",
            "[0.7444181, 0.61648774]\n",
            "Epoch 106/5000 -- Loss:  1.2397\n",
            "[0.74467385, 0.6112589]\n",
            "Epoch 107/5000 -- Loss:  1.2251\n",
            "[0.74479, 0.60560393]\n",
            "Epoch 108/5000 -- Loss:  1.2098\n",
            "[0.7447687, 0.59951204]\n",
            "Epoch 109/5000 -- Loss:  1.1938\n",
            "[0.74461174, 0.5929725]\n",
            "Epoch 110/5000 -- Loss:  1.1771\n",
            "[0.74432147, 0.58597535]\n",
            "Epoch 111/5000 -- Loss:  1.1598\n",
            "[0.74389994, 0.5785112]\n",
            "Epoch 112/5000 -- Loss:  1.1419\n",
            "[0.7433495, 0.5705715]\n",
            "Epoch 113/5000 -- Loss:  1.1234\n",
            "[0.7426725, 0.5621489]\n",
            "Epoch 114/5000 -- Loss:  1.1043\n",
            "[0.74187154, 0.5532376]\n",
            "Epoch 115/5000 -- Loss:  1.0847\n",
            "[0.7409493, 0.5438334]\n",
            "Epoch 116/5000 -- Loss:  1.0647\n",
            "[0.7399083, 0.5339341]\n",
            "Epoch 117/5000 -- Loss:  1.0442\n",
            "[0.7387519, 0.5235404]\n",
            "Epoch 118/5000 -- Loss:  1.0233\n",
            "[0.737483, 0.51265526]\n",
            "Epoch 119/5000 -- Loss:  1.0021\n",
            "[0.736105, 0.50128514]\n",
            "Epoch 120/5000 -- Loss:  0.9806\n",
            "[0.7346215, 0.48943993]\n",
            "Epoch 121/5000 -- Loss:  0.9590\n",
            "[0.7330363, 0.47713348]\n",
            "Epoch 122/5000 -- Loss:  0.9372\n",
            "[0.73135334, 0.46438378]\n",
            "Epoch 123/5000 -- Loss:  0.9153\n",
            "[0.7295769, 0.45121324]\n",
            "Epoch 124/5000 -- Loss:  0.8935\n",
            "[0.7277113, 0.43764895]\n",
            "Epoch 125/5000 -- Loss:  0.8717\n",
            "[0.7257615, 0.4237229]\n",
            "Epoch 126/5000 -- Loss:  0.8501\n",
            "[0.72373235, 0.40947148]\n",
            "Epoch 127/5000 -- Loss:  0.8287\n",
            "[0.7216291, 0.3949362]\n",
            "Epoch 128/5000 -- Loss:  0.8076\n",
            "[0.7194572, 0.38016275]\n",
            "Epoch 129/5000 -- Loss:  0.7868\n",
            "[0.7172225, 0.3652008]\n",
            "Epoch 130/5000 -- Loss:  0.7665\n",
            "[0.7149309, 0.35010383]\n",
            "Epoch 131/5000 -- Loss:  0.7467\n",
            "[0.7125885, 0.334928]\n",
            "Epoch 132/5000 -- Loss:  0.7275\n",
            "[0.7102017, 0.31973183]\n",
            "Epoch 133/5000 -- Loss:  0.7089\n",
            "[0.7077771, 0.304575]\n",
            "Epoch 134/5000 -- Loss:  0.6909\n",
            "[0.7053213, 0.28951785]\n",
            "Epoch 135/5000 -- Loss:  0.6737\n",
            "[0.70284134, 0.27461982]\n",
            "Epoch 136/5000 -- Loss:  0.6572\n",
            "[0.7003439, 0.25993901]\n",
            "Epoch 137/5000 -- Loss:  0.6415\n",
            "[0.6978362, 0.24553074]\n",
            "Epoch 138/5000 -- Loss:  0.6266\n",
            "[0.69532526, 0.23144722]\n",
            "Epoch 139/5000 -- Loss:  0.6126\n",
            "[0.69281805, 0.21773612]\n",
            "Epoch 140/5000 -- Loss:  0.5993\n",
            "[0.69032186, 0.20444046]\n",
            "Epoch 141/5000 -- Loss:  0.5869\n",
            "[0.68784344, 0.19159766]\n",
            "Epoch 142/5000 -- Loss:  0.5753\n",
            "[0.6853899, 0.17923935]\n",
            "Epoch 143/5000 -- Loss:  0.5645\n",
            "[0.682968, 0.16739109]\n",
            "Epoch 144/5000 -- Loss:  0.5545\n",
            "[0.6805845, 0.15607251]\n",
            "Epoch 145/5000 -- Loss:  0.5452\n",
            "[0.67824584, 0.145297]\n",
            "Epoch 146/5000 -- Loss:  0.5367\n",
            "[0.6759585, 0.13507248]\n",
            "Epoch 147/5000 -- Loss:  0.5289\n",
            "[0.67372864, 0.12540121]\n",
            "Epoch 148/5000 -- Loss:  0.5218\n",
            "[0.6715622, 0.116280615]\n",
            "Epoch 149/5000 -- Loss:  0.5152\n",
            "[0.6694649, 0.10770369]\n",
            "Epoch 150/5000 -- Loss:  0.5093\n",
            "[0.66744214, 0.09965942]\n",
            "Epoch 151/5000 -- Loss:  0.5039\n",
            "[0.6654993, 0.09213352]\n",
            "Epoch 152/5000 -- Loss:  0.4990\n",
            "[0.6636414, 0.085108906]\n",
            "Epoch 153/5000 -- Loss:  0.4945\n",
            "[0.66187304, 0.07856626]\n",
            "Epoch 154/5000 -- Loss:  0.4905\n",
            "[0.6601987, 0.07248462]\n",
            "Epoch 155/5000 -- Loss:  0.4868\n",
            "[0.6586226, 0.06684178]\n",
            "Epoch 156/5000 -- Loss:  0.4834\n",
            "[0.65714866, 0.06161476]\n",
            "Epoch 157/5000 -- Loss:  0.4804\n",
            "[0.6557806, 0.05678018]\n",
            "Epoch 158/5000 -- Loss:  0.4776\n",
            "[0.6545218, 0.05231466]\n",
            "Epoch 159/5000 -- Loss:  0.4750\n",
            "[0.65337545, 0.04819501]\n",
            "Epoch 160/5000 -- Loss:  0.4726\n",
            "[0.65234435, 0.044398516]\n",
            "Epoch 161/5000 -- Loss:  0.4703\n",
            "[0.65143114, 0.040903106]\n",
            "Epoch 162/5000 -- Loss:  0.4682\n",
            "[0.6506384, 0.037687544]\n",
            "Epoch 163/5000 -- Loss:  0.4662\n",
            "[0.6499682, 0.034731492]\n",
            "Epoch 164/5000 -- Loss:  0.4642\n",
            "[0.64942247, 0.03201558]\n",
            "Epoch 165/5000 -- Loss:  0.4623\n",
            "[0.6490029, 0.02952152]\n",
            "Epoch 166/5000 -- Loss:  0.4604\n",
            "[0.648711, 0.027232034]\n",
            "Epoch 167/5000 -- Loss:  0.4585\n",
            "[0.64854825, 0.025130982]\n",
            "Epoch 168/5000 -- Loss:  0.4565\n",
            "[0.6485154, 0.023203192]\n",
            "Epoch 169/5000 -- Loss:  0.4546\n",
            "[0.64861363, 0.021434622]\n",
            "Epoch 170/5000 -- Loss:  0.4526\n",
            "[0.64884347, 0.019812182]\n",
            "Epoch 171/5000 -- Loss:  0.4505\n",
            "[0.6492053, 0.018323746]\n",
            "Epoch 172/5000 -- Loss:  0.4483\n",
            "[0.6496996, 0.01695811]\n",
            "Epoch 173/5000 -- Loss:  0.4461\n",
            "[0.6503265, 0.015704954]\n",
            "Epoch 174/5000 -- Loss:  0.4438\n",
            "[0.6510856, 0.014554764]\n",
            "Epoch 175/5000 -- Loss:  0.4413\n",
            "[0.65197706, 0.013498797]\n",
            "Epoch 176/5000 -- Loss:  0.4388\n",
            "[0.6530002, 0.0125290295]\n",
            "Epoch 177/5000 -- Loss:  0.4361\n",
            "[0.6541544, 0.011638098]\n",
            "Epoch 178/5000 -- Loss:  0.4333\n",
            "[0.655439, 0.010819265]\n",
            "Epoch 179/5000 -- Loss:  0.4304\n",
            "[0.6568531, 0.010066357]\n",
            "Epoch 180/5000 -- Loss:  0.4274\n",
            "[0.6583954, 0.009373744]\n",
            "Epoch 181/5000 -- Loss:  0.4242\n",
            "[0.6600647, 0.008736266]\n",
            "Epoch 182/5000 -- Loss:  0.4209\n",
            "[0.6618597, 0.008149227]\n",
            "Epoch 183/5000 -- Loss:  0.4174\n",
            "[0.66377866, 0.007608321]\n",
            "Epoch 184/5000 -- Loss:  0.4139\n",
            "[0.66581994, 0.0071096276]\n",
            "Epoch 185/5000 -- Loss:  0.4102\n",
            "[0.6679815, 0.00664958]\n",
            "Epoch 186/5000 -- Loss:  0.4063\n",
            "[0.67026144, 0.0062249023]\n",
            "Epoch 187/5000 -- Loss:  0.4024\n",
            "[0.67265743, 0.0058326204]\n",
            "Epoch 188/5000 -- Loss:  0.3983\n",
            "[0.6751672, 0.005470032]\n",
            "Epoch 189/5000 -- Loss:  0.3941\n",
            "[0.6777882, 0.005134652]\n",
            "Epoch 190/5000 -- Loss:  0.3897\n",
            "[0.6805178, 0.004824222]\n",
            "Epoch 191/5000 -- Loss:  0.3853\n",
            "[0.6833531, 0.004536687]\n",
            "Epoch 192/5000 -- Loss:  0.3807\n",
            "[0.6862914, 0.004270164]\n",
            "Epoch 193/5000 -- Loss:  0.3761\n",
            "[0.6893295, 0.0040229424]\n",
            "Epoch 194/5000 -- Loss:  0.3713\n",
            "[0.69246435, 0.0037934482]\n",
            "Epoch 195/5000 -- Loss:  0.3664\n",
            "[0.6956925, 0.0035802599]\n",
            "Epoch 196/5000 -- Loss:  0.3615\n",
            "[0.6990106, 0.0033820693]\n",
            "Epoch 197/5000 -- Loss:  0.3564\n",
            "[0.7024151, 0.003197683]\n",
            "Epoch 198/5000 -- Loss:  0.3513\n",
            "[0.70590234, 0.0030260102]\n",
            "Epoch 199/5000 -- Loss:  0.3461\n",
            "[0.7094686, 0.0028660547]\n",
            "Epoch 200/5000 -- Loss:  0.3408\n",
            "[0.71311, 0.0027169008]\n",
            "Epoch 201/5000 -- Loss:  0.3355\n",
            "[0.71682274, 0.0025777149]\n",
            "Epoch 202/5000 -- Loss:  0.3301\n",
            "[0.7206027, 0.0024477318]\n",
            "Epoch 203/5000 -- Loss:  0.3247\n",
            "[0.72444576, 0.0023262524]\n",
            "Epoch 204/5000 -- Loss:  0.3192\n",
            "[0.7283478, 0.0022126324]\n",
            "Epoch 205/5000 -- Loss:  0.3137\n",
            "[0.73230463, 0.0021062845]\n",
            "Epoch 206/5000 -- Loss:  0.3081\n",
            "[0.73631203, 0.002006666]\n",
            "Epoch 207/5000 -- Loss:  0.3025\n",
            "[0.7403657, 0.0019132822]\n",
            "Epoch 208/5000 -- Loss:  0.2969\n",
            "[0.74446124, 0.0018256797]\n",
            "Epoch 209/5000 -- Loss:  0.2913\n",
            "[0.7485944, 0.0017434342]\n",
            "Epoch 210/5000 -- Loss:  0.2857\n",
            "[0.75276077, 0.0016661634]\n",
            "Epoch 211/5000 -- Loss:  0.2800\n",
            "[0.7569561, 0.0015935116]\n",
            "Epoch 212/5000 -- Loss:  0.2744\n",
            "[0.76117593, 0.001525153]\n",
            "Epoch 213/5000 -- Loss:  0.2688\n",
            "[0.765416, 0.0014607871]\n",
            "Epoch 214/5000 -- Loss:  0.2632\n",
            "[0.769672, 0.0014001362]\n",
            "Epoch 215/5000 -- Loss:  0.2576\n",
            "[0.77393967, 0.0013429443]\n",
            "Epoch 216/5000 -- Loss:  0.2520\n",
            "[0.7782148, 0.0012889741]\n",
            "Epoch 217/5000 -- Loss:  0.2465\n",
            "[0.78249335, 0.0012380106]\n",
            "Epoch 218/5000 -- Loss:  0.2410\n",
            "[0.7867712, 0.0011898505]\n",
            "Epoch 219/5000 -- Loss:  0.2355\n",
            "[0.7910443, 0.0011443071]\n",
            "Epoch 220/5000 -- Loss:  0.2301\n",
            "[0.7953088, 0.0011012082]\n",
            "Epoch 221/5000 -- Loss:  0.2248\n",
            "[0.79956096, 0.0010603959]\n",
            "Epoch 222/5000 -- Loss:  0.2194\n",
            "[0.80379695, 0.0010217212]\n",
            "Epoch 223/5000 -- Loss:  0.2142\n",
            "[0.80801314, 0.0009850472]\n",
            "Epoch 224/5000 -- Loss:  0.2090\n",
            "[0.8122062, 0.0009502473]\n",
            "Epoch 225/5000 -- Loss:  0.2038\n",
            "[0.81637275, 0.00091720244]\n",
            "Epoch 226/5000 -- Loss:  0.1987\n",
            "[0.82050943, 0.0008858044]\n",
            "Epoch 227/5000 -- Loss:  0.1937\n",
            "[0.8246133, 0.00085595204]\n",
            "Epoch 228/5000 -- Loss:  0.1887\n",
            "[0.8286813, 0.0008275491]\n",
            "Epoch 229/5000 -- Loss:  0.1839\n",
            "[0.83271056, 0.00080050796]\n",
            "Epoch 230/5000 -- Loss:  0.1791\n",
            "[0.8366987, 0.0007747488]\n",
            "Epoch 231/5000 -- Loss:  0.1743\n",
            "[0.84064287, 0.00075019355]\n",
            "Epoch 232/5000 -- Loss:  0.1697\n",
            "[0.84454083, 0.0007267715]\n",
            "Epoch 233/5000 -- Loss:  0.1651\n",
            "[0.84839046, 0.0007044167]\n",
            "Epoch 234/5000 -- Loss:  0.1606\n",
            "[0.85218954, 0.00068306766]\n",
            "Epoch 235/5000 -- Loss:  0.1562\n",
            "[0.8559362, 0.0006626671]\n",
            "Epoch 236/5000 -- Loss:  0.1519\n",
            "[0.85962874, 0.0006431601]\n",
            "Epoch 237/5000 -- Loss:  0.1477\n",
            "[0.8632655, 0.0006244971]\n",
            "Epoch 238/5000 -- Loss:  0.1435\n",
            "[0.866845, 0.0006066308]\n",
            "Epoch 239/5000 -- Loss:  0.1394\n",
            "[0.87036586, 0.00058951805]\n",
            "Epoch 240/5000 -- Loss:  0.1354\n",
            "[0.8738271, 0.0005731169]\n",
            "Epoch 241/5000 -- Loss:  0.1315\n",
            "[0.8772275, 0.0005573895]\n",
            "Epoch 242/5000 -- Loss:  0.1277\n",
            "[0.8805662, 0.00054229866]\n",
            "Epoch 243/5000 -- Loss:  0.1240\n",
            "[0.88384247, 0.00052781135]\n",
            "Epoch 244/5000 -- Loss:  0.1204\n",
            "[0.8870557, 0.00051389623]\n",
            "Epoch 245/5000 -- Loss:  0.1168\n",
            "[0.8902053, 0.00050052284]\n",
            "Epoch 246/5000 -- Loss:  0.1133\n",
            "[0.89329106, 0.0004876637]\n",
            "Epoch 247/5000 -- Loss:  0.1099\n",
            "[0.8963125, 0.00047529195]\n",
            "Epoch 248/5000 -- Loss:  0.1066\n",
            "[0.8992696, 0.00046338365]\n",
            "Epoch 249/5000 -- Loss:  0.1034\n",
            "[0.90216225, 0.00045191572]\n",
            "Epoch 250/5000 -- Loss:  0.1003\n",
            "[0.90499055, 0.0004408653]\n",
            "Epoch 251/5000 -- Loss:  0.0972\n",
            "[0.90775466, 0.00043021288]\n",
            "Epoch 252/5000 -- Loss:  0.0942\n",
            "[0.9104547, 0.00041993882]\n",
            "Epoch 253/5000 -- Loss:  0.0913\n",
            "[0.91309106, 0.00041002495]\n",
            "Epoch 254/5000 -- Loss:  0.0885\n",
            "[0.91566426, 0.0004004536]\n",
            "Epoch 255/5000 -- Loss:  0.0858\n",
            "[0.9181747, 0.00039120927]\n",
            "Epoch 256/5000 -- Loss:  0.0831\n",
            "[0.92062277, 0.00038227605]\n",
            "Epoch 257/5000 -- Loss:  0.0805\n",
            "[0.92300946, 0.0003736408]\n",
            "Epoch 258/5000 -- Loss:  0.0780\n",
            "[0.92533505, 0.0003652888]\n",
            "Epoch 259/5000 -- Loss:  0.0755\n",
            "[0.92760044, 0.00035720758]\n",
            "Epoch 260/5000 -- Loss:  0.0731\n",
            "[0.9298065, 0.0003493848]\n",
            "Epoch 261/5000 -- Loss:  0.0708\n",
            "[0.93195385, 0.0003418089]\n",
            "Epoch 262/5000 -- Loss:  0.0686\n",
            "[0.9340436, 0.00033446954]\n",
            "Epoch 263/5000 -- Loss:  0.0664\n",
            "[0.93607646, 0.0003273564]\n",
            "Epoch 264/5000 -- Loss:  0.0643\n",
            "[0.9380535, 0.00032045922]\n",
            "Epoch 265/5000 -- Loss:  0.0622\n",
            "[0.93997556, 0.00031376898]\n",
            "Epoch 266/5000 -- Loss:  0.0602\n",
            "[0.9418437, 0.0003072781]\n",
            "Epoch 267/5000 -- Loss:  0.0583\n",
            "[0.94365895, 0.00030097683]\n",
            "Epoch 268/5000 -- Loss:  0.0564\n",
            "[0.9454224, 0.0002948576]\n",
            "Epoch 269/5000 -- Loss:  0.0546\n",
            "[0.9471349, 0.0002889131]\n",
            "Epoch 270/5000 -- Loss:  0.0528\n",
            "[0.9487976, 0.00028313705]\n",
            "Epoch 271/5000 -- Loss:  0.0511\n",
            "[0.9504117, 0.00027752196]\n",
            "Epoch 272/5000 -- Loss:  0.0495\n",
            "[0.9519781, 0.00027206098]\n",
            "Epoch 273/5000 -- Loss:  0.0479\n",
            "[0.95349795, 0.00026674962]\n",
            "Epoch 274/5000 -- Loss:  0.0463\n",
            "[0.95497227, 0.00026158057]\n",
            "Epoch 275/5000 -- Loss:  0.0448\n",
            "[0.9564021, 0.00025654936]\n",
            "Epoch 276/5000 -- Loss:  0.0434\n",
            "[0.95778877, 0.00025165014]\n",
            "Epoch 277/5000 -- Loss:  0.0420\n",
            "[0.95913315, 0.00024687836]\n",
            "Epoch 278/5000 -- Loss:  0.0406\n",
            "[0.96043617, 0.00024222961]\n",
            "Epoch 279/5000 -- Loss:  0.0393\n",
            "[0.9616991, 0.00023769827]\n",
            "Epoch 280/5000 -- Loss:  0.0380\n",
            "[0.96292293, 0.0002332815]\n",
            "Epoch 281/5000 -- Loss:  0.0368\n",
            "[0.96410865, 0.00022897405]\n",
            "Epoch 282/5000 -- Loss:  0.0356\n",
            "[0.9652573, 0.00022477313]\n",
            "Epoch 283/5000 -- Loss:  0.0344\n",
            "[0.9663699, 0.00022067389]\n",
            "Epoch 284/5000 -- Loss:  0.0333\n",
            "[0.96744746, 0.00021667333]\n",
            "Epoch 285/5000 -- Loss:  0.0322\n",
            "[0.9684908, 0.00021276843]\n",
            "Epoch 286/5000 -- Loss:  0.0312\n",
            "[0.9695011, 0.00020895539]\n",
            "Epoch 287/5000 -- Loss:  0.0302\n",
            "[0.97047913, 0.00020523142]\n",
            "Epoch 288/5000 -- Loss:  0.0292\n",
            "[0.97142583, 0.00020159342]\n",
            "Epoch 289/5000 -- Loss:  0.0282\n",
            "[0.9723423, 0.00019803876]\n",
            "Epoch 290/5000 -- Loss:  0.0273\n",
            "[0.9732292, 0.00019456458]\n",
            "Epoch 291/5000 -- Loss:  0.0264\n",
            "[0.97408754, 0.0001911681]\n",
            "Epoch 292/5000 -- Loss:  0.0256\n",
            "[0.9749181, 0.00018784773]\n",
            "Epoch 293/5000 -- Loss:  0.0248\n",
            "[0.9757218, 0.0001846]\n",
            "Epoch 294/5000 -- Loss:  0.0240\n",
            "[0.9764994, 0.00018142346]\n",
            "Epoch 295/5000 -- Loss:  0.0232\n",
            "[0.97725165, 0.00017831533]\n",
            "Epoch 296/5000 -- Loss:  0.0224\n",
            "[0.9779795, 0.00017527398]\n",
            "Epoch 297/5000 -- Loss:  0.0217\n",
            "[0.97868335, 0.00017229699]\n",
            "Epoch 298/5000 -- Loss:  0.0210\n",
            "[0.97936434, 0.00016938297]\n",
            "Epoch 299/5000 -- Loss:  0.0203\n",
            "[0.9800229, 0.00016652967]\n",
            "Epoch 300/5000 -- Loss:  0.0197\n",
            "[0.98066, 0.00016373582]\n",
            "Epoch 301/5000 -- Loss:  0.0191\n",
            "[0.98127604, 0.00016099897]\n",
            "Epoch 302/5000 -- Loss:  0.0185\n",
            "[0.9818719, 0.00015831813]\n",
            "Epoch 303/5000 -- Loss:  0.0179\n",
            "[0.98244816, 0.0001556914]\n",
            "Epoch 304/5000 -- Loss:  0.0173\n",
            "[0.98300546, 0.00015311733]\n",
            "Epoch 305/5000 -- Loss:  0.0167\n",
            "[0.9835443, 0.00015059469]\n",
            "Epoch 306/5000 -- Loss:  0.0162\n",
            "[0.9840655, 0.00014812194]\n",
            "Epoch 307/5000 -- Loss:  0.0157\n",
            "[0.9845694, 0.00014569744]\n",
            "Epoch 308/5000 -- Loss:  0.0152\n",
            "[0.9850566, 0.0001433204]\n",
            "Epoch 309/5000 -- Loss:  0.0147\n",
            "[0.9855278, 0.00014098939]\n",
            "Epoch 310/5000 -- Loss:  0.0143\n",
            "[0.98598343, 0.00013870317]\n",
            "Epoch 311/5000 -- Loss:  0.0138\n",
            "[0.986424, 0.0001364604]\n",
            "Epoch 312/5000 -- Loss:  0.0134\n",
            "[0.98685, 0.00013426054]\n",
            "Epoch 313/5000 -- Loss:  0.0130\n",
            "[0.98726195, 0.00013210194]\n",
            "Epoch 314/5000 -- Loss:  0.0125\n",
            "[0.9876602, 0.00012998372]\n",
            "Epoch 315/5000 -- Loss:  0.0122\n",
            "[0.9880454, 0.00012790484]\n",
            "Epoch 316/5000 -- Loss:  0.0118\n",
            "[0.9884178, 0.00012586461]\n",
            "Epoch 317/5000 -- Loss:  0.0114\n",
            "[0.98877805, 0.00012386187]\n",
            "Epoch 318/5000 -- Loss:  0.0111\n",
            "[0.9891263, 0.00012189566]\n",
            "Epoch 319/5000 -- Loss:  0.0107\n",
            "[0.98946303, 0.00011996545]\n",
            "Epoch 320/5000 -- Loss:  0.0104\n",
            "[0.9897886, 0.00011806985]\n",
            "Epoch 321/5000 -- Loss:  0.0101\n",
            "[0.9901036, 0.000116208415]\n",
            "Epoch 322/5000 -- Loss:  0.0098\n",
            "[0.9904081, 0.00011438048]\n",
            "Epoch 323/5000 -- Loss:  0.0095\n",
            "[0.99070275, 0.00011258493]\n",
            "Epoch 324/5000 -- Loss:  0.0092\n",
            "[0.9909875, 0.000110820845]\n",
            "Epoch 325/5000 -- Loss:  0.0089\n",
            "[0.99126303, 0.00010908805]\n",
            "Epoch 326/5000 -- Loss:  0.0086\n",
            "[0.99152946, 0.000107385706]\n",
            "Epoch 327/5000 -- Loss:  0.0084\n",
            "[0.99178714, 0.00010571306]\n",
            "Epoch 328/5000 -- Loss:  0.0081\n",
            "[0.9920364, 0.00010406933]\n",
            "Epoch 329/5000 -- Loss:  0.0079\n",
            "[0.9922775, 0.000102454105]\n",
            "Epoch 330/5000 -- Loss:  0.0076\n",
            "[0.9925108, 0.000100866826]\n",
            "Epoch 331/5000 -- Loss:  0.0074\n",
            "[0.99273634, 9.930659e-05]\n",
            "Epoch 332/5000 -- Loss:  0.0072\n",
            "[0.9929547, 9.777301e-05]\n",
            "Epoch 333/5000 -- Loss:  0.0070\n",
            "[0.9931658, 9.62654e-05]\n",
            "Epoch 334/5000 -- Loss:  0.0067\n",
            "[0.99336994, 9.478357e-05]\n",
            "Epoch 335/5000 -- Loss:  0.0065\n",
            "[0.99356765, 9.332642e-05]\n",
            "Epoch 336/5000 -- Loss:  0.0064\n",
            "[0.9937589, 9.1894115e-05]\n",
            "Epoch 337/5000 -- Loss:  0.0062\n",
            "[0.99394387, 9.0485686e-05]\n",
            "Epoch 338/5000 -- Loss:  0.0060\n",
            "[0.9941229, 8.91008e-05]\n",
            "Epoch 339/5000 -- Loss:  0.0058\n",
            "[0.99429613, 8.773879e-05]\n",
            "Epoch 340/5000 -- Loss:  0.0056\n",
            "[0.99446386, 8.63994e-05]\n",
            "Epoch 341/5000 -- Loss:  0.0055\n",
            "[0.9946261, 8.508209e-05]\n",
            "Epoch 342/5000 -- Loss:  0.0053\n",
            "[0.9947831, 8.37866e-05]\n",
            "Epoch 343/5000 -- Loss:  0.0052\n",
            "[0.9949351, 8.251242e-05]\n",
            "Epoch 344/5000 -- Loss:  0.0050\n",
            "[0.99508226, 8.1258855e-05]\n",
            "Epoch 345/5000 -- Loss:  0.0049\n",
            "[0.9952247, 8.0025864e-05]\n",
            "Epoch 346/5000 -- Loss:  0.0047\n",
            "[0.9953625, 7.8812925e-05]\n",
            "Epoch 347/5000 -- Loss:  0.0046\n",
            "[0.99549603, 7.761956e-05]\n",
            "Epoch 348/5000 -- Loss:  0.0045\n",
            "[0.99562526, 7.644542e-05]\n",
            "Epoch 349/5000 -- Loss:  0.0043\n",
            "[0.9957504, 7.529034e-05]\n",
            "Epoch 350/5000 -- Loss:  0.0042\n",
            "[0.9958716, 7.4153846e-05]\n",
            "Epoch 351/5000 -- Loss:  0.0041\n",
            "[0.9959889, 7.303561e-05]\n",
            "Epoch 352/5000 -- Loss:  0.0040\n",
            "[0.99610245, 7.1935276e-05]\n",
            "Epoch 353/5000 -- Loss:  0.0039\n",
            "[0.9962126, 7.085245e-05]\n",
            "Epoch 354/5000 -- Loss:  0.0038\n",
            "[0.99631906, 6.97868e-05]\n",
            "Epoch 355/5000 -- Loss:  0.0037\n",
            "[0.9964224, 6.873822e-05]\n",
            "Epoch 356/5000 -- Loss:  0.0036\n",
            "[0.9965223, 6.770617e-05]\n",
            "Epoch 357/5000 -- Loss:  0.0035\n",
            "[0.9966191, 6.669038e-05]\n",
            "Epoch 358/5000 -- Loss:  0.0034\n",
            "[0.99671304, 6.5690765e-05]\n",
            "Epoch 359/5000 -- Loss:  0.0033\n",
            "[0.996804, 6.470675e-05]\n",
            "Epoch 360/5000 -- Loss:  0.0032\n",
            "[0.99689204, 6.373827e-05]\n",
            "Epoch 361/5000 -- Loss:  0.0031\n",
            "[0.99697745, 6.278505e-05]\n",
            "Epoch 362/5000 -- Loss:  0.0030\n",
            "[0.9970601, 6.1846804e-05]\n",
            "Epoch 363/5000 -- Loss:  0.0029\n",
            "[0.9971403, 6.092316e-05]\n",
            "Epoch 364/5000 -- Loss:  0.0028\n",
            "[0.997218, 6.001387e-05]\n",
            "Epoch 365/5000 -- Loss:  0.0028\n",
            "[0.9972933, 5.9118724e-05]\n",
            "Epoch 366/5000 -- Loss:  0.0027\n",
            "[0.99736637, 5.8237594e-05]\n",
            "Epoch 367/5000 -- Loss:  0.0026\n",
            "[0.997437, 5.737003e-05]\n",
            "Epoch 368/5000 -- Loss:  0.0026\n",
            "[0.9975057, 5.6515877e-05]\n",
            "Epoch 369/5000 -- Loss:  0.0025\n",
            "[0.99757224, 5.5675024e-05]\n",
            "Epoch 370/5000 -- Loss:  0.0024\n",
            "[0.9976367, 5.4847147e-05]\n",
            "Epoch 371/5000 -- Loss:  0.0024\n",
            "[0.9976992, 5.4032e-05]\n",
            "Epoch 372/5000 -- Loss:  0.0023\n",
            "[0.9977598, 5.3229418e-05]\n",
            "Epoch 373/5000 -- Loss:  0.0022\n",
            "[0.9978187, 5.2439213e-05]\n",
            "Epoch 374/5000 -- Loss:  0.0022\n",
            "[0.9978758, 5.166113e-05]\n",
            "Epoch 375/5000 -- Loss:  0.0021\n",
            "[0.9979311, 5.089488e-05]\n",
            "Epoch 376/5000 -- Loss:  0.0021\n",
            "[0.99798477, 5.0140472e-05]\n",
            "Epoch 377/5000 -- Loss:  0.0020\n",
            "[0.9980368, 4.9397528e-05]\n",
            "Epoch 378/5000 -- Loss:  0.0020\n",
            "[0.99808735, 4.8665876e-05]\n",
            "Epoch 379/5000 -- Loss:  0.0019\n",
            "[0.9981364, 4.7945512e-05]\n",
            "Epoch 380/5000 -- Loss:  0.0019\n",
            "[0.9981839, 4.7236084e-05]\n",
            "Epoch 381/5000 -- Loss:  0.0018\n",
            "[0.99823016, 4.653751e-05]\n",
            "Epoch 382/5000 -- Loss:  0.0018\n",
            "[0.9982749, 4.584952e-05]\n",
            "Epoch 383/5000 -- Loss:  0.0017\n",
            "[0.99831843, 4.5171924e-05]\n",
            "Epoch 384/5000 -- Loss:  0.0017\n",
            "[0.9983606, 4.450455e-05]\n",
            "Epoch 385/5000 -- Loss:  0.0016\n",
            "[0.9984016, 4.3847456e-05]\n",
            "Epoch 386/5000 -- Loss:  0.0016\n",
            "[0.9984414, 4.320027e-05]\n",
            "Epoch 387/5000 -- Loss:  0.0016\n",
            "[0.99848, 4.256276e-05]\n",
            "Epoch 388/5000 -- Loss:  0.0015\n",
            "[0.99851745, 4.193493e-05]\n",
            "Epoch 389/5000 -- Loss:  0.0015\n",
            "[0.9985538, 4.1316638e-05]\n",
            "Epoch 390/5000 -- Loss:  0.0015\n",
            "[0.9985892, 4.070766e-05]\n",
            "Epoch 391/5000 -- Loss:  0.0014\n",
            "[0.9986236, 4.0107847e-05]\n",
            "Epoch 392/5000 -- Loss:  0.0014\n",
            "[0.99865687, 3.9517057e-05]\n",
            "Epoch 393/5000 -- Loss:  0.0014\n",
            "[0.99868935, 3.8935046e-05]\n",
            "Epoch 394/5000 -- Loss:  0.0013\n",
            "[0.9987207, 3.836186e-05]\n",
            "Epoch 395/5000 -- Loss:  0.0013\n",
            "[0.9987513, 3.7797297e-05]\n",
            "Epoch 396/5000 -- Loss:  0.0013\n",
            "[0.998781, 3.7241254e-05]\n",
            "Epoch 397/5000 -- Loss:  0.0012\n",
            "[0.99880993, 3.669346e-05]\n",
            "Epoch 398/5000 -- Loss:  0.0012\n",
            "[0.9988379, 3.615386e-05]\n",
            "Epoch 399/5000 -- Loss:  0.0012\n",
            "[0.99886525, 3.5622365e-05]\n",
            "Epoch 400/5000 -- Loss:  0.0011\n",
            "[0.99889165, 3.5098852e-05]\n",
            "Epoch 401/5000 -- Loss:  0.0011\n",
            "[0.99891746, 3.4583165e-05]\n",
            "Epoch 402/5000 -- Loss:  0.0011\n",
            "[0.99894243, 3.407518e-05]\n",
            "Epoch 403/5000 -- Loss:  0.0011\n",
            "[0.9989667, 3.3574786e-05]\n",
            "Epoch 404/5000 -- Loss:  0.0010\n",
            "[0.99899036, 3.308187e-05]\n",
            "Epoch 405/5000 -- Loss:  0.0010\n",
            "[0.9990133, 3.259625e-05]\n",
            "Epoch 406/5000 -- Loss:  0.0010\n",
            "[0.9990357, 3.211788e-05]\n",
            "Epoch 407/5000 -- Loss:  0.0010\n",
            "[0.9990575, 3.1646654e-05]\n",
            "Epoch 408/5000 -- Loss:  0.0010\n",
            "[0.99907863, 3.1182397e-05]\n",
            "Epoch 409/5000 -- Loss:  0.0009\n",
            "[0.99909925, 3.072507e-05]\n",
            "Epoch 410/5000 -- Loss:  0.0009\n",
            "[0.9991192, 3.0274623e-05]\n",
            "Epoch 411/5000 -- Loss:  0.0009\n",
            "[0.9991386, 2.9830835e-05]\n",
            "Epoch 412/5000 -- Loss:  0.0009\n",
            "[0.99915755, 2.9393641e-05]\n",
            "Epoch 413/5000 -- Loss:  0.0009\n",
            "[0.99917597, 2.8962877e-05]\n",
            "Epoch 414/5000 -- Loss:  0.0008\n",
            "[0.99919385, 2.8538536e-05]\n",
            "Epoch 415/5000 -- Loss:  0.0008\n",
            "[0.9992113, 2.8120465e-05]\n",
            "Epoch 416/5000 -- Loss:  0.0008\n",
            "[0.99922824, 2.7708624e-05]\n",
            "Epoch 417/5000 -- Loss:  0.0008\n",
            "[0.9992448, 2.7302844e-05]\n",
            "Epoch 418/5000 -- Loss:  0.0008\n",
            "[0.99926084, 2.6903077e-05]\n",
            "Epoch 419/5000 -- Loss:  0.0008\n",
            "[0.99927646, 2.6509268e-05]\n",
            "Epoch 420/5000 -- Loss:  0.0007\n",
            "[0.99929166, 2.6121324e-05]\n",
            "Epoch 421/5000 -- Loss:  0.0007\n",
            "[0.99930656, 2.573908e-05]\n",
            "Epoch 422/5000 -- Loss:  0.0007\n",
            "[0.999321, 2.5362502e-05]\n",
            "Epoch 423/5000 -- Loss:  0.0007\n",
            "[0.999335, 2.4991528e-05]\n",
            "Epoch 424/5000 -- Loss:  0.0007\n",
            "[0.9993487, 2.4625959e-05]\n",
            "Epoch 425/5000 -- Loss:  0.0007\n",
            "[0.99936193, 2.4265852e-05]\n",
            "Epoch 426/5000 -- Loss:  0.0006\n",
            "[0.9993749, 2.391101e-05]\n",
            "Epoch 427/5000 -- Loss:  0.0006\n",
            "[0.9993876, 2.356138e-05]\n",
            "Epoch 428/5000 -- Loss:  0.0006\n",
            "[0.9993999, 2.3216928e-05]\n",
            "Epoch 429/5000 -- Loss:  0.0006\n",
            "[0.99941194, 2.2877577e-05]\n",
            "Epoch 430/5000 -- Loss:  0.0006\n",
            "[0.9994236, 2.2543209e-05]\n",
            "Epoch 431/5000 -- Loss:  0.0006\n",
            "[0.9994349, 2.2213811e-05]\n",
            "Epoch 432/5000 -- Loss:  0.0006\n",
            "[0.999446, 2.1889227e-05]\n",
            "Epoch 433/5000 -- Loss:  0.0006\n",
            "[0.9994568, 2.1569509e-05]\n",
            "Epoch 434/5000 -- Loss:  0.0006\n",
            "[0.9994673, 2.1254438e-05]\n",
            "Epoch 435/5000 -- Loss:  0.0005\n",
            "[0.99947757, 2.0943991e-05]\n",
            "Epoch 436/5000 -- Loss:  0.0005\n",
            "[0.9994875, 2.063814e-05]\n",
            "Epoch 437/5000 -- Loss:  0.0005\n",
            "[0.9994973, 2.033677e-05]\n",
            "Epoch 438/5000 -- Loss:  0.0005\n",
            "[0.99950683, 2.0039823e-05]\n",
            "Epoch 439/5000 -- Loss:  0.0005\n",
            "[0.999516, 1.9747267e-05]\n",
            "Epoch 440/5000 -- Loss:  0.0005\n",
            "[0.99952507, 1.9458965e-05]\n",
            "Epoch 441/5000 -- Loss:  0.0005\n",
            "[0.9995339, 1.9174926e-05]\n",
            "Epoch 442/5000 -- Loss:  0.0005\n",
            "[0.9995425, 1.8895087e-05]\n",
            "Epoch 443/5000 -- Loss:  0.0005\n",
            "[0.99955076, 1.8619348e-05]\n",
            "Epoch 444/5000 -- Loss:  0.0005\n",
            "[0.999559, 1.8347688e-05]\n",
            "Epoch 445/5000 -- Loss:  0.0005\n",
            "[0.999567, 1.8079954e-05]\n",
            "Epoch 446/5000 -- Loss:  0.0004\n",
            "[0.9995747, 1.7816214e-05]\n",
            "Epoch 447/5000 -- Loss:  0.0004\n",
            "[0.99958223, 1.7556338e-05]\n",
            "Epoch 448/5000 -- Loss:  0.0004\n",
            "[0.9995896, 1.7300268e-05]\n",
            "Epoch 449/5000 -- Loss:  0.0004\n",
            "[0.9995969, 1.7047949e-05]\n",
            "Epoch 450/5000 -- Loss:  0.0004\n",
            "[0.9996039, 1.6799278e-05]\n",
            "Epoch 451/5000 -- Loss:  0.0004\n",
            "[0.9996107, 1.6554315e-05]\n",
            "Epoch 452/5000 -- Loss:  0.0004\n",
            "[0.99961734, 1.6312906e-05]\n",
            "Epoch 453/5000 -- Loss:  0.0004\n",
            "[0.9996239, 1.6075064e-05]\n",
            "Epoch 454/5000 -- Loss:  0.0004\n",
            "[0.99963033, 1.5840706e-05]\n",
            "Epoch 455/5000 -- Loss:  0.0004\n",
            "[0.99963653, 1.5609763e-05]\n",
            "Epoch 456/5000 -- Loss:  0.0004\n",
            "[0.9996426, 1.5382204e-05]\n",
            "Epoch 457/5000 -- Loss:  0.0004\n",
            "[0.99964845, 1.5157974e-05]\n",
            "Epoch 458/5000 -- Loss:  0.0004\n",
            "[0.9996543, 1.4937043e-05]\n",
            "Epoch 459/5000 -- Loss:  0.0004\n",
            "[0.9996599, 1.4719372e-05]\n",
            "Epoch 460/5000 -- Loss:  0.0003\n",
            "[0.9996654, 1.4504916e-05]\n",
            "Epoch 461/5000 -- Loss:  0.0003\n",
            "[0.99967074, 1.4293558e-05]\n",
            "Epoch 462/5000 -- Loss:  0.0003\n",
            "[0.999676, 1.4085293e-05]\n",
            "Epoch 463/5000 -- Loss:  0.0003\n",
            "[0.9996811, 1.3880101e-05]\n",
            "Epoch 464/5000 -- Loss:  0.0003\n",
            "[0.9996861, 1.3677898e-05]\n",
            "Epoch 465/5000 -- Loss:  0.0003\n",
            "[0.999691, 1.3478654e-05]\n",
            "Epoch 466/5000 -- Loss:  0.0003\n",
            "[0.9996958, 1.3282325e-05]\n",
            "Epoch 467/5000 -- Loss:  0.0003\n",
            "[0.9997004, 1.3088857e-05]\n",
            "Epoch 468/5000 -- Loss:  0.0003\n",
            "[0.9997049, 1.2898217e-05]\n",
            "Epoch 469/5000 -- Loss:  0.0003\n",
            "[0.9997094, 1.2710368e-05]\n",
            "Epoch 470/5000 -- Loss:  0.0003\n",
            "[0.9997137, 1.2525277e-05]\n",
            "Epoch 471/5000 -- Loss:  0.0003\n",
            "[0.999718, 1.2342857e-05]\n",
            "Epoch 472/5000 -- Loss:  0.0003\n",
            "[0.99972206, 1.2163108e-05]\n",
            "Epoch 473/5000 -- Loss:  0.0003\n",
            "[0.9997261, 1.1986032e-05]\n",
            "Epoch 474/5000 -- Loss:  0.0003\n",
            "[0.99973005, 1.18114895e-05]\n",
            "Epoch 475/5000 -- Loss:  0.0003\n",
            "[0.999734, 1.16395e-05]\n",
            "Epoch 476/5000 -- Loss:  0.0003\n",
            "[0.9997377, 1.1470069e-05]\n",
            "Epoch 477/5000 -- Loss:  0.0003\n",
            "[0.9997414, 1.1303104e-05]\n",
            "Epoch 478/5000 -- Loss:  0.0003\n",
            "[0.99974495, 1.113857e-05]\n",
            "Epoch 479/5000 -- Loss:  0.0003\n",
            "[0.9997485, 1.0976462e-05]\n",
            "Epoch 480/5000 -- Loss:  0.0003\n",
            "[0.999752, 1.0816724e-05]\n",
            "Epoch 481/5000 -- Loss:  0.0003\n",
            "[0.9997553, 1.0659311e-05]\n",
            "Epoch 482/5000 -- Loss:  0.0003\n",
            "[0.99975854, 1.0504188e-05]\n",
            "Epoch 483/5000 -- Loss:  0.0002\n",
            "[0.99976176, 1.0351332e-05]\n",
            "Epoch 484/5000 -- Loss:  0.0002\n",
            "[0.999765, 1.0200731e-05]\n",
            "Epoch 485/5000 -- Loss:  0.0002\n",
            "[0.99976796, 1.00523e-05]\n",
            "Epoch 486/5000 -- Loss:  0.0002\n",
            "[0.99977094, 9.90603e-06]\n",
            "Epoch 487/5000 -- Loss:  0.0002\n",
            "[0.9997739, 9.761888e-06]\n",
            "Epoch 488/5000 -- Loss:  0.0002\n",
            "[0.9997768, 9.619871e-06]\n",
            "Epoch 489/5000 -- Loss:  0.0002\n",
            "[0.9997795, 9.479929e-06]\n",
            "Epoch 490/5000 -- Loss:  0.0002\n",
            "[0.99978226, 9.342005e-06]\n",
            "Epoch 491/5000 -- Loss:  0.0002\n",
            "[0.9997849, 9.206088e-06]\n",
            "Epoch 492/5000 -- Loss:  0.0002\n",
            "[0.9997875, 9.072174e-06]\n",
            "Epoch 493/5000 -- Loss:  0.0002\n",
            "[0.99979013, 8.940199e-06]\n",
            "Epoch 494/5000 -- Loss:  0.0002\n",
            "[0.99979264, 8.810179e-06]\n",
            "Epoch 495/5000 -- Loss:  0.0002\n",
            "[0.999795, 8.682032e-06]\n",
            "Epoch 496/5000 -- Loss:  0.0002\n",
            "[0.9997974, 8.555732e-06]\n",
            "Epoch 497/5000 -- Loss:  0.0002\n",
            "[0.99979967, 8.431294e-06]\n",
            "Epoch 498/5000 -- Loss:  0.0002\n",
            "[0.99980193, 8.308674e-06]\n",
            "Epoch 499/5000 -- Loss:  0.0002\n",
            "[0.9998042, 8.187838e-06]\n",
            "Epoch 500/5000 -- Loss:  0.0002\n",
            "[0.99980634, 8.068759e-06]\n",
            "Epoch 501/5000 -- Loss:  0.0002\n",
            "[0.9998085, 7.951427e-06]\n",
            "Epoch 502/5000 -- Loss:  0.0002\n",
            "[0.9998105, 7.835808e-06]\n",
            "Epoch 503/5000 -- Loss:  0.0002\n",
            "[0.99981254, 7.721877e-06]\n",
            "Epoch 504/5000 -- Loss:  0.0002\n",
            "[0.99981457, 7.6096044e-06]\n",
            "Epoch 505/5000 -- Loss:  0.0002\n",
            "[0.9998165, 7.4989625e-06]\n",
            "Epoch 506/5000 -- Loss:  0.0002\n",
            "[0.9998184, 7.38993e-06]\n",
            "Epoch 507/5000 -- Loss:  0.0002\n",
            "[0.9998203, 7.2824832e-06]\n",
            "Epoch 508/5000 -- Loss:  0.0002\n",
            "[0.9998221, 7.176605e-06]\n",
            "Epoch 509/5000 -- Loss:  0.0002\n",
            "[0.99982375, 7.0722867e-06]\n",
            "Epoch 510/5000 -- Loss:  0.0002\n",
            "[0.99982554, 6.9694706e-06]\n",
            "Epoch 511/5000 -- Loss:  0.0002\n",
            "[0.9998272, 6.8681497e-06]\n",
            "Epoch 512/5000 -- Loss:  0.0002\n",
            "[0.9998288, 6.768302e-06]\n",
            "Epoch 513/5000 -- Loss:  0.0002\n",
            "[0.99983037, 6.6699245e-06]\n",
            "Epoch 514/5000 -- Loss:  0.0002\n",
            "[0.9998319, 6.5730087e-06]\n",
            "Epoch 515/5000 -- Loss:  0.0002\n",
            "[0.99983346, 6.477488e-06]\n",
            "Epoch 516/5000 -- Loss:  0.0002\n",
            "[0.999835, 6.383357e-06]\n",
            "Epoch 517/5000 -- Loss:  0.0002\n",
            "[0.99983644, 6.2906047e-06]\n",
            "Epoch 518/5000 -- Loss:  0.0002\n",
            "[0.9998379, 6.1991886e-06]\n",
            "Epoch 519/5000 -- Loss:  0.0002\n",
            "[0.9998393, 6.1091064e-06]\n",
            "Epoch 520/5000 -- Loss:  0.0002\n",
            "[0.9998406, 6.0203506e-06]\n",
            "Epoch 521/5000 -- Loss:  0.0002\n",
            "[0.9998419, 5.9328727e-06]\n",
            "Epoch 522/5000 -- Loss:  0.0002\n",
            "[0.99984324, 5.846678e-06]\n",
            "Epoch 523/5000 -- Loss:  0.0002\n",
            "[0.99984455, 5.7617235e-06]\n",
            "Epoch 524/5000 -- Loss:  0.0002\n",
            "[0.99984574, 5.678004e-06]\n",
            "Epoch 525/5000 -- Loss:  0.0002\n",
            "[0.99984694, 5.5955225e-06]\n",
            "Epoch 526/5000 -- Loss:  0.0002\n",
            "[0.9998481, 5.514228e-06]\n",
            "Epoch 527/5000 -- Loss:  0.0002\n",
            "[0.9998493, 5.4341253e-06]\n",
            "Epoch 528/5000 -- Loss:  0.0002\n",
            "[0.9998504, 5.355176e-06]\n",
            "Epoch 529/5000 -- Loss:  0.0002\n",
            "[0.99985147, 5.277394e-06]\n",
            "Epoch 530/5000 -- Loss:  0.0002\n",
            "[0.99985254, 5.2007413e-06]\n",
            "Epoch 531/5000 -- Loss:  0.0002\n",
            "[0.9998536, 5.1252123e-06]\n",
            "Epoch 532/5000 -- Loss:  0.0002\n",
            "[0.99985456, 5.0507797e-06]\n",
            "Epoch 533/5000 -- Loss:  0.0001\n",
            "[0.9998555, 4.9774185e-06]\n",
            "Epoch 534/5000 -- Loss:  0.0001\n",
            "[0.9998565, 4.9051323e-06]\n",
            "Epoch 535/5000 -- Loss:  0.0001\n",
            "[0.9998574, 4.8338966e-06]\n",
            "Epoch 536/5000 -- Loss:  0.0001\n",
            "[0.9998584, 4.7636763e-06]\n",
            "Epoch 537/5000 -- Loss:  0.0001\n",
            "[0.9998592, 4.6944942e-06]\n",
            "Epoch 538/5000 -- Loss:  0.0001\n",
            "[0.99986005, 4.626317e-06]\n",
            "Epoch 539/5000 -- Loss:  0.0001\n",
            "[0.9998609, 4.5591214e-06]\n",
            "Epoch 540/5000 -- Loss:  0.0001\n",
            "[0.9998617, 4.49291e-06]\n",
            "Epoch 541/5000 -- Loss:  0.0001\n",
            "[0.99986255, 4.4276603e-06]\n",
            "Epoch 542/5000 -- Loss:  0.0001\n",
            "[0.99986327, 4.3633586e-06]\n",
            "Epoch 543/5000 -- Loss:  0.0001\n",
            "[0.999864, 4.29999e-06]\n",
            "Epoch 544/5000 -- Loss:  0.0001\n",
            "[0.9998647, 4.237558e-06]\n",
            "Epoch 545/5000 -- Loss:  0.0001\n",
            "[0.9998654, 4.176025e-06]\n",
            "Epoch 546/5000 -- Loss:  0.0001\n",
            "[0.9998661, 4.115393e-06]\n",
            "Epoch 547/5000 -- Loss:  0.0001\n",
            "[0.9998667, 4.0556415e-06]\n",
            "Epoch 548/5000 -- Loss:  0.0001\n",
            "[0.9998673, 3.996757e-06]\n",
            "Epoch 549/5000 -- Loss:  0.0001\n",
            "[0.99986804, 3.9387205e-06]\n",
            "Epoch 550/5000 -- Loss:  0.0001\n",
            "[0.99986863, 3.881534e-06]\n",
            "Epoch 551/5000 -- Loss:  0.0001\n",
            "[0.9998691, 3.825178e-06]\n",
            "Epoch 552/5000 -- Loss:  0.0001\n",
            "[0.9998697, 3.7696402e-06]\n",
            "Epoch 553/5000 -- Loss:  0.0001\n",
            "[0.9998702, 3.7149155e-06]\n",
            "Epoch 554/5000 -- Loss:  0.0001\n",
            "[0.9998708, 3.6609854e-06]\n",
            "Epoch 555/5000 -- Loss:  0.0001\n",
            "[0.99987125, 3.607838e-06]\n",
            "Epoch 556/5000 -- Loss:  0.0001\n",
            "[0.99987173, 3.5554624e-06]\n",
            "Epoch 557/5000 -- Loss:  0.0001\n",
            "[0.9998722, 3.503847e-06]\n",
            "Epoch 558/5000 -- Loss:  0.0001\n",
            "[0.99987257, 3.4529812e-06]\n",
            "Epoch 559/5000 -- Loss:  0.0001\n",
            "[0.99987304, 3.4028667e-06]\n",
            "Epoch 560/5000 -- Loss:  0.0001\n",
            "[0.9998734, 3.3534793e-06]\n",
            "Epoch 561/5000 -- Loss:  0.0001\n",
            "[0.99987376, 3.3048025e-06]\n",
            "Epoch 562/5000 -- Loss:  0.0001\n",
            "[0.9998741, 3.2568385e-06]\n",
            "Epoch 563/5000 -- Loss:  0.0001\n",
            "[0.9998745, 3.2095707e-06]\n",
            "Epoch 564/5000 -- Loss:  0.0001\n",
            "[0.99987483, 3.1629886e-06]\n",
            "Epoch 565/5000 -- Loss:  0.0001\n",
            "[0.9998752, 3.1170828e-06]\n",
            "Epoch 566/5000 -- Loss:  0.0001\n",
            "[0.9998754, 3.0718434e-06]\n",
            "Epoch 567/5000 -- Loss:  0.0001\n",
            "[0.9998758, 3.0272604e-06]\n",
            "Epoch 568/5000 -- Loss:  0.0001\n",
            "[0.999876, 2.9833186e-06]\n",
            "Epoch 569/5000 -- Loss:  0.0001\n",
            "[0.99987626, 2.9400205e-06]\n",
            "Epoch 570/5000 -- Loss:  0.0001\n",
            "[0.9998765, 2.8973507e-06]\n",
            "Epoch 571/5000 -- Loss:  0.0001\n",
            "[0.9998766, 2.8553054e-06]\n",
            "Epoch 572/5000 -- Loss:  0.0001\n",
            "[0.99987686, 2.813876e-06]\n",
            "Epoch 573/5000 -- Loss:  0.0001\n",
            "[0.999877, 2.773053e-06]\n",
            "Epoch 574/5000 -- Loss:  0.0001\n",
            "[0.9998772, 2.7328272e-06]\n",
            "Epoch 575/5000 -- Loss:  0.0001\n",
            "[0.99987733, 2.693185e-06]\n",
            "Epoch 576/5000 -- Loss:  0.0001\n",
            "[0.99987745, 2.654118e-06]\n",
            "Epoch 577/5000 -- Loss:  0.0001\n",
            "[0.9998776, 2.6156174e-06]\n",
            "Epoch 578/5000 -- Loss:  0.0001\n",
            "[0.9998777, 2.5776756e-06]\n",
            "Epoch 579/5000 -- Loss:  0.0001\n",
            "[0.9998777, 2.540289e-06]\n",
            "Epoch 580/5000 -- Loss:  0.0001\n",
            "[0.9998778, 2.5034444e-06]\n",
            "Epoch 581/5000 -- Loss:  0.0001\n",
            "[0.9998778, 2.4671342e-06]\n",
            "Epoch 582/5000 -- Loss:  0.0001\n",
            "[0.9998778, 2.431351e-06]\n",
            "Epoch 583/5000 -- Loss:  0.0001\n",
            "[0.9998779, 2.3960865e-06]\n",
            "Epoch 584/5000 -- Loss:  0.0001\n",
            "[0.9998778, 2.361329e-06]\n",
            "Epoch 585/5000 -- Loss:  0.0001\n",
            "[0.9998778, 2.3270802e-06]\n",
            "Epoch 586/5000 -- Loss:  0.0001\n",
            "[0.9998778, 2.2933282e-06]\n",
            "Epoch 587/5000 -- Loss:  0.0001\n",
            "[0.9998778, 2.2600657e-06]\n",
            "Epoch 588/5000 -- Loss:  0.0001\n",
            "[0.9998777, 2.2272857e-06]\n",
            "Epoch 589/5000 -- Loss:  0.0001\n",
            "[0.9998776, 2.194985e-06]\n",
            "Epoch 590/5000 -- Loss:  0.0001\n",
            "[0.99987745, 2.1631572e-06]\n",
            "Epoch 591/5000 -- Loss:  0.0001\n",
            "[0.99987733, 2.1317908e-06]\n",
            "Epoch 592/5000 -- Loss:  0.0001\n",
            "[0.9998772, 2.1008793e-06]\n",
            "Epoch 593/5000 -- Loss:  0.0001\n",
            "[0.9998771, 2.070408e-06]\n",
            "Epoch 594/5000 -- Loss:  0.0001\n",
            "[0.999877, 2.0403863e-06]\n",
            "Epoch 595/5000 -- Loss:  0.0001\n",
            "[0.99987674, 2.0108002e-06]\n",
            "Epoch 596/5000 -- Loss:  0.0001\n",
            "[0.9998765, 1.981643e-06]\n",
            "Epoch 597/5000 -- Loss:  0.0001\n",
            "[0.99987626, 1.9529086e-06]\n",
            "Epoch 598/5000 -- Loss:  0.0001\n",
            "[0.999876, 1.9245908e-06]\n",
            "Epoch 599/5000 -- Loss:  0.0001\n",
            "[0.9998758, 1.8966838e-06]\n",
            "Epoch 600/5000 -- Loss:  0.0001\n",
            "[0.99987555, 1.8691813e-06]\n",
            "Epoch 601/5000 -- Loss:  0.0001\n",
            "[0.9998753, 1.8420775e-06]\n",
            "Epoch 602/5000 -- Loss:  0.0001\n",
            "[0.99987495, 1.815367e-06]\n",
            "Epoch 603/5000 -- Loss:  0.0001\n",
            "[0.9998746, 1.7890436e-06]\n",
            "Epoch 604/5000 -- Loss:  0.0001\n",
            "[0.99987435, 1.7631087e-06]\n",
            "Epoch 605/5000 -- Loss:  0.0001\n",
            "[0.999874, 1.7375464e-06]\n",
            "Epoch 606/5000 -- Loss:  0.0001\n",
            "[0.99987364, 1.7123645e-06]\n",
            "Epoch 607/5000 -- Loss:  0.0001\n",
            "[0.99987316, 1.6875412e-06]\n",
            "Epoch 608/5000 -- Loss:  0.0001\n",
            "[0.9998728, 1.6630839e-06]\n",
            "Epoch 609/5000 -- Loss:  0.0001\n",
            "[0.9998723, 1.6389749e-06]\n",
            "Epoch 610/5000 -- Loss:  0.0001\n",
            "[0.99987197, 1.6152217e-06]\n",
            "Epoch 611/5000 -- Loss:  0.0001\n",
            "[0.9998715, 1.5918066e-06]\n",
            "Epoch 612/5000 -- Loss:  0.0001\n",
            "[0.999871, 1.5687369e-06]\n",
            "Epoch 613/5000 -- Loss:  0.0001\n",
            "[0.9998704, 1.5459957e-06]\n",
            "Epoch 614/5000 -- Loss:  0.0001\n",
            "[0.99986994, 1.52359e-06]\n",
            "Epoch 615/5000 -- Loss:  0.0001\n",
            "[0.99986947, 1.5015032e-06]\n",
            "Epoch 616/5000 -- Loss:  0.0001\n",
            "[0.99986887, 1.4797422e-06]\n",
            "Epoch 617/5000 -- Loss:  0.0001\n",
            "[0.9998683, 1.458291e-06]\n",
            "Epoch 618/5000 -- Loss:  0.0001\n",
            "[0.9998677, 1.4371564e-06]\n",
            "Epoch 619/5000 -- Loss:  0.0001\n",
            "[0.9998671, 1.4163226e-06]\n",
            "Epoch 620/5000 -- Loss:  0.0001\n",
            "[0.99986637, 1.3957988e-06]\n",
            "Epoch 621/5000 -- Loss:  0.0001\n",
            "[0.9998658, 1.3755697e-06]\n",
            "Epoch 622/5000 -- Loss:  0.0001\n",
            "[0.99986506, 1.3556391e-06]\n",
            "Epoch 623/5000 -- Loss:  0.0001\n",
            "[0.99986434, 1.3360024e-06]\n",
            "Epoch 624/5000 -- Loss:  0.0001\n",
            "[0.9998636, 1.316645e-06]\n",
            "Epoch 625/5000 -- Loss:  0.0001\n",
            "[0.9998629, 1.297573e-06]\n",
            "Epoch 626/5000 -- Loss:  0.0001\n",
            "[0.9998622, 1.2787773e-06]\n",
            "Epoch 627/5000 -- Loss:  0.0001\n",
            "[0.99986136, 1.260254e-06]\n",
            "Epoch 628/5000 -- Loss:  0.0001\n",
            "[0.9998605, 1.2419988e-06]\n",
            "Epoch 629/5000 -- Loss:  0.0001\n",
            "[0.9998597, 1.2240081e-06]\n",
            "Epoch 630/5000 -- Loss:  0.0001\n",
            "[0.99985886, 1.2062781e-06]\n",
            "Epoch 631/5000 -- Loss:  0.0001\n",
            "[0.999858, 1.1888047e-06]\n",
            "Epoch 632/5000 -- Loss:  0.0001\n",
            "[0.99985707, 1.1715847e-06]\n",
            "Epoch 633/5000 -- Loss:  0.0001\n",
            "[0.9998561, 1.1546139e-06]\n",
            "Epoch 634/5000 -- Loss:  0.0001\n",
            "[0.99985516, 1.1378912e-06]\n",
            "Epoch 635/5000 -- Loss:  0.0001\n",
            "[0.9998542, 1.1214129e-06]\n",
            "Epoch 636/5000 -- Loss:  0.0001\n",
            "[0.99985325, 1.105173e-06]\n",
            "Epoch 637/5000 -- Loss:  0.0001\n",
            "[0.9998522, 1.0891686e-06]\n",
            "Epoch 638/5000 -- Loss:  0.0001\n",
            "[0.9998511, 1.0733958e-06]\n",
            "Epoch 639/5000 -- Loss:  0.0002\n",
            "[0.99985003, 1.0578514e-06]\n",
            "Epoch 640/5000 -- Loss:  0.0002\n",
            "[0.99984896, 1.0425321e-06]\n",
            "Epoch 641/5000 -- Loss:  0.0002\n",
            "[0.99984777, 1.0274347e-06]\n",
            "Epoch 642/5000 -- Loss:  0.0002\n",
            "[0.9998467, 1.0125597e-06]\n",
            "Epoch 643/5000 -- Loss:  0.0002\n",
            "[0.9998455, 9.978962e-07]\n",
            "Epoch 644/5000 -- Loss:  0.0002\n",
            "[0.9998442, 9.83449e-07]\n",
            "Epoch 645/5000 -- Loss:  0.0002\n",
            "[0.999843, 9.692071e-07]\n",
            "Epoch 646/5000 -- Loss:  0.0002\n",
            "[0.9998417, 9.55177e-07]\n",
            "Epoch 647/5000 -- Loss:  0.0002\n",
            "[0.9998404, 9.413482e-07]\n",
            "Epoch 648/5000 -- Loss:  0.0002\n",
            "[0.99983907, 9.277232e-07]\n",
            "Epoch 649/5000 -- Loss:  0.0002\n",
            "[0.99983764, 9.142918e-07]\n",
            "Epoch 650/5000 -- Loss:  0.0002\n",
            "[0.9998363, 9.010584e-07]\n",
            "Epoch 651/5000 -- Loss:  0.0002\n",
            "[0.9998349, 8.880131e-07]\n",
            "Epoch 652/5000 -- Loss:  0.0002\n",
            "[0.99983335, 8.7515997e-07]\n",
            "Epoch 653/5000 -- Loss:  0.0002\n",
            "[0.9998319, 8.624896e-07]\n",
            "Epoch 654/5000 -- Loss:  0.0002\n",
            "[0.99983037, 8.5000596e-07]\n",
            "Epoch 655/5000 -- Loss:  0.0002\n",
            "[0.9998287, 8.3769976e-07]\n",
            "Epoch 656/5000 -- Loss:  0.0002\n",
            "[0.9998272, 8.2557176e-07]\n",
            "Epoch 657/5000 -- Loss:  0.0002\n",
            "[0.99982554, 8.136225e-07]\n",
            "Epoch 658/5000 -- Loss:  0.0002\n",
            "[0.99982387, 8.0185066e-07]\n",
            "Epoch 659/5000 -- Loss:  0.0002\n",
            "[0.9998221, 7.9025074e-07]\n",
            "Epoch 660/5000 -- Loss:  0.0002\n",
            "[0.9998203, 7.788186e-07]\n",
            "Epoch 661/5000 -- Loss:  0.0002\n",
            "[0.9998185, 7.675518e-07]\n",
            "Epoch 662/5000 -- Loss:  0.0002\n",
            "[0.9998167, 7.5644806e-07]\n",
            "Epoch 663/5000 -- Loss:  0.0002\n",
            "[0.9998148, 7.455049e-07]\n",
            "Epoch 664/5000 -- Loss:  0.0002\n",
            "[0.9998129, 7.347201e-07]\n",
            "Epoch 665/5000 -- Loss:  0.0002\n",
            "[0.9998109, 7.2409125e-07]\n",
            "Epoch 666/5000 -- Loss:  0.0002\n",
            "[0.99980885, 7.1361626e-07]\n",
            "Epoch 667/5000 -- Loss:  0.0002\n",
            "[0.9998068, 7.0329276e-07]\n",
            "Epoch 668/5000 -- Loss:  0.0002\n",
            "[0.9998047, 6.931199e-07]\n",
            "Epoch 669/5000 -- Loss:  0.0002\n",
            "[0.99980253, 6.830955e-07]\n",
            "Epoch 670/5000 -- Loss:  0.0002\n",
            "[0.9998004, 6.732161e-07]\n",
            "Epoch 671/5000 -- Loss:  0.0002\n",
            "[0.9997981, 6.634795e-07]\n",
            "Epoch 672/5000 -- Loss:  0.0002\n",
            "[0.99979573, 6.5388633e-07]\n",
            "Epoch 673/5000 -- Loss:  0.0002\n",
            "[0.99979347, 6.4442935e-07]\n",
            "Epoch 674/5000 -- Loss:  0.0002\n",
            "[0.99979097, 6.351116e-07]\n",
            "Epoch 675/5000 -- Loss:  0.0002\n",
            "[0.9997886, 6.2592613e-07]\n",
            "Epoch 676/5000 -- Loss:  0.0002\n",
            "[0.9997861, 6.1687587e-07]\n",
            "Epoch 677/5000 -- Loss:  0.0002\n",
            "[0.99978346, 6.0795537e-07]\n",
            "Epoch 678/5000 -- Loss:  0.0002\n",
            "[0.99978083, 5.991672e-07]\n",
            "Epoch 679/5000 -- Loss:  0.0002\n",
            "[0.9997781, 5.9050393e-07]\n",
            "Epoch 680/5000 -- Loss:  0.0002\n",
            "[0.99977535, 5.8196804e-07]\n",
            "Epoch 681/5000 -- Loss:  0.0002\n",
            "[0.9997726, 5.735556e-07]\n",
            "Epoch 682/5000 -- Loss:  0.0002\n",
            "[0.9997696, 5.6526477e-07]\n",
            "Epoch 683/5000 -- Loss:  0.0002\n",
            "[0.99976677, 5.5709376e-07]\n",
            "Epoch 684/5000 -- Loss:  0.0002\n",
            "[0.9997638, 5.490429e-07]\n",
            "Epoch 685/5000 -- Loss:  0.0002\n",
            "[0.9997607, 5.411085e-07]\n",
            "Epoch 686/5000 -- Loss:  0.0002\n",
            "[0.99975747, 5.3328966e-07]\n",
            "Epoch 687/5000 -- Loss:  0.0002\n",
            "[0.99975437, 5.255849e-07]\n",
            "Epoch 688/5000 -- Loss:  0.0002\n",
            "[0.99975103, 5.1799145e-07]\n",
            "Epoch 689/5000 -- Loss:  0.0003\n",
            "[0.9997477, 5.105077e-07]\n",
            "Epoch 690/5000 -- Loss:  0.0003\n",
            "[0.99974424, 5.031339e-07]\n",
            "Epoch 691/5000 -- Loss:  0.0003\n",
            "[0.9997408, 4.958648e-07]\n",
            "Epoch 692/5000 -- Loss:  0.0003\n",
            "[0.9997372, 4.887026e-07]\n",
            "Epoch 693/5000 -- Loss:  0.0003\n",
            "[0.9997335, 4.8164287e-07]\n",
            "Epoch 694/5000 -- Loss:  0.0003\n",
            "[0.9997297, 4.7468785e-07]\n",
            "Epoch 695/5000 -- Loss:  0.0003\n",
            "[0.9997259, 4.6783148e-07]\n",
            "Epoch 696/5000 -- Loss:  0.0003\n",
            "[0.99972194, 4.6107596e-07]\n",
            "Epoch 697/5000 -- Loss:  0.0003\n",
            "[0.999718, 4.544162e-07]\n",
            "Epoch 698/5000 -- Loss:  0.0003\n",
            "[0.99971384, 4.4785435e-07]\n",
            "Epoch 699/5000 -- Loss:  0.0003\n",
            "[0.99970967, 4.4138724e-07]\n",
            "Epoch 700/5000 -- Loss:  0.0003\n",
            "[0.9997054, 4.3501439e-07]\n",
            "Epoch 701/5000 -- Loss:  0.0003\n",
            "[0.999701, 4.2873432e-07]\n",
            "Epoch 702/5000 -- Loss:  0.0003\n",
            "[0.9996966, 4.2254658e-07]\n",
            "Epoch 703/5000 -- Loss:  0.0003\n",
            "[0.99969196, 4.1644813e-07]\n",
            "Epoch 704/5000 -- Loss:  0.0003\n",
            "[0.9996873, 4.1043768e-07]\n",
            "Epoch 705/5000 -- Loss:  0.0003\n",
            "[0.9996824, 4.0451553e-07]\n",
            "Epoch 706/5000 -- Loss:  0.0003\n",
            "[0.99967754, 3.9867805e-07]\n",
            "Epoch 707/5000 -- Loss:  0.0003\n",
            "[0.99967253, 3.9292706e-07]\n",
            "Epoch 708/5000 -- Loss:  0.0003\n",
            "[0.9996674, 3.8725756e-07]\n",
            "Epoch 709/5000 -- Loss:  0.0003\n",
            "[0.99966216, 3.8167133e-07]\n",
            "Epoch 710/5000 -- Loss:  0.0003\n",
            "[0.9996568, 3.7616425e-07]\n",
            "Epoch 711/5000 -- Loss:  0.0003\n",
            "[0.9996513, 3.70738e-07]\n",
            "Epoch 712/5000 -- Loss:  0.0004\n",
            "[0.9996456, 3.6539078e-07]\n",
            "Epoch 713/5000 -- Loss:  0.0004\n",
            "[0.99963987, 3.6012136e-07]\n",
            "Epoch 714/5000 -- Loss:  0.0004\n",
            "[0.999634, 3.5492792e-07]\n",
            "Epoch 715/5000 -- Loss:  0.0004\n",
            "[0.99962795, 3.4980934e-07]\n",
            "Epoch 716/5000 -- Loss:  0.0004\n",
            "[0.99962175, 3.4476594e-07]\n",
            "Epoch 717/5000 -- Loss:  0.0004\n",
            "[0.99961543, 3.397972e-07]\n",
            "Epoch 718/5000 -- Loss:  0.0004\n",
            "[0.99960905, 3.348994e-07]\n",
            "Epoch 719/5000 -- Loss:  0.0004\n",
            "[0.9996024, 3.3007348e-07]\n",
            "Epoch 720/5000 -- Loss:  0.0004\n",
            "[0.9995957, 3.2531588e-07]\n",
            "Epoch 721/5000 -- Loss:  0.0004\n",
            "[0.9995888, 3.2062803e-07]\n",
            "Epoch 722/5000 -- Loss:  0.0004\n",
            "[0.99958163, 3.1600837e-07]\n",
            "Epoch 723/5000 -- Loss:  0.0004\n",
            "[0.99957436, 3.1145467e-07]\n",
            "Epoch 724/5000 -- Loss:  0.0004\n",
            "[0.999567, 3.069689e-07]\n",
            "Epoch 725/5000 -- Loss:  0.0004\n",
            "[0.99955934, 3.0254546e-07]\n",
            "Epoch 726/5000 -- Loss:  0.0004\n",
            "[0.9995516, 2.981886e-07]\n",
            "Epoch 727/5000 -- Loss:  0.0005\n",
            "[0.9995436, 2.9389503e-07]\n",
            "Epoch 728/5000 -- Loss:  0.0005\n",
            "[0.99953544, 2.896644e-07]\n",
            "Epoch 729/5000 -- Loss:  0.0005\n",
            "[0.9995271, 2.8549357e-07]\n",
            "Epoch 730/5000 -- Loss:  0.0005\n",
            "[0.9995185, 2.8138442e-07]\n",
            "Epoch 731/5000 -- Loss:  0.0005\n",
            "[0.9995098, 2.7733387e-07]\n",
            "Epoch 732/5000 -- Loss:  0.0005\n",
            "[0.9995009, 2.7334372e-07]\n",
            "Epoch 733/5000 -- Loss:  0.0005\n",
            "[0.9994917, 2.6940893e-07]\n",
            "Epoch 734/5000 -- Loss:  0.0005\n",
            "[0.9994823, 2.6553332e-07]\n",
            "Epoch 735/5000 -- Loss:  0.0005\n",
            "[0.9994727, 2.6171296e-07]\n",
            "Epoch 736/5000 -- Loss:  0.0005\n",
            "[0.9994628, 2.579495e-07]\n",
            "Epoch 737/5000 -- Loss:  0.0005\n",
            "[0.99945265, 2.5423924e-07]\n",
            "Epoch 738/5000 -- Loss:  0.0006\n",
            "[0.9994423, 2.5058372e-07]\n",
            "Epoch 739/5000 -- Loss:  0.0006\n",
            "[0.99943167, 2.4698127e-07]\n",
            "Epoch 740/5000 -- Loss:  0.0006\n",
            "[0.9994209, 2.434297e-07]\n",
            "Epoch 741/5000 -- Loss:  0.0006\n",
            "[0.9994098, 2.3993147e-07]\n",
            "Epoch 742/5000 -- Loss:  0.0006\n",
            "[0.99939835, 2.3648305e-07]\n",
            "Epoch 743/5000 -- Loss:  0.0006\n",
            "[0.99938667, 2.3308597e-07]\n",
            "Epoch 744/5000 -- Loss:  0.0006\n",
            "[0.9993748, 2.2973725e-07]\n",
            "Epoch 745/5000 -- Loss:  0.0006\n",
            "[0.9993625, 2.2643798e-07]\n",
            "Epoch 746/5000 -- Loss:  0.0007\n",
            "[0.99935, 2.2318603e-07]\n",
            "Epoch 747/5000 -- Loss:  0.0007\n",
            "[0.99933714, 2.1998125e-07]\n",
            "Epoch 748/5000 -- Loss:  0.0007\n",
            "[0.99932396, 2.1682287e-07]\n",
            "Epoch 749/5000 -- Loss:  0.0007\n",
            "[0.9993105, 2.1371065e-07]\n",
            "Epoch 750/5000 -- Loss:  0.0007\n",
            "[0.99929667, 2.1064432e-07]\n",
            "Epoch 751/5000 -- Loss:  0.0007\n",
            "[0.99928254, 2.076224e-07]\n",
            "Epoch 752/5000 -- Loss:  0.0007\n",
            "[0.999268, 2.0464303e-07]\n",
            "Epoch 753/5000 -- Loss:  0.0007\n",
            "[0.9992531, 2.0170833e-07]\n",
            "Epoch 754/5000 -- Loss:  0.0008\n",
            "[0.9992379, 1.9881536e-07]\n",
            "Epoch 755/5000 -- Loss:  0.0008\n",
            "[0.99922216, 1.9596499e-07]\n",
            "Epoch 756/5000 -- Loss:  0.0008\n",
            "[0.99920624, 1.9315658e-07]\n",
            "Epoch 757/5000 -- Loss:  0.0008\n",
            "[0.9991898, 1.9038806e-07]\n",
            "Epoch 758/5000 -- Loss:  0.0008\n",
            "[0.99917287, 1.8766103e-07]\n",
            "Epoch 759/5000 -- Loss:  0.0008\n",
            "[0.99915564, 1.8497234e-07]\n",
            "Epoch 760/5000 -- Loss:  0.0009\n",
            "[0.999138, 1.8232322e-07]\n",
            "Epoch 761/5000 -- Loss:  0.0009\n",
            "[0.9991198, 1.7971306e-07]\n",
            "Epoch 762/5000 -- Loss:  0.0009\n",
            "[0.9991013, 1.771406e-07]\n",
            "Epoch 763/5000 -- Loss:  0.0009\n",
            "[0.9990822, 1.7460532e-07]\n",
            "Epoch 764/5000 -- Loss:  0.0009\n",
            "[0.9990627, 1.7210728e-07]\n",
            "Epoch 765/5000 -- Loss:  0.0010\n",
            "[0.9990427, 1.6964597e-07]\n",
            "Epoch 766/5000 -- Loss:  0.0010\n",
            "[0.9990221, 1.6722018e-07]\n",
            "Epoch 767/5000 -- Loss:  0.0010\n",
            "[0.9990012, 1.6482939e-07]\n",
            "Epoch 768/5000 -- Loss:  0.0010\n",
            "[0.9989795, 1.624737e-07]\n",
            "Epoch 769/5000 -- Loss:  0.0010\n",
            "[0.9989574, 1.601526e-07]\n",
            "Epoch 770/5000 -- Loss:  0.0011\n",
            "[0.9989348, 1.5786496e-07]\n",
            "Epoch 771/5000 -- Loss:  0.0011\n",
            "[0.9989115, 1.5561059e-07]\n",
            "Epoch 772/5000 -- Loss:  0.0011\n",
            "[0.9988877, 1.5338986e-07]\n",
            "Epoch 773/5000 -- Loss:  0.0011\n",
            "[0.99886334, 1.5120114e-07]\n",
            "Epoch 774/5000 -- Loss:  0.0012\n",
            "[0.99883825, 1.490445e-07]\n",
            "Epoch 775/5000 -- Loss:  0.0012\n",
            "[0.99881256, 1.4691888e-07]\n",
            "Epoch 776/5000 -- Loss:  0.0012\n",
            "[0.99878615, 1.4482414e-07]\n",
            "Epoch 777/5000 -- Loss:  0.0012\n",
            "[0.99875915, 1.4276063e-07]\n",
            "Epoch 778/5000 -- Loss:  0.0013\n",
            "[0.99873143, 1.4072678e-07]\n",
            "Epoch 779/5000 -- Loss:  0.0013\n",
            "[0.998703, 1.3872244e-07]\n",
            "Epoch 780/5000 -- Loss:  0.0013\n",
            "[0.99867404, 1.3674796e-07]\n",
            "Epoch 781/5000 -- Loss:  0.0014\n",
            "[0.99864405, 1.3480182e-07]\n",
            "Epoch 782/5000 -- Loss:  0.0014\n",
            "[0.9986135, 1.3288391e-07]\n",
            "Epoch 783/5000 -- Loss:  0.0014\n",
            "[0.9985821, 1.3099452e-07]\n",
            "Epoch 784/5000 -- Loss:  0.0015\n",
            "[0.9985499, 1.2913225e-07]\n",
            "Epoch 785/5000 -- Loss:  0.0015\n",
            "[0.99851686, 1.2729693e-07]\n",
            "Epoch 786/5000 -- Loss:  0.0015\n",
            "[0.9984831, 1.2548914e-07]\n",
            "Epoch 787/5000 -- Loss:  0.0016\n",
            "[0.9984484, 1.2370795e-07]\n",
            "Epoch 788/5000 -- Loss:  0.0016\n",
            "[0.9984127, 1.219523e-07]\n",
            "Epoch 789/5000 -- Loss:  0.0016\n",
            "[0.99837637, 1.2022224e-07]\n",
            "Epoch 790/5000 -- Loss:  0.0017\n",
            "[0.99833894, 1.1851808e-07]\n",
            "Epoch 791/5000 -- Loss:  0.0017\n",
            "[0.9983006, 1.1683852e-07]\n",
            "Epoch 792/5000 -- Loss:  0.0017\n",
            "[0.9982613, 1.15183205e-07]\n",
            "Epoch 793/5000 -- Loss:  0.0018\n",
            "[0.998221, 1.13552645e-07]\n",
            "Epoch 794/5000 -- Loss:  0.0018\n",
            "[0.9981797, 1.1194644e-07]\n",
            "Epoch 795/5000 -- Loss:  0.0019\n",
            "[0.99813735, 1.1036337e-07]\n",
            "Epoch 796/5000 -- Loss:  0.0019\n",
            "[0.9980939, 1.0880312e-07]\n",
            "Epoch 797/5000 -- Loss:  0.0020\n",
            "[0.9980495, 1.07266146e-07]\n",
            "Epoch 798/5000 -- Loss:  0.0020\n",
            "[0.9980039, 1.05752086e-07]\n",
            "Epoch 799/5000 -- Loss:  0.0020\n",
            "[0.9979571, 1.04259804e-07]\n",
            "Epoch 800/5000 -- Loss:  0.0021\n",
            "[0.9979091, 1.02788974e-07]\n",
            "Epoch 801/5000 -- Loss:  0.0021\n",
            "[0.99786013, 1.0134044e-07]\n",
            "Epoch 802/5000 -- Loss:  0.0022\n",
            "[0.9978098, 9.99127e-08]\n",
            "Epoch 803/5000 -- Loss:  0.0022\n",
            "[0.9977582, 9.850619e-08]\n",
            "Epoch 804/5000 -- Loss:  0.0023\n",
            "[0.9977054, 9.712042e-08]\n",
            "Epoch 805/5000 -- Loss:  0.0024\n",
            "[0.9976513, 9.5755055e-08]\n",
            "Epoch 806/5000 -- Loss:  0.0024\n",
            "[0.9975957, 9.440997e-08]\n",
            "Epoch 807/5000 -- Loss:  0.0025\n",
            "[0.99753904, 9.308447e-08]\n",
            "Epoch 808/5000 -- Loss:  0.0025\n",
            "[0.9974808, 9.1778475e-08]\n",
            "Epoch 809/5000 -- Loss:  0.0026\n",
            "[0.99742115, 9.049201e-08]\n",
            "Epoch 810/5000 -- Loss:  0.0026\n",
            "[0.99736005, 8.922442e-08]\n",
            "Epoch 811/5000 -- Loss:  0.0027\n",
            "[0.9972976, 8.7975764e-08]\n",
            "Epoch 812/5000 -- Loss:  0.0028\n",
            "[0.99723357, 8.6745246e-08]\n",
            "Epoch 813/5000 -- Loss:  0.0028\n",
            "[0.997168, 8.5532754e-08]\n",
            "Epoch 814/5000 -- Loss:  0.0029\n",
            "[0.9971009, 8.433834e-08]\n",
            "Epoch 815/5000 -- Loss:  0.0030\n",
            "[0.99703217, 8.31617e-08]\n",
            "Epoch 816/5000 -- Loss:  0.0030\n",
            "[0.9969618, 8.200274e-08]\n",
            "Epoch 817/5000 -- Loss:  0.0031\n",
            "[0.9968899, 8.0861014e-08]\n",
            "Epoch 818/5000 -- Loss:  0.0032\n",
            "[0.9968162, 7.9735635e-08]\n",
            "Epoch 819/5000 -- Loss:  0.0033\n",
            "[0.9967409, 7.862772e-08]\n",
            "Epoch 820/5000 -- Loss:  0.0033\n",
            "[0.99666375, 7.753564e-08]\n",
            "Epoch 821/5000 -- Loss:  0.0034\n",
            "[0.9965849, 7.646033e-08]\n",
            "Epoch 822/5000 -- Loss:  0.0035\n",
            "[0.9965043, 7.5400514e-08]\n",
            "Epoch 823/5000 -- Loss:  0.0036\n",
            "[0.9964218, 7.435709e-08]\n",
            "Epoch 824/5000 -- Loss:  0.0037\n",
            "[0.99633753, 7.3328664e-08]\n",
            "Epoch 825/5000 -- Loss:  0.0038\n",
            "[0.9962514, 7.231611e-08]\n",
            "Epoch 826/5000 -- Loss:  0.0038\n",
            "[0.99616325, 7.13181e-08]\n",
            "Epoch 827/5000 -- Loss:  0.0039\n",
            "[0.99607325, 7.033545e-08]\n",
            "Epoch 828/5000 -- Loss:  0.0040\n",
            "[0.99598134, 6.9366884e-08]\n",
            "Epoch 829/5000 -- Loss:  0.0041\n",
            "[0.99588746, 6.8413215e-08]\n",
            "Epoch 830/5000 -- Loss:  0.0042\n",
            "[0.99579144, 6.747369e-08]\n",
            "Epoch 831/5000 -- Loss:  0.0043\n",
            "[0.99569345, 6.654808e-08]\n",
            "Epoch 832/5000 -- Loss:  0.0044\n",
            "[0.9955936, 6.56368e-08]\n",
            "Epoch 833/5000 -- Loss:  0.0045\n",
            "[0.99549156, 6.473886e-08]\n",
            "Epoch 834/5000 -- Loss:  0.0046\n",
            "[0.9953875, 6.385454e-08]\n",
            "Epoch 835/5000 -- Loss:  0.0047\n",
            "[0.9952813, 6.298363e-08]\n",
            "Epoch 836/5000 -- Loss:  0.0048\n",
            "[0.99517304, 6.21259e-08]\n",
            "Epoch 837/5000 -- Loss:  0.0049\n",
            "[0.99506265, 6.12809e-08]\n",
            "Epoch 838/5000 -- Loss:  0.0051\n",
            "[0.99495006, 6.0448656e-08]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-4269c33e6a58>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfeature_representations_3\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0my_pred_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-d7e07e18ba2a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, features, X, X_train, y_train, epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mcum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mxnet/gluon/trainer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, batch_size, ignore_stale_grad)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_allreduce_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_stale_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mallreduce_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mxnet/gluon/trainer.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self, ignore_stale_grad)\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mupd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                     \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mupd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m                     \u001b[0mupdater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mxnet/optimizer/optimizer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, index, grad, weight)\u001b[0m\n\u001b[1;32m   2117\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m                         \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2119\u001b[0;31m                     self.optimizer.update_multi_precision(\n\u001b[0m\u001b[1;32m   2120\u001b[0m                         \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcurrent_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m                         \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcurrent_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mxnet/optimizer/optimizer.py\u001b[0m in \u001b[0;36mupdate_multi_precision\u001b[0;34m(self, index, weight, grad, state)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m             \u001b[0muse_multi_precision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_precision\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         self._update_impl(index, weight, grad, state,\n\u001b[0m\u001b[1;32m    669\u001b[0m                           multi_precision=use_multi_precision)\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mxnet/optimizer/optimizer.py\u001b[0m in \u001b[0;36m_update_impl\u001b[0;34m(self, indices, weights, grads, states, multi_precision)\u001b[0m\n\u001b[1;32m    626\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmulti_precision\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m                     multi_sgd_mom_update(*_flatten_list(zip(weights, grads, states)), out=weights,\n\u001b[0m\u001b[1;32m    629\u001b[0m                                          num_weights=len(weights), lrs=lrs, wds=wds, **kwargs)\n\u001b[1;32m    630\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mxnet/ndarray/register.py\u001b[0m in \u001b[0;36mmulti_sgd_mom_update\u001b[0;34m(*data, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mxnet/_ctypes/ndarray.py\u001b[0m in \u001b[0;36m_imperative_invoke\u001b[0;34m(handle, ndargs, keys, vals, out, is_np_op, output_is_list)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mc_str_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         ctypes.byref(out_stypes)))\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mxnet/_ctypes/ndarray.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mc_str_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         ctypes.byref(out_stypes)))\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above two models, it can be seen that using the cycles that the node appears in definitely increases the accuracy of the model.\n",
        "\n",
        "Moreover, it can be seen that the second model(feature: 0/1 depending on if the node appears in a cycle), performs better than the first."
      ],
      "metadata": {
        "id": "WuQDkSUElOKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proving that hexagons helps boost the accuracy of the model than that of triangles."
      ],
      "metadata": {
        "id": "WZ6Ai93LhHF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below the feature being added is the number of times a node appears in a cycle but the model is split into two-one for triangles and one for hexagons"
      ],
      "metadata": {
        "id": "5sPeBj-Mls3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_7 = nd.zeros((A.shape[0], 1))\n",
        "for i in cycle3_1:\n",
        "  for node in i:\n",
        "     X_7[node][0]+=1\n",
        "X_8 = nd.zeros((A.shape[0], 1))\n",
        "for i in cycle6_1:\n",
        "  for node in i:\n",
        "     X_8[node][0]+=1"
      ],
      "metadata": {
        "id": "wtOC8ES0UVAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "X_7 is the feature that tells us the number of times a node appeared in a cycle of length 3 and X_8 is the feature that tells us the number of times a node appeared in a cycle of length 6."
      ],
      "metadata": {
        "id": "7NiSPtnKl2kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_7=nd.concat(X_1,X_7)\n",
        "model_3, features_3 = build_model(nd.array(A), X_7)\n",
        "print(model_3(X_7))\n",
        "feature_representations_3= train(model_3, features_3, X_7, X_train, y_train, epochs=100)\n",
        "y_pred_3 = predict(model_3, X_7, X_test)\n",
        "print(classification_report(y_test, y_pred_3))"
      ],
      "metadata": {
        "id": "dpfFt22pSJxk",
        "outputId": "b74ff00b-d987-46d1-a72f-27c280a50b4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[[0.6979481 ]\n",
            " [0.69723535]\n",
            " [0.69667363]\n",
            " [0.6951346 ]\n",
            " [0.6881926 ]\n",
            " [0.69643235]\n",
            " [0.70095503]\n",
            " [0.6984027 ]\n",
            " [0.6923291 ]\n",
            " [0.7004488 ]\n",
            " [0.6724116 ]\n",
            " [0.6898331 ]\n",
            " [0.6922736 ]\n",
            " [0.7047528 ]\n",
            " [0.71150243]\n",
            " [0.70099115]\n",
            " [0.69414455]\n",
            " [0.7075158 ]\n",
            " [0.6995478 ]\n",
            " [0.69949377]\n",
            " [0.7046067 ]\n",
            " [0.6977357 ]\n",
            " [0.7041006 ]\n",
            " [0.6977061 ]\n",
            " [0.68337137]\n",
            " [0.67663556]\n",
            " [0.69951326]\n",
            " [0.6949233 ]\n",
            " [0.68514216]\n",
            " [0.69879216]\n",
            " [0.70801044]\n",
            " [0.69696516]\n",
            " [0.6944202 ]\n",
            " [0.68114156]]\n",
            "<NDArray 34x1 @cpu(0)>\n",
            "Epoch 1/100 -- Loss:  1.5028\n",
            "[0.6979481, 0.6812078]\n",
            "Epoch 2/100 -- Loss:  1.5025\n",
            "[0.6979316, 0.6810962]\n",
            "Epoch 3/100 -- Loss:  1.5018\n",
            "[0.6977536, 0.68080664]\n",
            "Epoch 4/100 -- Loss:  1.5009\n",
            "[0.6974141, 0.6803392]\n",
            "Epoch 5/100 -- Loss:  1.4996\n",
            "[0.6969131, 0.67969376]\n",
            "Epoch 6/100 -- Loss:  1.4980\n",
            "[0.69625056, 0.6788703]\n",
            "Epoch 7/100 -- Loss:  1.4960\n",
            "[0.6954265, 0.67786866]\n",
            "Epoch 8/100 -- Loss:  1.4938\n",
            "[0.69444096, 0.6766888]\n",
            "Epoch 9/100 -- Loss:  1.4912\n",
            "[0.69329387, 0.6753305]\n",
            "Epoch 10/100 -- Loss:  1.4884\n",
            "[0.69198525, 0.67379385]\n",
            "Epoch 11/100 -- Loss:  1.4853\n",
            "[0.69051534, 0.6720786]\n",
            "Epoch 12/100 -- Loss:  1.4819\n",
            "[0.6888841, 0.67018473]\n",
            "Epoch 13/100 -- Loss:  1.4782\n",
            "[0.6870916, 0.66811234]\n",
            "Epoch 14/100 -- Loss:  1.4743\n",
            "[0.68513817, 0.6658613]\n",
            "Epoch 15/100 -- Loss:  1.4702\n",
            "[0.68302405, 0.6634318]\n",
            "Epoch 16/100 -- Loss:  1.4658\n",
            "[0.6807494, 0.66082406]\n",
            "Epoch 17/100 -- Loss:  1.4612\n",
            "[0.67831486, 0.65803814]\n",
            "Epoch 18/100 -- Loss:  1.4564\n",
            "[0.6757209, 0.6550746]\n",
            "Epoch 19/100 -- Loss:  1.4514\n",
            "[0.672968, 0.6519338]\n",
            "Epoch 20/100 -- Loss:  1.4463\n",
            "[0.67005694, 0.6486163]\n",
            "Epoch 21/100 -- Loss:  1.4410\n",
            "[0.66698885, 0.64512295]\n",
            "Epoch 22/100 -- Loss:  1.4355\n",
            "[0.6637646, 0.6414547]\n",
            "Epoch 23/100 -- Loss:  1.4300\n",
            "[0.66038555, 0.63761264]\n",
            "Epoch 24/100 -- Loss:  1.4243\n",
            "[0.656853, 0.6335981]\n",
            "Epoch 25/100 -- Loss:  1.4186\n",
            "[0.65316874, 0.62941265]\n",
            "Epoch 26/100 -- Loss:  1.4128\n",
            "[0.6493345, 0.625058]\n",
            "Epoch 27/100 -- Loss:  1.4070\n",
            "[0.6453524, 0.62053627]\n",
            "Epoch 28/100 -- Loss:  1.4011\n",
            "[0.6412248, 0.6158498]\n",
            "Epoch 29/100 -- Loss:  1.3952\n",
            "[0.6369543, 0.6110012]\n",
            "Epoch 30/100 -- Loss:  1.3894\n",
            "[0.63254374, 0.60599333]\n",
            "Epoch 31/100 -- Loss:  1.3836\n",
            "[0.6279963, 0.6008295]\n",
            "Epoch 32/100 -- Loss:  1.3778\n",
            "[0.6233155, 0.5955132]\n",
            "Epoch 33/100 -- Loss:  1.3722\n",
            "[0.6185051, 0.59004843]\n",
            "Epoch 34/100 -- Loss:  1.3666\n",
            "[0.6135692, 0.5844394]\n",
            "Epoch 35/100 -- Loss:  1.3611\n",
            "[0.6085121, 0.5786907]\n",
            "Epoch 36/100 -- Loss:  1.3558\n",
            "[0.60333866, 0.5728073]\n",
            "Epoch 37/100 -- Loss:  1.3506\n",
            "[0.59805375, 0.5667945]\n",
            "Epoch 38/100 -- Loss:  1.3456\n",
            "[0.59266293, 0.5606581]\n",
            "Epoch 39/100 -- Loss:  1.3408\n",
            "[0.58717185, 0.55440396]\n",
            "Epoch 40/100 -- Loss:  1.3362\n",
            "[0.5815864, 0.5480385]\n",
            "Epoch 41/100 -- Loss:  1.3317\n",
            "[0.5759131, 0.54156846]\n",
            "Epoch 42/100 -- Loss:  1.3276\n",
            "[0.57015836, 0.53500086]\n",
            "Epoch 43/100 -- Loss:  1.3236\n",
            "[0.5643292, 0.52834296]\n",
            "Epoch 44/100 -- Loss:  1.3199\n",
            "[0.55843276, 0.52160245]\n",
            "Epoch 45/100 -- Loss:  1.3165\n",
            "[0.5524764, 0.5147872]\n",
            "Epoch 46/100 -- Loss:  1.3134\n",
            "[0.5464678, 0.5079052]\n",
            "Epoch 47/100 -- Loss:  1.3105\n",
            "[0.5404147, 0.5009648]\n",
            "Epoch 48/100 -- Loss:  1.3079\n",
            "[0.53432524, 0.49397463]\n",
            "Epoch 49/100 -- Loss:  1.3056\n",
            "[0.52820754, 0.48694313]\n",
            "Epoch 50/100 -- Loss:  1.3036\n",
            "[0.5220699, 0.47987902]\n",
            "Epoch 51/100 -- Loss:  1.3020\n",
            "[0.5159207, 0.47279128]\n",
            "Epoch 52/100 -- Loss:  1.3006\n",
            "[0.5097683, 0.46568865]\n",
            "Epoch 53/100 -- Loss:  1.2995\n",
            "[0.5036214, 0.45858002]\n",
            "Epoch 54/100 -- Loss:  1.2987\n",
            "[0.4974884, 0.45147425]\n",
            "Epoch 55/100 -- Loss:  1.2982\n",
            "[0.49137774, 0.44438]\n",
            "Epoch 56/100 -- Loss:  1.2980\n",
            "[0.48529783, 0.437306]\n",
            "Epoch 57/100 -- Loss:  1.2981\n",
            "[0.47925702, 0.43026066]\n",
            "Epoch 58/100 -- Loss:  1.2985\n",
            "[0.47326356, 0.4232524]\n",
            "Epoch 59/100 -- Loss:  1.2991\n",
            "[0.4673255, 0.41628933]\n",
            "Epoch 60/100 -- Loss:  1.3000\n",
            "[0.46145076, 0.40937924]\n",
            "Epoch 61/100 -- Loss:  1.3011\n",
            "[0.45564696, 0.40252978]\n",
            "Epoch 62/100 -- Loss:  1.3024\n",
            "[0.44992167, 0.39574832]\n",
            "Epoch 63/100 -- Loss:  1.3040\n",
            "[0.4442821, 0.38904175]\n",
            "Epoch 64/100 -- Loss:  1.3058\n",
            "[0.43873543, 0.3824168]\n",
            "Epoch 65/100 -- Loss:  1.3078\n",
            "[0.43328816, 0.37587982]\n",
            "Epoch 66/100 -- Loss:  1.3099\n",
            "[0.4279469, 0.36943662]\n",
            "Epoch 67/100 -- Loss:  1.3122\n",
            "[0.42271778, 0.36309287]\n",
            "Epoch 68/100 -- Loss:  1.3146\n",
            "[0.4176067, 0.35685366]\n",
            "Epoch 69/100 -- Loss:  1.3171\n",
            "[0.41261926, 0.35072377]\n",
            "Epoch 70/100 -- Loss:  1.3197\n",
            "[0.40776065, 0.34470764]\n",
            "Epoch 71/100 -- Loss:  1.3224\n",
            "[0.4030359, 0.33880913]\n",
            "Epoch 72/100 -- Loss:  1.3252\n",
            "[0.39844963, 0.33303195]\n",
            "Epoch 73/100 -- Loss:  1.3280\n",
            "[0.3940063, 0.32737923]\n",
            "Epoch 74/100 -- Loss:  1.3307\n",
            "[0.38970995, 0.32185376]\n",
            "Epoch 75/100 -- Loss:  1.3335\n",
            "[0.38556433, 0.31645802]\n",
            "Epoch 76/100 -- Loss:  1.3362\n",
            "[0.38157302, 0.3111941]\n",
            "Epoch 77/100 -- Loss:  1.3389\n",
            "[0.37773925, 0.30606368]\n",
            "Epoch 78/100 -- Loss:  1.3415\n",
            "[0.37406605, 0.30106816]\n",
            "Epoch 79/100 -- Loss:  1.3440\n",
            "[0.37055627, 0.29620865]\n",
            "Epoch 80/100 -- Loss:  1.3464\n",
            "[0.36721236, 0.29148588]\n",
            "Epoch 81/100 -- Loss:  1.3486\n",
            "[0.36403674, 0.2869003]\n",
            "Epoch 82/100 -- Loss:  1.3507\n",
            "[0.3610316, 0.28245217]\n",
            "Epoch 83/100 -- Loss:  1.3526\n",
            "[0.35819888, 0.27814144]\n",
            "Epoch 84/100 -- Loss:  1.3543\n",
            "[0.35554042, 0.27396777]\n",
            "Epoch 85/100 -- Loss:  1.3557\n",
            "[0.35305798, 0.26993066]\n",
            "Epoch 86/100 -- Loss:  1.3570\n",
            "[0.35075304, 0.26602942]\n",
            "Epoch 87/100 -- Loss:  1.3579\n",
            "[0.34862712, 0.2622631]\n",
            "Epoch 88/100 -- Loss:  1.3586\n",
            "[0.34668154, 0.25863063]\n",
            "Epoch 89/100 -- Loss:  1.3590\n",
            "[0.34491763, 0.25513086]\n",
            "Epoch 90/100 -- Loss:  1.3591\n",
            "[0.34333658, 0.2517623]\n",
            "Epoch 91/100 -- Loss:  1.3588\n",
            "[0.3419395, 0.24852349]\n",
            "Epoch 92/100 -- Loss:  1.3583\n",
            "[0.3407277, 0.24541283]\n",
            "Epoch 93/100 -- Loss:  1.3573\n",
            "[0.33970207, 0.24242859]\n",
            "Epoch 94/100 -- Loss:  1.3560\n",
            "[0.3388638, 0.23956893]\n",
            "Epoch 95/100 -- Loss:  1.3544\n",
            "[0.33821395, 0.23683196]\n",
            "Epoch 96/100 -- Loss:  1.3523\n",
            "[0.33775356, 0.23421577]\n",
            "Epoch 97/100 -- Loss:  1.3498\n",
            "[0.33748373, 0.23171829]\n",
            "Epoch 98/100 -- Loss:  1.3470\n",
            "[0.33740562, 0.22933745]\n",
            "Epoch 99/100 -- Loss:  1.3437\n",
            "[0.3375203, 0.22707115]\n",
            "Epoch 100/100 -- Loss:  1.3400\n",
            "[0.3378289, 0.22491726]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.50      1.00      0.67        16\n",
            "        True       0.00      0.00      0.00        16\n",
            "\n",
            "    accuracy                           0.50        32\n",
            "   macro avg       0.25      0.50      0.33        32\n",
            "weighted avg       0.25      0.50      0.33        32\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_7=nd.concat(X_1,X_7)\n",
        "model_3, features_3 = build_model(nd.array(A), X_7)\n",
        "print(model_3(X_7))\n",
        "feature_representations_3= train(model_3, features_3, X_7, X_train, y_train, epochs=100)\n",
        "y_pred_3 = predict(model_3, X_7, X_test)\n",
        "print(classification_report(y_test, y_pred_3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2on8-o_uGHoz",
        "outputId": "5e86c16b-5e02-4908-a7d3-ead5a54f1228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[[0.27936447]\n",
            " [0.2794988 ]\n",
            " [0.27933797]\n",
            " [0.27965903]\n",
            " [0.28113008]\n",
            " [0.27946708]\n",
            " [0.27987838]\n",
            " [0.27953252]\n",
            " [0.27940497]\n",
            " [0.2799175 ]\n",
            " [0.27782714]\n",
            " [0.28126898]\n",
            " [0.27940616]\n",
            " [0.2807575 ]\n",
            " [0.27895692]\n",
            " [0.27858493]\n",
            " [0.2791136 ]\n",
            " [0.28012267]\n",
            " [0.2795817 ]\n",
            " [0.27869233]\n",
            " [0.27956566]\n",
            " [0.27938268]\n",
            " [0.28041467]\n",
            " [0.2792799 ]\n",
            " [0.28119674]\n",
            " [0.27838445]\n",
            " [0.27838248]\n",
            " [0.27907333]\n",
            " [0.28018144]\n",
            " [0.27990115]\n",
            " [0.27951166]\n",
            " [0.27992758]\n",
            " [0.27860522]\n",
            " [0.27953458]]\n",
            "<NDArray 34x1 @cpu(0)>\n",
            "Epoch 1/100 -- Loss:  1.6033\n",
            "[0.27936447, 0.2796798]\n",
            "Epoch 2/100 -- Loss:  1.6029\n",
            "[0.27959844, 0.28000283]\n",
            "Epoch 3/100 -- Loss:  1.6021\n",
            "[0.28001016, 0.28050372]\n",
            "Epoch 4/100 -- Loss:  1.6010\n",
            "[0.2805997, 0.28118268]\n",
            "Epoch 5/100 -- Loss:  1.5994\n",
            "[0.28136745, 0.28203997]\n",
            "Epoch 6/100 -- Loss:  1.5975\n",
            "[0.28231347, 0.2830758]\n",
            "Epoch 7/100 -- Loss:  1.5952\n",
            "[0.28343818, 0.28429055]\n",
            "Epoch 8/100 -- Loss:  1.5926\n",
            "[0.2847419, 0.28568462]\n",
            "Epoch 9/100 -- Loss:  1.5896\n",
            "[0.28622502, 0.2872583]\n",
            "Epoch 10/100 -- Loss:  1.5863\n",
            "[0.28788796, 0.2890121]\n",
            "Epoch 11/100 -- Loss:  1.5826\n",
            "[0.28973112, 0.2909464]\n",
            "Epoch 12/100 -- Loss:  1.5787\n",
            "[0.29175493, 0.2930616]\n",
            "Epoch 13/100 -- Loss:  1.5744\n",
            "[0.29395977, 0.29535815]\n",
            "Epoch 14/100 -- Loss:  1.5698\n",
            "[0.29634607, 0.29783636]\n",
            "Epoch 15/100 -- Loss:  1.5650\n",
            "[0.2989141, 0.30049652]\n",
            "Epoch 16/100 -- Loss:  1.5599\n",
            "[0.30166423, 0.30333892]\n",
            "Epoch 17/100 -- Loss:  1.5546\n",
            "[0.30459654, 0.3063636]\n",
            "Epoch 18/100 -- Loss:  1.5490\n",
            "[0.30771124, 0.3095707]\n",
            "Epoch 19/100 -- Loss:  1.5433\n",
            "[0.31100824, 0.31296006]\n",
            "Epoch 20/100 -- Loss:  1.5374\n",
            "[0.31448737, 0.31653145]\n",
            "Epoch 21/100 -- Loss:  1.5313\n",
            "[0.31814831, 0.32028443]\n",
            "Epoch 22/100 -- Loss:  1.5251\n",
            "[0.3219905, 0.32421836]\n",
            "Epoch 23/100 -- Loss:  1.5188\n",
            "[0.3260133, 0.3283324]\n",
            "Epoch 24/100 -- Loss:  1.5124\n",
            "[0.33021563, 0.33262548]\n",
            "Epoch 25/100 -- Loss:  1.5060\n",
            "[0.33459637, 0.33709615]\n",
            "Epoch 26/100 -- Loss:  1.4995\n",
            "[0.33915395, 0.3417428]\n",
            "Epoch 27/100 -- Loss:  1.4930\n",
            "[0.34388655, 0.34656343]\n",
            "Epoch 28/100 -- Loss:  1.4865\n",
            "[0.34879208, 0.35155573]\n",
            "Epoch 29/100 -- Loss:  1.4800\n",
            "[0.353868, 0.356717]\n",
            "Epoch 30/100 -- Loss:  1.4736\n",
            "[0.35911146, 0.3620442]\n",
            "Epoch 31/100 -- Loss:  1.4673\n",
            "[0.36451924, 0.3675338]\n",
            "Epoch 32/100 -- Loss:  1.4611\n",
            "[0.3700876, 0.37318197]\n",
            "Epoch 33/100 -- Loss:  1.4551\n",
            "[0.37581247, 0.37898436]\n",
            "Epoch 34/100 -- Loss:  1.4492\n",
            "[0.38168934, 0.3849362]\n",
            "Epoch 35/100 -- Loss:  1.4435\n",
            "[0.38771328, 0.39103234]\n",
            "Epoch 36/100 -- Loss:  1.4380\n",
            "[0.3938788, 0.39726707]\n",
            "Epoch 37/100 -- Loss:  1.4327\n",
            "[0.40018, 0.40363425]\n",
            "Epoch 38/100 -- Loss:  1.4277\n",
            "[0.4066106, 0.41012737]\n",
            "Epoch 39/100 -- Loss:  1.4230\n",
            "[0.4131638, 0.41673934]\n",
            "Epoch 40/100 -- Loss:  1.4186\n",
            "[0.41983235, 0.4234627]\n",
            "Epoch 41/100 -- Loss:  1.4145\n",
            "[0.4266086, 0.43028957]\n",
            "Epoch 42/100 -- Loss:  1.4108\n",
            "[0.4334844, 0.43721172]\n",
            "Epoch 43/100 -- Loss:  1.4073\n",
            "[0.44045132, 0.44422033]\n",
            "Epoch 44/100 -- Loss:  1.4043\n",
            "[0.4475005, 0.45130655]\n",
            "Epoch 45/100 -- Loss:  1.4016\n",
            "[0.45462275, 0.4584608]\n",
            "Epoch 46/100 -- Loss:  1.3994\n",
            "[0.46180853, 0.46567366]\n",
            "Epoch 47/100 -- Loss:  1.3975\n",
            "[0.46904814, 0.472935]\n",
            "Epoch 48/100 -- Loss:  1.3960\n",
            "[0.47633144, 0.48023486]\n",
            "Epoch 49/100 -- Loss:  1.3950\n",
            "[0.48364836, 0.48756287]\n",
            "Epoch 50/100 -- Loss:  1.3944\n",
            "[0.49098843, 0.49490863]\n",
            "Epoch 51/100 -- Loss:  1.3942\n",
            "[0.49834117, 0.50226164]\n",
            "Epoch 52/100 -- Loss:  1.3944\n",
            "[0.5056961, 0.50961137]\n",
            "Epoch 53/100 -- Loss:  1.3950\n",
            "[0.5130428, 0.51694727]\n",
            "Epoch 54/100 -- Loss:  1.3961\n",
            "[0.5203706, 0.5242589]\n",
            "Epoch 55/100 -- Loss:  1.3976\n",
            "[0.5276692, 0.5315359]\n",
            "Epoch 56/100 -- Loss:  1.3995\n",
            "[0.5349282, 0.5387681]\n",
            "Epoch 57/100 -- Loss:  1.4018\n",
            "[0.5421376, 0.5459456]\n",
            "Epoch 58/100 -- Loss:  1.4045\n",
            "[0.5492875, 0.55305845]\n",
            "Epoch 59/100 -- Loss:  1.4075\n",
            "[0.5563683, 0.5600973]\n",
            "Epoch 60/100 -- Loss:  1.4110\n",
            "[0.5633706, 0.56705296]\n",
            "Epoch 61/100 -- Loss:  1.4147\n",
            "[0.5702854, 0.57391655]\n",
            "Epoch 62/100 -- Loss:  1.4189\n",
            "[0.5771041, 0.5806798]\n",
            "Epoch 63/100 -- Loss:  1.4233\n",
            "[0.5838185, 0.58733445]\n",
            "Epoch 64/100 -- Loss:  1.4280\n",
            "[0.59042066, 0.5938731]\n",
            "Epoch 65/100 -- Loss:  1.4330\n",
            "[0.59690315, 0.6002884]\n",
            "Epoch 66/100 -- Loss:  1.4383\n",
            "[0.6032591, 0.60657364]\n",
            "Epoch 67/100 -- Loss:  1.4438\n",
            "[0.60948193, 0.6127226]\n",
            "Epoch 68/100 -- Loss:  1.4495\n",
            "[0.61556554, 0.6187293]\n",
            "Epoch 69/100 -- Loss:  1.4553\n",
            "[0.62150425, 0.6245885]\n",
            "Epoch 70/100 -- Loss:  1.4614\n",
            "[0.62729305, 0.63029516]\n",
            "Epoch 71/100 -- Loss:  1.4676\n",
            "[0.63292706, 0.6358449]\n",
            "Epoch 72/100 -- Loss:  1.4739\n",
            "[0.6384022, 0.64123356]\n",
            "Epoch 73/100 -- Loss:  1.4803\n",
            "[0.6437145, 0.6464577]\n",
            "Epoch 74/100 -- Loss:  1.4867\n",
            "[0.6488606, 0.65151393]\n",
            "Epoch 75/100 -- Loss:  1.4932\n",
            "[0.6538374, 0.65639955]\n",
            "Epoch 76/100 -- Loss:  1.4997\n",
            "[0.6586423, 0.661112]\n",
            "Epoch 77/100 -- Loss:  1.5061\n",
            "[0.663273, 0.66564924]\n",
            "Epoch 78/100 -- Loss:  1.5126\n",
            "[0.6677276, 0.67000943]\n",
            "Epoch 79/100 -- Loss:  1.5189\n",
            "[0.67200434, 0.6741912]\n",
            "Epoch 80/100 -- Loss:  1.5252\n",
            "[0.676102, 0.67819315]\n",
            "Epoch 81/100 -- Loss:  1.5314\n",
            "[0.68001944, 0.68201464]\n",
            "Epoch 82/100 -- Loss:  1.5374\n",
            "[0.6837559, 0.6856547]\n",
            "Epoch 83/100 -- Loss:  1.5433\n",
            "[0.6873107, 0.6891129]\n",
            "Epoch 84/100 -- Loss:  1.5490\n",
            "[0.69068354, 0.692389]\n",
            "Epoch 85/100 -- Loss:  1.5545\n",
            "[0.69387406, 0.69548285]\n",
            "Epoch 86/100 -- Loss:  1.5598\n",
            "[0.69688225, 0.6983943]\n",
            "Epoch 87/100 -- Loss:  1.5648\n",
            "[0.69970816, 0.70112354]\n",
            "Epoch 88/100 -- Loss:  1.5696\n",
            "[0.70235187, 0.7036708]\n",
            "Epoch 89/100 -- Loss:  1.5741\n",
            "[0.7048137, 0.7060364]\n",
            "Epoch 90/100 -- Loss:  1.5783\n",
            "[0.70709395, 0.7082205]\n",
            "Epoch 91/100 -- Loss:  1.5823\n",
            "[0.70919293, 0.7102237]\n",
            "Epoch 92/100 -- Loss:  1.5859\n",
            "[0.7111112, 0.7120462]\n",
            "Epoch 93/100 -- Loss:  1.5892\n",
            "[0.7128489, 0.7136887]\n",
            "Epoch 94/100 -- Loss:  1.5921\n",
            "[0.7144067, 0.71515137]\n",
            "Epoch 95/100 -- Loss:  1.5947\n",
            "[0.715785, 0.71643466]\n",
            "Epoch 96/100 -- Loss:  1.5969\n",
            "[0.71698403, 0.717539]\n",
            "Epoch 97/100 -- Loss:  1.5988\n",
            "[0.7180043, 0.71846485]\n",
            "Epoch 98/100 -- Loss:  1.6003\n",
            "[0.71884614, 0.71921235]\n",
            "Epoch 99/100 -- Loss:  1.6014\n",
            "[0.71950966, 0.71978176]\n",
            "Epoch 100/100 -- Loss:  1.6021\n",
            "[0.7199954, 0.72017336]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.00      0.00      0.00        16\n",
            "        True       0.50      1.00      0.67        16\n",
            "\n",
            "    accuracy                           0.50        32\n",
            "   macro avg       0.25      0.50      0.33        32\n",
            "weighted avg       0.25      0.50      0.33        32\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_7=nd.concat(X_1,X_7)\n",
        "model_3, features_3 = build_model(nd.array(A), X_7)\n",
        "print(model_3(X_7))\n",
        "feature_representations_3= train(model_3, features_3, X_7, X_train, y_train, epochs=5000)\n",
        "y_pred_3 = predict(model_3, X_7, X_test)\n",
        "print(classification_report(y_test, y_pred_3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT5epDbThGLN",
        "outputId": "7ad9b8e9-6ee1-47c8-da06-02291b90a41d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[[0.5226396 ]\n",
            " [0.5270827 ]\n",
            " [0.52459645]\n",
            " [0.5327337 ]\n",
            " [0.525947  ]\n",
            " [0.5176379 ]\n",
            " [0.52636015]\n",
            " [0.5330097 ]\n",
            " [0.52146995]\n",
            " [0.52803206]\n",
            " [0.5224489 ]\n",
            " [0.528861  ]\n",
            " [0.5275249 ]\n",
            " [0.53305775]\n",
            " [0.5261056 ]\n",
            " [0.52135557]\n",
            " [0.5237588 ]\n",
            " [0.52964896]\n",
            " [0.51887774]\n",
            " [0.52082807]\n",
            " [0.52242947]\n",
            " [0.52237535]\n",
            " [0.51713914]\n",
            " [0.5211786 ]\n",
            " [0.5293599 ]\n",
            " [0.5275892 ]\n",
            " [0.5291563 ]\n",
            " [0.54838187]\n",
            " [0.51632416]\n",
            " [0.52906936]\n",
            " [0.520063  ]\n",
            " [0.53511524]\n",
            " [0.5172097 ]\n",
            " [0.5448391 ]]\n",
            "<NDArray 34x1 @cpu(0)>\n",
            "Epoch 500/5000 -- Loss:  0.0268\n",
            "[0.98598886, 0.0126054445]\n",
            "Epoch 1000/5000 -- Loss:  0.0000\n",
            "[0.9999995, 4.635547e-07]\n",
            "Epoch 1500/5000 -- Loss:  0.0000\n",
            "[1.0, 2.0082274e-11]\n",
            "Epoch 2000/5000 -- Loss:  0.0000\n",
            "[1.0, 8.4027786e-16]\n",
            "Epoch 2500/5000 -- Loss:  0.0000\n",
            "[1.0, 3.7910928e-20]\n",
            "Epoch 3000/5000 -- Loss:  0.0000\n",
            "[1.0, 1.7754366e-24]\n",
            "Epoch 3500/5000 -- Loss:  0.0000\n",
            "[1.0, 8.4442705e-29]\n",
            "Epoch 4000/5000 -- Loss:  0.0000\n",
            "[1.0, 4.045975e-33]\n",
            "Epoch 4500/5000 -- Loss:  0.0000\n",
            "[1.0, 1.9402988e-37]\n",
            "Epoch 5000/5000 -- Loss:  0.0000\n",
            "[1.0, 0.0]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       1.00      0.06      0.12        16\n",
            "        True       0.52      1.00      0.68        16\n",
            "\n",
            "    accuracy                           0.53        32\n",
            "   macro avg       0.76      0.53      0.40        32\n",
            "weighted avg       0.76      0.53      0.40        32\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_8=nd.concat(X_1,X_8)\n",
        "model_3, features_3 = build_model(nd.array(A), X_8)\n",
        "print(model_3(X_8))\n",
        "feature_representations_3= train(model_3, features_3, X_8, X_train, y_train, epochs=100)\n",
        "y_pred_3 = predict(model_3, X_8, X_test)\n",
        "print(classification_report(y_test, y_pred_3))"
      ],
      "metadata": {
        "id": "-KvSQJBjSGHi",
        "outputId": "c85b38a9-a67e-4558-c947-6cafd88bdf30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[[0.42661676]\n",
            " [0.43012822]\n",
            " [0.4293438 ]\n",
            " [0.4338204 ]\n",
            " [0.41684446]\n",
            " [0.42286757]\n",
            " [0.4233009 ]\n",
            " [0.4385064 ]\n",
            " [0.43569726]\n",
            " [0.42301375]\n",
            " [0.43296874]\n",
            " [0.44985288]\n",
            " [0.43570608]\n",
            " [0.44971445]\n",
            " [0.44231096]\n",
            " [0.4489802 ]\n",
            " [0.43373573]\n",
            " [0.43839732]\n",
            " [0.44849566]\n",
            " [0.438458  ]\n",
            " [0.4423559 ]\n",
            " [0.42815545]\n",
            " [0.4228666 ]\n",
            " [0.42631844]\n",
            " [0.44930404]\n",
            " [0.44839764]\n",
            " [0.45018739]\n",
            " [0.4499863 ]\n",
            " [0.4490951 ]\n",
            " [0.43577665]\n",
            " [0.44283745]\n",
            " [0.43882188]\n",
            " [0.44277903]\n",
            " [0.44787207]]\n",
            "<NDArray 34x1 @cpu(0)>\n",
            "Epoch 1/100 -- Loss:  1.4461\n",
            "[0.42661676, 0.4480263]\n",
            "Epoch 2/100 -- Loss:  1.4460\n",
            "[0.42678398, 0.448182]\n",
            "Epoch 3/100 -- Loss:  1.4458\n",
            "[0.42699903, 0.44833884]\n",
            "Epoch 4/100 -- Loss:  1.4455\n",
            "[0.42726186, 0.4484966]\n",
            "Epoch 5/100 -- Loss:  1.4450\n",
            "[0.42757213, 0.44865498]\n",
            "Epoch 6/100 -- Loss:  1.4445\n",
            "[0.42792973, 0.4488137]\n",
            "Epoch 7/100 -- Loss:  1.4438\n",
            "[0.42833436, 0.44897214]\n",
            "Epoch 8/100 -- Loss:  1.4431\n",
            "[0.42878583, 0.44913006]\n",
            "Epoch 9/100 -- Loss:  1.4422\n",
            "[0.4292838, 0.4492869]\n",
            "Epoch 10/100 -- Loss:  1.4412\n",
            "[0.42982802, 0.44944212]\n",
            "Epoch 11/100 -- Loss:  1.4401\n",
            "[0.4304181, 0.44959518]\n",
            "Epoch 12/100 -- Loss:  1.4389\n",
            "[0.43105382, 0.4497453]\n",
            "Epoch 13/100 -- Loss:  1.4376\n",
            "[0.43173474, 0.44989195]\n",
            "Epoch 14/100 -- Loss:  1.4362\n",
            "[0.43246052, 0.45003426]\n",
            "Epoch 15/100 -- Loss:  1.4346\n",
            "[0.4332307, 0.45017153]\n",
            "Epoch 16/100 -- Loss:  1.4330\n",
            "[0.43404496, 0.4503027]\n",
            "Epoch 17/100 -- Loss:  1.4312\n",
            "[0.43490285, 0.45042694]\n",
            "Epoch 18/100 -- Loss:  1.4294\n",
            "[0.4358039, 0.4505432]\n",
            "Epoch 19/100 -- Loss:  1.4274\n",
            "[0.4367476, 0.45065036]\n",
            "Epoch 20/100 -- Loss:  1.4253\n",
            "[0.43773353, 0.45074737]\n",
            "Epoch 21/100 -- Loss:  1.4232\n",
            "[0.43876117, 0.4508329]\n",
            "Epoch 22/100 -- Loss:  1.4209\n",
            "[0.43982998, 0.4509056]\n",
            "Epoch 23/100 -- Loss:  1.4184\n",
            "[0.44093952, 0.45096412]\n",
            "Epoch 24/100 -- Loss:  1.4159\n",
            "[0.4420892, 0.45100704]\n",
            "Epoch 25/100 -- Loss:  1.4133\n",
            "[0.44327846, 0.45103264]\n",
            "Epoch 26/100 -- Loss:  1.4105\n",
            "[0.4445068, 0.45103952]\n",
            "Epoch 27/100 -- Loss:  1.4076\n",
            "[0.44577357, 0.45102584]\n",
            "Epoch 28/100 -- Loss:  1.4047\n",
            "[0.44707826, 0.45098996]\n",
            "Epoch 29/100 -- Loss:  1.4016\n",
            "[0.4484203, 0.45092994]\n",
            "Epoch 30/100 -- Loss:  1.3983\n",
            "[0.44979903, 0.45084405]\n",
            "Epoch 31/100 -- Loss:  1.3950\n",
            "[0.45121396, 0.4507303]\n",
            "Epoch 32/100 -- Loss:  1.3915\n",
            "[0.45266438, 0.45058686]\n",
            "Epoch 33/100 -- Loss:  1.3879\n",
            "[0.45414978, 0.4504118]\n",
            "Epoch 34/100 -- Loss:  1.3842\n",
            "[0.45566946, 0.45020324]\n",
            "Epoch 35/100 -- Loss:  1.3803\n",
            "[0.45722288, 0.44995925]\n",
            "Epoch 36/100 -- Loss:  1.3764\n",
            "[0.45880938, 0.44967803]\n",
            "Epoch 37/100 -- Loss:  1.3723\n",
            "[0.4604282, 0.4493579]\n",
            "Epoch 38/100 -- Loss:  1.3680\n",
            "[0.46207878, 0.44899717]\n",
            "Epoch 39/100 -- Loss:  1.3637\n",
            "[0.46376052, 0.44859436]\n",
            "Epoch 40/100 -- Loss:  1.3592\n",
            "[0.46547267, 0.4481481]\n",
            "Epoch 41/100 -- Loss:  1.3546\n",
            "[0.46721458, 0.44765717]\n",
            "Epoch 42/100 -- Loss:  1.3498\n",
            "[0.46898547, 0.44712052]\n",
            "Epoch 43/100 -- Loss:  1.3449\n",
            "[0.47078463, 0.44653746]\n",
            "Epoch 44/100 -- Loss:  1.3399\n",
            "[0.47261137, 0.4459072]\n",
            "Epoch 45/100 -- Loss:  1.3348\n",
            "[0.474465, 0.4452296]\n",
            "Epoch 46/100 -- Loss:  1.3295\n",
            "[0.47634473, 0.44450432]\n",
            "Epoch 47/100 -- Loss:  1.3241\n",
            "[0.47824976, 0.44373158]\n",
            "Epoch 48/100 -- Loss:  1.3186\n",
            "[0.48017943, 0.4429117]\n",
            "Epoch 49/100 -- Loss:  1.3130\n",
            "[0.48213288, 0.44204527]\n",
            "Epoch 50/100 -- Loss:  1.3073\n",
            "[0.4841094, 0.44113308]\n",
            "Epoch 51/100 -- Loss:  1.3015\n",
            "[0.4861082, 0.44017607]\n",
            "Epoch 52/100 -- Loss:  1.2955\n",
            "[0.48812842, 0.43917552]\n",
            "Epoch 53/100 -- Loss:  1.2895\n",
            "[0.49016944, 0.4381327]\n",
            "Epoch 54/100 -- Loss:  1.2834\n",
            "[0.49223033, 0.43704924]\n",
            "Epoch 55/100 -- Loss:  1.2772\n",
            "[0.4943103, 0.43592682]\n",
            "Epoch 56/100 -- Loss:  1.2709\n",
            "[0.49640864, 0.43476716]\n",
            "Epoch 57/100 -- Loss:  1.2645\n",
            "[0.4985244, 0.43357232]\n",
            "Epoch 58/100 -- Loss:  1.2581\n",
            "[0.50065684, 0.43234426]\n",
            "Epoch 59/100 -- Loss:  1.2516\n",
            "[0.5028051, 0.43108505]\n",
            "Epoch 60/100 -- Loss:  1.2450\n",
            "[0.5049683, 0.42979702]\n",
            "Epoch 61/100 -- Loss:  1.2384\n",
            "[0.5071455, 0.42848226]\n",
            "Epoch 62/100 -- Loss:  1.2318\n",
            "[0.5093359, 0.4271432]\n",
            "Epoch 63/100 -- Loss:  1.2251\n",
            "[0.5115384, 0.42578214]\n",
            "Epoch 64/100 -- Loss:  1.2184\n",
            "[0.51375204, 0.4244013]\n",
            "Epoch 65/100 -- Loss:  1.2116\n",
            "[0.5159757, 0.42300314]\n",
            "Epoch 66/100 -- Loss:  1.2048\n",
            "[0.5182084, 0.4215898]\n",
            "Epoch 67/100 -- Loss:  1.1981\n",
            "[0.5204488, 0.42016342]\n",
            "Epoch 68/100 -- Loss:  1.1913\n",
            "[0.5226957, 0.41872603]\n",
            "Epoch 69/100 -- Loss:  1.1845\n",
            "[0.5249478, 0.41727942]\n",
            "Epoch 70/100 -- Loss:  1.1777\n",
            "[0.5272038, 0.41582504]\n",
            "Epoch 71/100 -- Loss:  1.1710\n",
            "[0.52946246, 0.41436422]\n",
            "Epoch 72/100 -- Loss:  1.1642\n",
            "[0.5317226, 0.41289762]\n",
            "Epoch 73/100 -- Loss:  1.1574\n",
            "[0.5339837, 0.41142562]\n",
            "Epoch 74/100 -- Loss:  1.1507\n",
            "[0.5362458, 0.40994805]\n",
            "Epoch 75/100 -- Loss:  1.1440\n",
            "[0.53850996, 0.40846437]\n",
            "Epoch 76/100 -- Loss:  1.1373\n",
            "[0.5407787, 0.40697303]\n",
            "Epoch 77/100 -- Loss:  1.1305\n",
            "[0.543056, 0.4054723]\n",
            "Epoch 78/100 -- Loss:  1.1238\n",
            "[0.5453472, 0.4039599]\n",
            "Epoch 79/100 -- Loss:  1.1170\n",
            "[0.547658, 0.402433]\n",
            "Epoch 80/100 -- Loss:  1.1102\n",
            "[0.5499934, 0.40088847]\n",
            "Epoch 81/100 -- Loss:  1.1033\n",
            "[0.55235595, 0.3993228]\n",
            "Epoch 82/100 -- Loss:  1.0963\n",
            "[0.5547449, 0.39773238]\n",
            "Epoch 83/100 -- Loss:  1.0893\n",
            "[0.55715626, 0.3961132]\n",
            "Epoch 84/100 -- Loss:  1.0822\n",
            "[0.55958366, 0.39446124]\n",
            "Epoch 85/100 -- Loss:  1.0751\n",
            "[0.5620198, 0.3927722]\n",
            "Epoch 86/100 -- Loss:  1.0679\n",
            "[0.5644574, 0.3910418]\n",
            "Epoch 87/100 -- Loss:  1.0607\n",
            "[0.5668901, 0.3892655]\n",
            "Epoch 88/100 -- Loss:  1.0534\n",
            "[0.56931305, 0.38743886]\n",
            "Epoch 89/100 -- Loss:  1.0461\n",
            "[0.5717223, 0.38555747]\n",
            "Epoch 90/100 -- Loss:  1.0388\n",
            "[0.5741153, 0.38361692]\n",
            "Epoch 91/100 -- Loss:  1.0314\n",
            "[0.57648975, 0.38161302]\n",
            "Epoch 92/100 -- Loss:  1.0240\n",
            "[0.5788442, 0.37954155]\n",
            "Epoch 93/100 -- Loss:  1.0165\n",
            "[0.5811774, 0.3773987]\n",
            "Epoch 94/100 -- Loss:  1.0090\n",
            "[0.5834883, 0.37518096]\n",
            "Epoch 95/100 -- Loss:  1.0014\n",
            "[0.585776, 0.37288484]\n",
            "Epoch 96/100 -- Loss:  0.9938\n",
            "[0.5880394, 0.37050727]\n",
            "Epoch 97/100 -- Loss:  0.9861\n",
            "[0.59027797, 0.3680455]\n",
            "Epoch 98/100 -- Loss:  0.9783\n",
            "[0.5924908, 0.36549714]\n",
            "Epoch 99/100 -- Loss:  0.9705\n",
            "[0.5946772, 0.36286002]\n",
            "Epoch 100/100 -- Loss:  0.9626\n",
            "[0.59683645, 0.3601323]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.65      0.81      0.72        16\n",
            "        True       0.75      0.56      0.64        16\n",
            "\n",
            "    accuracy                           0.69        32\n",
            "   macro avg       0.70      0.69      0.68        32\n",
            "weighted avg       0.70      0.69      0.68        32\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_8=nd.concat(X_1,X_8)\n",
        "model_3, features_3 = build_model(nd.array(A), X_8)\n",
        "print(model_3(X_8))\n",
        "feature_representations_3= train(model_3, features_3, X_8, X_train, y_train, epochs=100)\n",
        "y_pred_3 = predict(model_3, X_8, X_test)\n",
        "print(classification_report(y_test, y_pred_3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc8BzSqwGMab",
        "outputId": "1b31e604-63d9-4002-83ba-d4d4136383e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[[0.29017612]\n",
            " [0.30060542]\n",
            " [0.29830104]\n",
            " [0.31143436]\n",
            " [0.28589392]\n",
            " [0.28136402]\n",
            " [0.2860643 ]\n",
            " [0.32562587]\n",
            " [0.3173949 ]\n",
            " [0.28352487]\n",
            " [0.22289181]\n",
            " [0.35652426]\n",
            " [0.3173949 ]\n",
            " [0.3564279 ]\n",
            " [0.33761957]\n",
            " [0.3564479 ]\n",
            " [0.31143436]\n",
            " [0.32562578]\n",
            " [0.35630083]\n",
            " [0.32562578]\n",
            " [0.33761957]\n",
            " [0.2947563 ]\n",
            " [0.3037258 ]\n",
            " [0.28935063]\n",
            " [0.35652372]\n",
            " [0.35640827]\n",
            " [0.3563365 ]\n",
            " [0.35655147]\n",
            " [0.35640767]\n",
            " [0.317395  ]\n",
            " [0.33770016]\n",
            " [0.32562813]\n",
            " [0.3376607 ]\n",
            " [0.3573119 ]]\n",
            "<NDArray 34x1 @cpu(0)>\n",
            "Epoch 1/100 -- Loss:  1.6797\n",
            "[0.29017612, 0.35749745]\n",
            "Epoch 2/100 -- Loss:  1.6793\n",
            "[0.29039285, 0.35777488]\n",
            "Epoch 3/100 -- Loss:  1.6787\n",
            "[0.29074258, 0.35814387]\n",
            "Epoch 4/100 -- Loss:  1.6778\n",
            "[0.29122522, 0.3586041]\n",
            "Epoch 5/100 -- Loss:  1.6765\n",
            "[0.29184082, 0.35915515]\n",
            "Epoch 6/100 -- Loss:  1.6750\n",
            "[0.29258937, 0.35979646]\n",
            "Epoch 7/100 -- Loss:  1.6731\n",
            "[0.29347092, 0.36052743]\n",
            "Epoch 8/100 -- Loss:  1.6709\n",
            "[0.29448545, 0.36134726]\n",
            "Epoch 9/100 -- Loss:  1.6685\n",
            "[0.29563296, 0.36225522]\n",
            "Epoch 10/100 -- Loss:  1.6657\n",
            "[0.29691353, 0.36325032]\n",
            "Epoch 11/100 -- Loss:  1.6626\n",
            "[0.2983271, 0.36433157]\n",
            "Epoch 12/100 -- Loss:  1.6593\n",
            "[0.2998737, 0.36549792]\n",
            "Epoch 13/100 -- Loss:  1.6557\n",
            "[0.3015533, 0.36674803]\n",
            "Epoch 14/100 -- Loss:  1.6518\n",
            "[0.30336586, 0.36808068]\n",
            "Epoch 15/100 -- Loss:  1.6477\n",
            "[0.30531123, 0.36949435]\n",
            "Epoch 16/100 -- Loss:  1.6432\n",
            "[0.30738935, 0.37098762]\n",
            "Epoch 17/100 -- Loss:  1.6386\n",
            "[0.30960009, 0.3725588]\n",
            "Epoch 18/100 -- Loss:  1.6337\n",
            "[0.31194314, 0.37420616]\n",
            "Epoch 19/100 -- Loss:  1.6285\n",
            "[0.3144183, 0.37592787]\n",
            "Epoch 20/100 -- Loss:  1.6231\n",
            "[0.31702515, 0.37772197]\n",
            "Epoch 21/100 -- Loss:  1.6175\n",
            "[0.3197633, 0.37958637]\n",
            "Epoch 22/100 -- Loss:  1.6117\n",
            "[0.32263228, 0.38151902]\n",
            "Epoch 23/100 -- Loss:  1.6057\n",
            "[0.32563144, 0.3835175]\n",
            "Epoch 24/100 -- Loss:  1.5995\n",
            "[0.32876012, 0.38557953]\n",
            "Epoch 25/100 -- Loss:  1.5931\n",
            "[0.33201742, 0.38770267]\n",
            "Epoch 26/100 -- Loss:  1.5865\n",
            "[0.3354025, 0.38988423]\n",
            "Epoch 27/100 -- Loss:  1.5798\n",
            "[0.33891422, 0.39212164]\n",
            "Epoch 28/100 -- Loss:  1.5729\n",
            "[0.34255138, 0.39441204]\n",
            "Epoch 29/100 -- Loss:  1.5658\n",
            "[0.3463127, 0.39675263]\n",
            "Epoch 30/100 -- Loss:  1.5587\n",
            "[0.3501966, 0.3991404]\n",
            "Epoch 31/100 -- Loss:  1.5513\n",
            "[0.3542014, 0.4015723]\n",
            "Epoch 32/100 -- Loss:  1.5439\n",
            "[0.3583254, 0.40404516]\n",
            "Epoch 33/100 -- Loss:  1.5364\n",
            "[0.3625664, 0.40655583]\n",
            "Epoch 34/100 -- Loss:  1.5287\n",
            "[0.36692235, 0.40910104]\n",
            "Epoch 35/100 -- Loss:  1.5210\n",
            "[0.3713908, 0.41167736]\n",
            "Epoch 36/100 -- Loss:  1.5132\n",
            "[0.3759692, 0.41428137]\n",
            "Epoch 37/100 -- Loss:  1.5053\n",
            "[0.38065478, 0.4169097]\n",
            "Epoch 38/100 -- Loss:  1.4973\n",
            "[0.38544452, 0.4195586]\n",
            "Epoch 39/100 -- Loss:  1.4893\n",
            "[0.39033532, 0.42222467]\n",
            "Epoch 40/100 -- Loss:  1.4813\n",
            "[0.39532375, 0.42490423]\n",
            "Epoch 41/100 -- Loss:  1.4732\n",
            "[0.4004063, 0.42759368]\n",
            "Epoch 42/100 -- Loss:  1.4651\n",
            "[0.40557906, 0.4302892]\n",
            "Epoch 43/100 -- Loss:  1.4569\n",
            "[0.41083822, 0.43298727]\n",
            "Epoch 44/100 -- Loss:  1.4488\n",
            "[0.41617954, 0.4356841]\n",
            "Epoch 45/100 -- Loss:  1.4406\n",
            "[0.42159858, 0.43837604]\n",
            "Epoch 46/100 -- Loss:  1.4325\n",
            "[0.42709088, 0.44105932]\n",
            "Epoch 47/100 -- Loss:  1.4243\n",
            "[0.43265164, 0.44373032]\n",
            "Epoch 48/100 -- Loss:  1.4162\n",
            "[0.43827605, 0.44638538]\n",
            "Epoch 49/100 -- Loss:  1.4081\n",
            "[0.44395897, 0.44902077]\n",
            "Epoch 50/100 -- Loss:  1.4000\n",
            "[0.44969526, 0.45163292]\n",
            "Epoch 51/100 -- Loss:  1.3919\n",
            "[0.4554795, 0.45421824]\n",
            "Epoch 52/100 -- Loss:  1.3839\n",
            "[0.46130627, 0.45677325]\n",
            "Epoch 53/100 -- Loss:  1.3759\n",
            "[0.46716994, 0.4592945]\n",
            "Epoch 54/100 -- Loss:  1.3680\n",
            "[0.4730649, 0.46177843]\n",
            "Epoch 55/100 -- Loss:  1.3601\n",
            "[0.47898522, 0.46422178]\n",
            "Epoch 56/100 -- Loss:  1.3523\n",
            "[0.4849252, 0.4666211]\n",
            "Epoch 57/100 -- Loss:  1.3445\n",
            "[0.4908789, 0.4689733]\n",
            "Epoch 58/100 -- Loss:  1.3368\n",
            "[0.49684042, 0.4712751]\n",
            "Epoch 59/100 -- Loss:  1.3291\n",
            "[0.5028037, 0.47352338]\n",
            "Epoch 60/100 -- Loss:  1.3215\n",
            "[0.50876284, 0.47571516]\n",
            "Epoch 61/100 -- Loss:  1.3139\n",
            "[0.51471186, 0.47784734]\n",
            "Epoch 62/100 -- Loss:  1.3065\n",
            "[0.5206449, 0.47991714]\n",
            "Epoch 63/100 -- Loss:  1.2990\n",
            "[0.526556, 0.4819216]\n",
            "Epoch 64/100 -- Loss:  1.2917\n",
            "[0.5324393, 0.48385802]\n",
            "Epoch 65/100 -- Loss:  1.2844\n",
            "[0.5382892, 0.4857236]\n",
            "Epoch 66/100 -- Loss:  1.2771\n",
            "[0.5440998, 0.48751566]\n",
            "Epoch 67/100 -- Loss:  1.2699\n",
            "[0.54986566, 0.48923162]\n",
            "Epoch 68/100 -- Loss:  1.2628\n",
            "[0.55558133, 0.49086902]\n",
            "Epoch 69/100 -- Loss:  1.2557\n",
            "[0.5612414, 0.49242514]\n",
            "Epoch 70/100 -- Loss:  1.2487\n",
            "[0.56684065, 0.49389762]\n",
            "Epoch 71/100 -- Loss:  1.2417\n",
            "[0.5723741, 0.4952841]\n",
            "Epoch 72/100 -- Loss:  1.2348\n",
            "[0.5778368, 0.49658206]\n",
            "Epoch 73/100 -- Loss:  1.2279\n",
            "[0.5832239, 0.49778917]\n",
            "Epoch 74/100 -- Loss:  1.2211\n",
            "[0.58853084, 0.49890295]\n",
            "Epoch 75/100 -- Loss:  1.2143\n",
            "[0.59375334, 0.4999212]\n",
            "Epoch 76/100 -- Loss:  1.2075\n",
            "[0.598887, 0.50084156]\n",
            "Epoch 77/100 -- Loss:  1.2008\n",
            "[0.6039278, 0.50166154]\n",
            "Epoch 78/100 -- Loss:  1.1941\n",
            "[0.60887176, 0.50237894]\n",
            "Epoch 79/100 -- Loss:  1.1874\n",
            "[0.6137154, 0.50299126]\n",
            "Epoch 80/100 -- Loss:  1.1807\n",
            "[0.61845493, 0.5034963]\n",
            "Epoch 81/100 -- Loss:  1.1740\n",
            "[0.6230872, 0.5038915]\n",
            "Epoch 82/100 -- Loss:  1.1674\n",
            "[0.627609, 0.50417453]\n",
            "Epoch 83/100 -- Loss:  1.1607\n",
            "[0.6320173, 0.5043429]\n",
            "Epoch 84/100 -- Loss:  1.1540\n",
            "[0.6363094, 0.50439423]\n",
            "Epoch 85/100 -- Loss:  1.1474\n",
            "[0.6404825, 0.5043259]\n",
            "Epoch 86/100 -- Loss:  1.1407\n",
            "[0.64453423, 0.5041355]\n",
            "Epoch 87/100 -- Loss:  1.1340\n",
            "[0.6484623, 0.5038204]\n",
            "Epoch 88/100 -- Loss:  1.1272\n",
            "[0.65226454, 0.50337803]\n",
            "Epoch 89/100 -- Loss:  1.1205\n",
            "[0.655939, 0.50280577]\n",
            "Epoch 90/100 -- Loss:  1.1137\n",
            "[0.65948373, 0.5021011]\n",
            "Epoch 91/100 -- Loss:  1.1068\n",
            "[0.66289717, 0.5012614]\n",
            "Epoch 92/100 -- Loss:  1.0999\n",
            "[0.6661777, 0.50028396]\n",
            "Epoch 93/100 -- Loss:  1.0930\n",
            "[0.6693238, 0.49916622]\n",
            "Epoch 94/100 -- Loss:  1.0860\n",
            "[0.6723343, 0.4979056]\n",
            "Epoch 95/100 -- Loss:  1.0789\n",
            "[0.675208, 0.4964996]\n",
            "Epoch 96/100 -- Loss:  1.0718\n",
            "[0.6779437, 0.4949456]\n",
            "Epoch 97/100 -- Loss:  1.0646\n",
            "[0.6805406, 0.4932412]\n",
            "Epoch 98/100 -- Loss:  1.0573\n",
            "[0.68299776, 0.49138394]\n",
            "Epoch 99/100 -- Loss:  1.0500\n",
            "[0.68531436, 0.4893718]\n",
            "Epoch 100/100 -- Loss:  1.0426\n",
            "[0.68748987, 0.48720247]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.78      0.44      0.56        16\n",
            "        True       0.61      0.88      0.72        16\n",
            "\n",
            "    accuracy                           0.66        32\n",
            "   macro avg       0.69      0.66      0.64        32\n",
            "weighted avg       0.69      0.66      0.64        32\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_8=nd.concat(X_1,X_8)\n",
        "model_3, features_3 = build_model(nd.array(A), X_8)\n",
        "print(model_3(X_8))\n",
        "feature_representations_3= train(model_3, features_3, X_8, X_train, y_train, epochs=5000)\n",
        "y_pred_3 = predict(model_3, X_8, X_test)\n",
        "print(classification_report(y_test, y_pred_3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq1FE2y7hRf4",
        "outputId": "a924cbf7-7da9-4728-c22b-f475ac7ef48a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[[0.41294786]\n",
            " [0.40638703]\n",
            " [0.40782818]\n",
            " [0.39965987]\n",
            " [0.42757154]\n",
            " [0.41249886]\n",
            " [0.4124183 ]\n",
            " [0.39089724]\n",
            " [0.3959788 ]\n",
            " [0.4013664 ]\n",
            " [0.3229094 ]\n",
            " [0.37121022]\n",
            " [0.3959788 ]\n",
            " [0.37117326]\n",
            " [0.3834654 ]\n",
            " [0.37128854]\n",
            " [0.3996607 ]\n",
            " [0.39089975]\n",
            " [0.37159806]\n",
            " [0.39089972]\n",
            " [0.38346535]\n",
            " [0.41005385]\n",
            " [0.42828467]\n",
            " [0.41347203]\n",
            " [0.36991102]\n",
            " [0.37086582]\n",
            " [0.3701081 ]\n",
            " [0.37095162]\n",
            " [0.37077492]\n",
            " [0.39597633]\n",
            " [0.38313347]\n",
            " [0.39084432]\n",
            " [0.38328496]\n",
            " [0.36455622]]\n",
            "<NDArray 34x1 @cpu(0)>\n",
            "Epoch 500/5000 -- Loss:  0.0030\n",
            "[0.99964356, 0.0026111582]\n",
            "Epoch 1000/5000 -- Loss:  0.0000\n",
            "[0.9999869, 2.88871e-07]\n",
            "Epoch 1500/5000 -- Loss:  0.0000\n",
            "[0.99999976, 3.262374e-11]\n",
            "Epoch 2000/5000 -- Loss:  0.0000\n",
            "[1.0, 3.7547327e-15]\n",
            "Epoch 2500/5000 -- Loss:  0.0000\n",
            "[1.0, 4.404345e-19]\n",
            "Epoch 3000/5000 -- Loss:  0.0000\n",
            "[1.0, 5.2654313e-23]\n",
            "Epoch 3500/5000 -- Loss:  0.0000\n",
            "[1.0, 6.4340266e-27]\n",
            "Epoch 4000/5000 -- Loss:  0.0000\n",
            "[1.0, 8.016917e-31]\n",
            "Epoch 4500/5000 -- Loss:  0.0000\n",
            "[1.0, 1.0189046e-34]\n",
            "Epoch 5000/5000 -- Loss:  0.0000\n",
            "[1.0, 1.3213171e-38]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.59      0.81      0.68        16\n",
            "        True       0.70      0.44      0.54        16\n",
            "\n",
            "    accuracy                           0.62        32\n",
            "   macro avg       0.65      0.62      0.61        32\n",
            "weighted avg       0.65      0.62      0.61        32\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above two models, it can be seen that by concatenating identity matrix and X_8 together, we get a higher accuracy than that of X_7 which proves our theory that hexagons are stronger and add more value."
      ],
      "metadata": {
        "id": "SkJkaGmHhsQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below the feature being added iseither 0/1 for a node, depending on if it appears in a cycle but the model is split into two-one for triangles and one for hexagons"
      ],
      "metadata": {
        "id": "BEi1-BhYhZbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_9 = nd.zeros((A.shape[0], 1))\n",
        "for i in cycle3_1:\n",
        "  for node in i:\n",
        "     X_9[node][0]=1\n",
        "X_10 = nd.zeros((A.shape[0], 1))\n",
        "for i in cycle6_1:\n",
        "  for node in i:\n",
        "     X_10[node][0]=1"
      ],
      "metadata": {
        "id": "cZc5CbH6hY8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_10 = nd.zeros((A.shape[0], 1))\n",
        "for i in cycle6_1:\n",
        "  for node in i:\n",
        "     X_10[node][0]=1"
      ],
      "metadata": {
        "id": "Zu_YEPHnieSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "X_9 consists of the features that tells us if a node appeared in a cycle of length 3 or not and X_10 consists of the features that tells us if a node appeared in a cycle of length 6 or not."
      ],
      "metadata": {
        "id": "2Zv6OwsimbIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylrAjmgIikwV",
        "outputId": "3b1361f5-431b-4325-a7d9-9c6d9e019fb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[[1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [0.]\n",
              " [1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [0.]\n",
              " [0.]\n",
              " [0.]\n",
              " [0.]\n",
              " [0.]\n",
              " [0.]]\n",
              "<NDArray 34x1 @cpu(0)>"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_9=nd.concat(X_1,X_9)\n",
        "model_3, features_3 = build_model(nd.array(A), X_9)\n",
        "print(model_3(X_9))\n",
        "feature_representations_3= train(model_3, features_3, X_9, X_train, y_train, epochs=100)\n",
        "y_pred_3 = predict(model_3, X_9, X_test)\n",
        "print(classification_report(y_test, y_pred_3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26myqqb3RxET",
        "outputId": "acf0dd72-e4ce-4694-d3a1-e7529838fea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[[0.5969849 ]\n",
            " [0.596696  ]\n",
            " [0.5967169 ]\n",
            " [0.5959865 ]\n",
            " [0.5965447 ]\n",
            " [0.5962217 ]\n",
            " [0.5965033 ]\n",
            " [0.5978034 ]\n",
            " [0.59678304]\n",
            " [0.59824425]\n",
            " [0.59721655]\n",
            " [0.5946266 ]\n",
            " [0.5985661 ]\n",
            " [0.59653646]\n",
            " [0.5959624 ]\n",
            " [0.59894526]\n",
            " [0.59789693]\n",
            " [0.5946882 ]\n",
            " [0.5969403 ]\n",
            " [0.5964127 ]\n",
            " [0.59616995]\n",
            " [0.5966195 ]\n",
            " [0.6025984 ]\n",
            " [0.596848  ]\n",
            " [0.5991243 ]\n",
            " [0.60129046]\n",
            " [0.5991148 ]\n",
            " [0.5990671 ]\n",
            " [0.60075605]\n",
            " [0.59564507]\n",
            " [0.59714526]\n",
            " [0.5977937 ]\n",
            " [0.59494823]\n",
            " [0.6007388 ]]\n",
            "<NDArray 34x1 @cpu(0)>\n",
            "Epoch 1/100 -- Loss:  1.4342\n",
            "[0.5969849, 0.60083556]\n",
            "Epoch 2/100 -- Loss:  1.4342\n",
            "[0.59703416, 0.6008321]\n",
            "Epoch 3/100 -- Loss:  1.4340\n",
            "[0.59698784, 0.6007283]\n",
            "Epoch 4/100 -- Loss:  1.4337\n",
            "[0.596846, 0.60052437]\n",
            "Epoch 5/100 -- Loss:  1.4333\n",
            "[0.59660876, 0.6002203]\n",
            "Epoch 6/100 -- Loss:  1.4329\n",
            "[0.59627634, 0.59981644]\n",
            "Epoch 7/100 -- Loss:  1.4323\n",
            "[0.5958489, 0.5993131]\n",
            "Epoch 8/100 -- Loss:  1.4317\n",
            "[0.5953269, 0.5987106]\n",
            "Epoch 9/100 -- Loss:  1.4310\n",
            "[0.59471065, 0.59800947]\n",
            "Epoch 10/100 -- Loss:  1.4302\n",
            "[0.59400064, 0.5972102]\n",
            "Epoch 11/100 -- Loss:  1.4293\n",
            "[0.59319746, 0.5963134]\n",
            "Epoch 12/100 -- Loss:  1.4284\n",
            "[0.5923017, 0.59531975]\n",
            "Epoch 13/100 -- Loss:  1.4274\n",
            "[0.59131414, 0.5942302]\n",
            "Epoch 14/100 -- Loss:  1.4263\n",
            "[0.5902355, 0.5930453]\n",
            "Epoch 15/100 -- Loss:  1.4251\n",
            "[0.58906657, 0.59176624]\n",
            "Epoch 16/100 -- Loss:  1.4239\n",
            "[0.5878084, 0.5903939]\n",
            "Epoch 17/100 -- Loss:  1.4226\n",
            "[0.58646196, 0.5889295]\n",
            "Epoch 18/100 -- Loss:  1.4213\n",
            "[0.5850284, 0.5873742]\n",
            "Epoch 19/100 -- Loss:  1.4199\n",
            "[0.58350897, 0.58572924]\n",
            "Epoch 20/100 -- Loss:  1.4185\n",
            "[0.5819049, 0.58399606]\n",
            "Epoch 21/100 -- Loss:  1.4170\n",
            "[0.5802175, 0.5821761]\n",
            "Epoch 22/100 -- Loss:  1.4156\n",
            "[0.57844824, 0.5802709]\n",
            "Epoch 23/100 -- Loss:  1.4140\n",
            "[0.5765987, 0.5782821]\n",
            "Epoch 24/100 -- Loss:  1.4125\n",
            "[0.5746705, 0.5762115]\n",
            "Epoch 25/100 -- Loss:  1.4109\n",
            "[0.5726654, 0.574061]\n",
            "Epoch 26/100 -- Loss:  1.4093\n",
            "[0.5705852, 0.5718323]\n",
            "Epoch 27/100 -- Loss:  1.4077\n",
            "[0.56843174, 0.56952757]\n",
            "Epoch 28/100 -- Loss:  1.4062\n",
            "[0.56620705, 0.567149]\n",
            "Epoch 29/100 -- Loss:  1.4046\n",
            "[0.56391317, 0.56469864]\n",
            "Epoch 30/100 -- Loss:  1.4030\n",
            "[0.5615524, 0.5621789]\n",
            "Epoch 31/100 -- Loss:  1.4014\n",
            "[0.5591269, 0.5595922]\n",
            "Epoch 32/100 -- Loss:  1.3999\n",
            "[0.556639, 0.5569409]\n",
            "Epoch 33/100 -- Loss:  1.3984\n",
            "[0.5540913, 0.5542277]\n",
            "Epoch 34/100 -- Loss:  1.3969\n",
            "[0.55148613, 0.5514552]\n",
            "Epoch 35/100 -- Loss:  1.3954\n",
            "[0.54882616, 0.5486261]\n",
            "Epoch 36/100 -- Loss:  1.3940\n",
            "[0.54611415, 0.5457434]\n",
            "Epoch 37/100 -- Loss:  1.3927\n",
            "[0.5433528, 0.54280984]\n",
            "Epoch 38/100 -- Loss:  1.3913\n",
            "[0.540545, 0.5398284]\n",
            "Epoch 39/100 -- Loss:  1.3901\n",
            "[0.53769356, 0.5368023]\n",
            "Epoch 40/100 -- Loss:  1.3889\n",
            "[0.5348016, 0.5337345]\n",
            "Epoch 41/100 -- Loss:  1.3877\n",
            "[0.5318722, 0.5306282]\n",
            "Epoch 42/100 -- Loss:  1.3866\n",
            "[0.5289084, 0.52748674]\n",
            "Epoch 43/100 -- Loss:  1.3856\n",
            "[0.52591336, 0.5243133]\n",
            "Epoch 44/100 -- Loss:  1.3847\n",
            "[0.5228903, 0.5211113]\n",
            "Epoch 45/100 -- Loss:  1.3838\n",
            "[0.51984257, 0.5178841]\n",
            "Epoch 46/100 -- Loss:  1.3830\n",
            "[0.51677346, 0.51463515]\n",
            "Epoch 47/100 -- Loss:  1.3823\n",
            "[0.51368636, 0.51136786]\n",
            "Epoch 48/100 -- Loss:  1.3816\n",
            "[0.51058453, 0.5080857]\n",
            "Epoch 49/100 -- Loss:  1.3811\n",
            "[0.50747156, 0.5047923]\n",
            "Epoch 50/100 -- Loss:  1.3806\n",
            "[0.5043508, 0.50149095]\n",
            "Epoch 51/100 -- Loss:  1.3802\n",
            "[0.5012257, 0.49818534]\n",
            "Epoch 52/100 -- Loss:  1.3799\n",
            "[0.4980997, 0.49487883]\n",
            "Epoch 53/100 -- Loss:  1.3797\n",
            "[0.49497637, 0.49157515]\n",
            "Epoch 54/100 -- Loss:  1.3795\n",
            "[0.491859, 0.48827752]\n",
            "Epoch 55/100 -- Loss:  1.3795\n",
            "[0.48875114, 0.48498955]\n",
            "Epoch 56/100 -- Loss:  1.3795\n",
            "[0.4856563, 0.48171467]\n",
            "Epoch 57/100 -- Loss:  1.3796\n",
            "[0.4825777, 0.47845632]\n",
            "Epoch 58/100 -- Loss:  1.3797\n",
            "[0.47951892, 0.47521773]\n",
            "Epoch 59/100 -- Loss:  1.3800\n",
            "[0.47648317, 0.4720023]\n",
            "Epoch 60/100 -- Loss:  1.3803\n",
            "[0.47347385, 0.46881333]\n",
            "Epoch 61/100 -- Loss:  1.3807\n",
            "[0.47049418, 0.46565396]\n",
            "Epoch 62/100 -- Loss:  1.3811\n",
            "[0.4675474, 0.46252736]\n",
            "Epoch 63/100 -- Loss:  1.3816\n",
            "[0.46463656, 0.45943657]\n",
            "Epoch 64/100 -- Loss:  1.3822\n",
            "[0.46176502, 0.45638463]\n",
            "Epoch 65/100 -- Loss:  1.3828\n",
            "[0.45893568, 0.45337445]\n",
            "Epoch 66/100 -- Loss:  1.3835\n",
            "[0.45615146, 0.4504089]\n",
            "Epoch 67/100 -- Loss:  1.3842\n",
            "[0.45341536, 0.44749066]\n",
            "Epoch 68/100 -- Loss:  1.3850\n",
            "[0.45073023, 0.44462243]\n",
            "Epoch 69/100 -- Loss:  1.3858\n",
            "[0.44809878, 0.44180685]\n",
            "Epoch 70/100 -- Loss:  1.3866\n",
            "[0.44552374, 0.43904632]\n",
            "Epoch 71/100 -- Loss:  1.3875\n",
            "[0.4430077, 0.43634325]\n",
            "Epoch 72/100 -- Loss:  1.3884\n",
            "[0.44055316, 0.4337]\n",
            "Epoch 73/100 -- Loss:  1.3892\n",
            "[0.43816265, 0.43111864]\n",
            "Epoch 74/100 -- Loss:  1.3902\n",
            "[0.4358383, 0.42860126]\n",
            "Epoch 75/100 -- Loss:  1.3911\n",
            "[0.43358263, 0.42615002]\n",
            "Epoch 76/100 -- Loss:  1.3920\n",
            "[0.4313977, 0.42376658]\n",
            "Epoch 77/100 -- Loss:  1.3929\n",
            "[0.4292856, 0.42145288]\n",
            "Epoch 78/100 -- Loss:  1.3938\n",
            "[0.42724836, 0.4192105]\n",
            "Epoch 79/100 -- Loss:  1.3946\n",
            "[0.4252879, 0.417041]\n",
            "Epoch 80/100 -- Loss:  1.3955\n",
            "[0.423406, 0.41494596]\n",
            "Epoch 81/100 -- Loss:  1.3963\n",
            "[0.42160445, 0.41292664]\n",
            "Epoch 82/100 -- Loss:  1.3971\n",
            "[0.4198849, 0.4109843]\n",
            "Epoch 83/100 -- Loss:  1.3978\n",
            "[0.4182489, 0.40912017]\n",
            "Epoch 84/100 -- Loss:  1.3985\n",
            "[0.41669795, 0.40733522]\n",
            "Epoch 85/100 -- Loss:  1.3992\n",
            "[0.41523346, 0.40563044]\n",
            "Epoch 86/100 -- Loss:  1.3998\n",
            "[0.41385674, 0.40400672]\n",
            "Epoch 87/100 -- Loss:  1.4003\n",
            "[0.41256905, 0.4024648]\n",
            "Epoch 88/100 -- Loss:  1.4008\n",
            "[0.41137156, 0.40100536]\n",
            "Epoch 89/100 -- Loss:  1.4012\n",
            "[0.41026533, 0.3996289]\n",
            "Epoch 90/100 -- Loss:  1.4015\n",
            "[0.40925133, 0.39833596]\n",
            "Epoch 91/100 -- Loss:  1.4017\n",
            "[0.4083306, 0.39712685]\n",
            "Epoch 92/100 -- Loss:  1.4019\n",
            "[0.40750387, 0.3960019]\n",
            "Epoch 93/100 -- Loss:  1.4020\n",
            "[0.40677202, 0.3949613]\n",
            "Epoch 94/100 -- Loss:  1.4020\n",
            "[0.4061358, 0.39400506]\n",
            "Epoch 95/100 -- Loss:  1.4018\n",
            "[0.4055957, 0.39313328]\n",
            "Epoch 96/100 -- Loss:  1.4016\n",
            "[0.40515247, 0.39234582]\n",
            "Epoch 97/100 -- Loss:  1.4013\n",
            "[0.40480652, 0.39164257]\n",
            "Epoch 98/100 -- Loss:  1.4009\n",
            "[0.4045583, 0.39102316]\n",
            "Epoch 99/100 -- Loss:  1.4004\n",
            "[0.40440825, 0.39048728]\n",
            "Epoch 100/100 -- Loss:  1.3998\n",
            "[0.4043566, 0.39003453]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.50      1.00      0.67        16\n",
            "        True       0.00      0.00      0.00        16\n",
            "\n",
            "    accuracy                           0.50        32\n",
            "   macro avg       0.25      0.50      0.33        32\n",
            "weighted avg       0.25      0.50      0.33        32\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_9=nd.concat(X_1,X_9)\n",
        "model_3, features_3 = build_model(nd.array(A), X_9)\n",
        "print(model_3(X_9))\n",
        "feature_representations_3= train(model_3, features_3, X_9, X_train, y_train, epochs=5000)\n",
        "y_pred_3 = predict(model_3, X_9, X_test)\n",
        "print(classification_report(y_test, y_pred_3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gReyEzTzhj5K",
        "outputId": "5ac4cf4d-276f-4e25-9944-eddda9fae248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[[0.449927  ]\n",
            " [0.44802478]\n",
            " [0.4479793 ]\n",
            " [0.4473323 ]\n",
            " [0.46354604]\n",
            " [0.44049895]\n",
            " [0.44820267]\n",
            " [0.43106285]\n",
            " [0.44640097]\n",
            " [0.44327548]\n",
            " [0.42242947]\n",
            " [0.4427535 ]\n",
            " [0.43767977]\n",
            " [0.4152144 ]\n",
            " [0.4420459 ]\n",
            " [0.4068467 ]\n",
            " [0.4527115 ]\n",
            " [0.4536315 ]\n",
            " [0.42840812]\n",
            " [0.4586172 ]\n",
            " [0.43613517]\n",
            " [0.44860873]\n",
            " [0.45232725]\n",
            " [0.44965717]\n",
            " [0.39818814]\n",
            " [0.40594897]\n",
            " [0.43506503]\n",
            " [0.4413454 ]\n",
            " [0.41293013]\n",
            " [0.43653506]\n",
            " [0.434208  ]\n",
            " [0.44336754]\n",
            " [0.46294644]\n",
            " [0.45468032]]\n",
            "<NDArray 34x1 @cpu(0)>\n",
            "Epoch 500/5000 -- Loss:  0.0005\n",
            "[0.9999293, 0.00044077178]\n",
            "Epoch 1000/5000 -- Loss:  0.0000\n",
            "[0.9999951, 1.3459142e-07]\n",
            "Epoch 1500/5000 -- Loss:  0.0000\n",
            "[0.9999628, 5.8812684e-11]\n",
            "Epoch 2000/5000 -- Loss:  0.0000\n",
            "[1.0, 5.4047966e-14]\n",
            "Epoch 2500/5000 -- Loss:  0.0000\n",
            "[1.0, 4.95697e-17]\n",
            "Epoch 3000/5000 -- Loss:  0.0000\n",
            "[1.0, 4.5462493e-20]\n",
            "Epoch 3500/5000 -- Loss:  0.0000\n",
            "[1.0, 4.1641553e-23]\n",
            "Epoch 4000/5000 -- Loss:  0.0000\n",
            "[1.0, 3.811847e-26]\n",
            "Epoch 4500/5000 -- Loss:  0.0000\n",
            "[1.0, 3.4893463e-29]\n",
            "Epoch 5000/5000 -- Loss:  0.0000\n",
            "[1.0, 3.1941793e-32]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.50      1.00      0.67        16\n",
            "        True       0.00      0.00      0.00        16\n",
            "\n",
            "    accuracy                           0.50        32\n",
            "   macro avg       0.25      0.50      0.33        32\n",
            "weighted avg       0.25      0.50      0.33        32\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_10=nd.concat(X_1,X_10)\n",
        "model_3, features_3 = build_model(nd.array(A), X_10)\n",
        "print(model_3(X_10))\n",
        "feature_representations_3= train(model_3, features_3, X_10, X_train, y_train, epochs=100)\n",
        "y_pred_3 = predict(model_3, X_10, X_test)\n",
        "print(classification_report(y_test, y_pred_3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-__cx9RRz3f",
        "outputId": "5ffd031d-ef24-4eec-fe45-70bd7b72dd5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[[0.5452912 ]\n",
            " [0.5454033 ]\n",
            " [0.54533446]\n",
            " [0.5452704 ]\n",
            " [0.54656565]\n",
            " [0.54507995]\n",
            " [0.5437336 ]\n",
            " [0.54408675]\n",
            " [0.54523677]\n",
            " [0.5447847 ]\n",
            " [0.54095113]\n",
            " [0.54438627]\n",
            " [0.54506415]\n",
            " [0.5463257 ]\n",
            " [0.5447946 ]\n",
            " [0.5459011 ]\n",
            " [0.54537374]\n",
            " [0.5448478 ]\n",
            " [0.5437739 ]\n",
            " [0.5437644 ]\n",
            " [0.5442356 ]\n",
            " [0.54534614]\n",
            " [0.54634094]\n",
            " [0.54535097]\n",
            " [0.54421693]\n",
            " [0.5478348 ]\n",
            " [0.5447993 ]\n",
            " [0.5444375 ]\n",
            " [0.5454569 ]\n",
            " [0.54574984]\n",
            " [0.54446596]\n",
            " [0.54545647]\n",
            " [0.54214996]\n",
            " [0.5425746 ]]\n",
            "<NDArray 34x1 @cpu(0)>\n",
            "Epoch 1/100 -- Loss:  1.3888\n",
            "[0.5452912, 0.5426875]\n",
            "Epoch 2/100 -- Loss:  1.3888\n",
            "[0.545382, 0.5427518]\n",
            "Epoch 3/100 -- Loss:  1.3887\n",
            "[0.5454291, 0.5427674]\n",
            "Epoch 4/100 -- Loss:  1.3887\n",
            "[0.54543227, 0.5427343]\n",
            "Epoch 5/100 -- Loss:  1.3886\n",
            "[0.5453917, 0.5426525]\n",
            "Epoch 6/100 -- Loss:  1.3884\n",
            "[0.5453072, 0.54252213]\n",
            "Epoch 7/100 -- Loss:  1.3883\n",
            "[0.545179, 0.5423432]\n",
            "Epoch 8/100 -- Loss:  1.3881\n",
            "[0.5450073, 0.5421159]\n",
            "Epoch 9/100 -- Loss:  1.3879\n",
            "[0.5447921, 0.54184043]\n",
            "Epoch 10/100 -- Loss:  1.3877\n",
            "[0.5445338, 0.54151696]\n",
            "Epoch 11/100 -- Loss:  1.3874\n",
            "[0.5442325, 0.5411458]\n",
            "Epoch 12/100 -- Loss:  1.3871\n",
            "[0.5438886, 0.5407272]\n",
            "Epoch 13/100 -- Loss:  1.3868\n",
            "[0.5435025, 0.5402616]\n",
            "Epoch 14/100 -- Loss:  1.3865\n",
            "[0.5430745, 0.5397493]\n",
            "Epoch 15/100 -- Loss:  1.3861\n",
            "[0.54260504, 0.5391908]\n",
            "Epoch 16/100 -- Loss:  1.3858\n",
            "[0.5420947, 0.53858656]\n",
            "Epoch 17/100 -- Loss:  1.3854\n",
            "[0.5415439, 0.5379371]\n",
            "Epoch 18/100 -- Loss:  1.3850\n",
            "[0.5409532, 0.5372428]\n",
            "Epoch 19/100 -- Loss:  1.3845\n",
            "[0.5403233, 0.5365045]\n",
            "Epoch 20/100 -- Loss:  1.3841\n",
            "[0.53965473, 0.5357227]\n",
            "Epoch 21/100 -- Loss:  1.3836\n",
            "[0.5389483, 0.53489804]\n",
            "Epoch 22/100 -- Loss:  1.3832\n",
            "[0.5382046, 0.5340313]\n",
            "Epoch 23/100 -- Loss:  1.3827\n",
            "[0.5374245, 0.53312314]\n",
            "Epoch 24/100 -- Loss:  1.3821\n",
            "[0.53660876, 0.53217435]\n",
            "Epoch 25/100 -- Loss:  1.3816\n",
            "[0.53575826, 0.53118575]\n",
            "Epoch 26/100 -- Loss:  1.3811\n",
            "[0.53487384, 0.53015816]\n",
            "Epoch 27/100 -- Loss:  1.3805\n",
            "[0.53395647, 0.52909255]\n",
            "Epoch 28/100 -- Loss:  1.3800\n",
            "[0.5330071, 0.52798975]\n",
            "Epoch 29/100 -- Loss:  1.3794\n",
            "[0.5320266, 0.52685076]\n",
            "Epoch 30/100 -- Loss:  1.3788\n",
            "[0.5310162, 0.5256764]\n",
            "Epoch 31/100 -- Loss:  1.3782\n",
            "[0.52997684, 0.5244679]\n",
            "Epoch 32/100 -- Loss:  1.3777\n",
            "[0.5289096, 0.52322614]\n",
            "Epoch 33/100 -- Loss:  1.3771\n",
            "[0.5278157, 0.5219522]\n",
            "Epoch 34/100 -- Loss:  1.3764\n",
            "[0.52669626, 0.52064717]\n",
            "Epoch 35/100 -- Loss:  1.3758\n",
            "[0.52555245, 0.51931226]\n",
            "Epoch 36/100 -- Loss:  1.3752\n",
            "[0.5243856, 0.5179485]\n",
            "Epoch 37/100 -- Loss:  1.3746\n",
            "[0.5231968, 0.5165571]\n",
            "Epoch 38/100 -- Loss:  1.3740\n",
            "[0.5219874, 0.51513916]\n",
            "Epoch 39/100 -- Loss:  1.3734\n",
            "[0.5207587, 0.513696]\n",
            "Epoch 40/100 -- Loss:  1.3728\n",
            "[0.51951206, 0.5122288]\n",
            "Epoch 41/100 -- Loss:  1.3722\n",
            "[0.51824874, 0.5107388]\n",
            "Epoch 42/100 -- Loss:  1.3715\n",
            "[0.5169702, 0.5092272]\n",
            "Epoch 43/100 -- Loss:  1.3709\n",
            "[0.51567787, 0.5076953]\n",
            "Epoch 44/100 -- Loss:  1.3703\n",
            "[0.51437306, 0.5061444]\n",
            "Epoch 45/100 -- Loss:  1.3697\n",
            "[0.51305723, 0.50457585]\n",
            "Epoch 46/100 -- Loss:  1.3691\n",
            "[0.51173186, 0.50299084]\n",
            "Epoch 47/100 -- Loss:  1.3685\n",
            "[0.5103984, 0.50139076]\n",
            "Epoch 48/100 -- Loss:  1.3679\n",
            "[0.5090583, 0.4997768]\n",
            "Epoch 49/100 -- Loss:  1.3673\n",
            "[0.50771314, 0.4981504]\n",
            "Epoch 50/100 -- Loss:  1.3667\n",
            "[0.5063643, 0.49651283]\n",
            "Epoch 51/100 -- Loss:  1.3661\n",
            "[0.5050133, 0.49486542]\n",
            "Epoch 52/100 -- Loss:  1.3655\n",
            "[0.50366175, 0.4932094]\n",
            "Epoch 53/100 -- Loss:  1.3649\n",
            "[0.50231105, 0.49154612]\n",
            "Epoch 54/100 -- Loss:  1.3643\n",
            "[0.5009628, 0.48987696]\n",
            "Epoch 55/100 -- Loss:  1.3637\n",
            "[0.49961847, 0.48820314]\n",
            "Epoch 56/100 -- Loss:  1.3631\n",
            "[0.49827966, 0.48652592]\n",
            "Epoch 57/100 -- Loss:  1.3626\n",
            "[0.4969478, 0.4848466]\n",
            "Epoch 58/100 -- Loss:  1.3620\n",
            "[0.49562442, 0.48316643]\n",
            "Epoch 59/100 -- Loss:  1.3614\n",
            "[0.49431112, 0.48148662]\n",
            "Epoch 60/100 -- Loss:  1.3608\n",
            "[0.49300933, 0.4798084]\n",
            "Epoch 61/100 -- Loss:  1.3602\n",
            "[0.49172056, 0.47813293]\n",
            "Epoch 62/100 -- Loss:  1.3596\n",
            "[0.49044633, 0.47646144]\n",
            "Epoch 63/100 -- Loss:  1.3590\n",
            "[0.48918808, 0.47479513]\n",
            "Epoch 64/100 -- Loss:  1.3584\n",
            "[0.4879473, 0.4731349]\n",
            "Epoch 65/100 -- Loss:  1.3577\n",
            "[0.48672545, 0.47148213]\n",
            "Epoch 66/100 -- Loss:  1.3571\n",
            "[0.48552394, 0.46983764]\n",
            "Epoch 67/100 -- Loss:  1.3565\n",
            "[0.48434424, 0.4682026]\n",
            "Epoch 68/100 -- Loss:  1.3558\n",
            "[0.48318774, 0.46657792]\n",
            "Epoch 69/100 -- Loss:  1.3551\n",
            "[0.4820558, 0.46496466]\n",
            "Epoch 70/100 -- Loss:  1.3544\n",
            "[0.4809498, 0.46336368]\n",
            "Epoch 71/100 -- Loss:  1.3537\n",
            "[0.47987106, 0.46177584]\n",
            "Epoch 72/100 -- Loss:  1.3530\n",
            "[0.47882095, 0.46020204]\n",
            "Epoch 73/100 -- Loss:  1.3522\n",
            "[0.47780073, 0.45864305]\n",
            "Epoch 74/100 -- Loss:  1.3515\n",
            "[0.47681174, 0.45709962]\n",
            "Epoch 75/100 -- Loss:  1.3507\n",
            "[0.47585517, 0.45557252]\n",
            "Epoch 76/100 -- Loss:  1.3498\n",
            "[0.47493222, 0.45406228]\n",
            "Epoch 77/100 -- Loss:  1.3490\n",
            "[0.4740441, 0.4525697]\n",
            "Epoch 78/100 -- Loss:  1.3481\n",
            "[0.473192, 0.45109522]\n",
            "Epoch 79/100 -- Loss:  1.3472\n",
            "[0.47237712, 0.44963932]\n",
            "Epoch 80/100 -- Loss:  1.3462\n",
            "[0.47160044, 0.44820258]\n",
            "Epoch 81/100 -- Loss:  1.3452\n",
            "[0.47086316, 0.44678542]\n",
            "Epoch 82/100 -- Loss:  1.3442\n",
            "[0.47016624, 0.4453881]\n",
            "Epoch 83/100 -- Loss:  1.3431\n",
            "[0.46951067, 0.44401103]\n",
            "Epoch 84/100 -- Loss:  1.3419\n",
            "[0.46889758, 0.44265434]\n",
            "Epoch 85/100 -- Loss:  1.3408\n",
            "[0.4683278, 0.44131827]\n",
            "Epoch 86/100 -- Loss:  1.3395\n",
            "[0.46780232, 0.44000295]\n",
            "Epoch 87/100 -- Loss:  1.3383\n",
            "[0.46732202, 0.43870848]\n",
            "Epoch 88/100 -- Loss:  1.3369\n",
            "[0.4668877, 0.43743488]\n",
            "Epoch 89/100 -- Loss:  1.3355\n",
            "[0.46650028, 0.43618196]\n",
            "Epoch 90/100 -- Loss:  1.3341\n",
            "[0.46616048, 0.4349498]\n",
            "Epoch 91/100 -- Loss:  1.3325\n",
            "[0.46586907, 0.4337381]\n",
            "Epoch 92/100 -- Loss:  1.3310\n",
            "[0.46562678, 0.43254668]\n",
            "Epoch 93/100 -- Loss:  1.3293\n",
            "[0.4654343, 0.43137518]\n",
            "Epoch 94/100 -- Loss:  1.3276\n",
            "[0.4652923, 0.43022323]\n",
            "Epoch 95/100 -- Loss:  1.3258\n",
            "[0.46520144, 0.42909047]\n",
            "Epoch 96/100 -- Loss:  1.3239\n",
            "[0.46516222, 0.4279763]\n",
            "Epoch 97/100 -- Loss:  1.3220\n",
            "[0.46517527, 0.42688027]\n",
            "Epoch 98/100 -- Loss:  1.3200\n",
            "[0.46524101, 0.42580158]\n",
            "Epoch 99/100 -- Loss:  1.3179\n",
            "[0.4653601, 0.4247397]\n",
            "Epoch 100/100 -- Loss:  1.3157\n",
            "[0.46553287, 0.4236937]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.50      1.00      0.67        16\n",
            "        True       0.00      0.00      0.00        16\n",
            "\n",
            "    accuracy                           0.50        32\n",
            "   macro avg       0.25      0.50      0.33        32\n",
            "weighted avg       0.25      0.50      0.33        32\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_10=nd.concat(X_1,X_10)\n",
        "model_3, features_3 = build_model(nd.array(A), X_10)\n",
        "print(model_3(X_10))\n",
        "feature_representations_3= train(model_3, features_3, X_10, X_train, y_train, epochs=5000)\n",
        "y_pred_3 = predict(model_3, X_10, X_test)\n",
        "print(classification_report(y_test, y_pred_3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA3-qCnBhm05",
        "outputId": "dfbdd2e6-7d0a-4d7e-9531-986945d39f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[[0.7080936 ]\n",
            " [0.7079525 ]\n",
            " [0.7112951 ]\n",
            " [0.70317346]\n",
            " [0.70873475]\n",
            " [0.70885074]\n",
            " [0.7324642 ]\n",
            " [0.71249753]\n",
            " [0.70978117]\n",
            " [0.7179765 ]\n",
            " [0.76838106]\n",
            " [0.70889884]\n",
            " [0.7151584 ]\n",
            " [0.6686872 ]\n",
            " [0.7062227 ]\n",
            " [0.7234348 ]\n",
            " [0.71074057]\n",
            " [0.70553917]\n",
            " [0.7354285 ]\n",
            " [0.69343764]\n",
            " [0.7222098 ]\n",
            " [0.7082248 ]\n",
            " [0.7117739 ]\n",
            " [0.70887685]\n",
            " [0.7206643 ]\n",
            " [0.72338843]\n",
            " [0.7379661 ]\n",
            " [0.7275046 ]\n",
            " [0.7089175 ]\n",
            " [0.7105246 ]\n",
            " [0.7114751 ]\n",
            " [0.7153212 ]\n",
            " [0.6945309 ]\n",
            " [0.67037386]]\n",
            "<NDArray 34x1 @cpu(0)>\n",
            "Epoch 500/5000 -- Loss:  0.0000\n",
            "[1.0, 2.804077e-05]\n",
            "Epoch 1000/5000 -- Loss:  0.0000\n",
            "[1.0, 4.9650826e-12]\n",
            "Epoch 1500/5000 -- Loss:  0.0000\n",
            "[1.0, 9.29431e-19]\n",
            "Epoch 2000/5000 -- Loss:  0.0000\n",
            "[1.0, 1.7392036e-25]\n",
            "Epoch 2500/5000 -- Loss:  0.0000\n",
            "[1.0, 3.2544463e-32]\n",
            "Epoch 3000/5000 -- Loss:  0.0000\n",
            "[1.0, 6.089903e-39]\n",
            "Epoch 3500/5000 -- Loss:  0.0000\n",
            "[1.0, 0.0]\n",
            "Epoch 4000/5000 -- Loss:  0.0000\n",
            "[1.0, 0.0]\n",
            "Epoch 4500/5000 -- Loss:  0.0000\n",
            "[1.0, 0.0]\n",
            "Epoch 5000/5000 -- Loss:  0.0000\n",
            "[1.0, 0.0]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.52      1.00      0.68        16\n",
            "        True       1.00      0.06      0.12        16\n",
            "\n",
            "    accuracy                           0.53        32\n",
            "   macro avg       0.76      0.53      0.40        32\n",
            "weighted avg       0.76      0.53      0.40        32\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above two models, it can be seen that by concatenating identity matrix and X_10 together, we get a higher accuracy than that of X_9 which proves our theory that hexagons are stronger and add more value."
      ],
      "metadata": {
        "id": "yGFoffspiXsP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "en8u3Ja1kEoG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}